[{
  "history_id" : "3rg6zw92wvd",
  "history_input" : "# get all of the cmaq model output variables and save to hourly csvs\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 6)   # end date\n\ndays = get_days_list(sdate, edate)\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntraining_input_folder = f\"{cmaq_folder}/training_input_hourly/\"\ncreate_and_clean_folder(training_input_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  df_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\"_extracted.nc\")\n  \n  # read mcip results \n  df_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\")\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value >= 0 and real_hour_value <=11:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n    # CMAQ data\n    # O3 variable\n    \n    o3=df_cmaq.variables['O3'][:].values[k,0]\n    cmaq_O3=list(np.ravel(o3).transpose())  \n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k,0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k,0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k,0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k,0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k,0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    lat_flt=np.ravel(lat)\n    LAT=np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    \n    filename = f'/groups/ESS/aalnaim/cmaq/training_input_hourly/train_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    print(f'Saving file: train_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(filename,index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220805\n",
  "history_begin_time" : 1660786748107,
  "history_end_time" : 1660786902446,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "g0syyu8xx33",
  "history_input" : "# get all the airnow station data and save to csvs as well\nimport glob, os\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 6)   # end date\ndays = get_days_list(sdate, edate)\n\nobservation_folder = f\"{cmaq_folder}/observation/\"\ncreate_and_clean_folder(observation_folder)\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in range(len(days)-1):\n  current_day = days[i]\n  next_day = days[i+1]\n  for x in range(len(time)):\n    if x >= 12:\n      i = next_day\n    else:\n      i = current_day\n    t = time[x]\n    files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n    \n    with open(files, 'r') as file:\n      text = file.read()\n      new_string = text.replace('\"', '')\n      out_file = f\"{observation_folder}/AQF5X_Hourly_{i}{t}.txt\"\n      print(\"Saving to :\", out_file)\n      outF = open(out_file, \"w\")\n      for line in new_string:\n        outF.write(line)\n      \n      outF.close()",
  "history_output" : "Saving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080512.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080513.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080514.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080515.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080516.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080517.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080518.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080519.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080520.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080521.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080522.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080523.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080600.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080601.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080602.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080603.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080604.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080605.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080606.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080607.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080608.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080609.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080610.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080611.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080612.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080613.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080614.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080615.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080616.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080617.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080618.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080619.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080620.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080621.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080622.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080623.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080700.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080701.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080702.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080703.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080704.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080705.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080706.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080707.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080708.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080709.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080710.txt\nSaving to : /groups/ESS/aalnaim/cmaq//observation//AQF5X_Hourly_2022080711.txt\n",
  "history_begin_time" : 1660786767102,
  "history_end_time" : 1660786902447,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "0m1b4ahgqno",
  "history_input" : "# take all the airnow observation csvs and merge into one observation.csv\n\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 6)   # end date\ndays = get_days_list(sdate, edate)\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  for y in range(len(time)):\n    t = time[y]\n    if y>=12:\n      d = next_day\n    else:\n      d = current_day\n    files=glob.glob(f\"{cmaq_folder}/observation/AQF5X_Hourly_{d}{t}.txt\")\n    for file in files:\n      print(file)\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # constructs an array for each hour of each day with length of data from total stations available\n      date_time.append(dt)\n      merged.append(data)\n            \ndata_frame = np.concatenate(merged)\n\n# This gets the first 4 columns in the observation file (AQSID, Latitude, Longitude, OZONE(ppb))\ndata_frame = np.delete(data_frame, np.s_[4:9], axis=1) \n\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\ndff=df.replace(',','', regex=True)\n\ndt = np.concatenate(date_time)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(f\"{cmaq_folder}/observation/observation.csv\",index=False)",
  "history_output" : "/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080512.txt\n2022080512\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080513.txt\n2022080513\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080514.txt\n2022080514\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080515.txt\n2022080515\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080516.txt\n2022080516\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080517.txt\n2022080517\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080518.txt\n2022080518\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080519.txt\n2022080519\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080520.txt\n2022080520\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080521.txt\n2022080521\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080522.txt\n2022080522\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080523.txt\n2022080523\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080600.txt\n2022080600\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080601.txt\n2022080601\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080602.txt\n2022080602\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080603.txt\n2022080603\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080604.txt\n2022080604\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080605.txt\n2022080605\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080606.txt\n2022080606\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080607.txt\n2022080607\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080608.txt\n2022080608\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080609.txt\n2022080609\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080610.txt\n2022080610\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080611.txt\n2022080611\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080612.txt\n2022080612\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080613.txt\n2022080613\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080614.txt\n2022080614\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080615.txt\n2022080615\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080616.txt\n2022080616\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080617.txt\n2022080617\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080618.txt\n2022080618\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080619.txt\n2022080619\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080620.txt\n2022080620\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080621.txt\n2022080621\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080622.txt\n2022080622\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080623.txt\n2022080623\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080700.txt\n2022080700\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080701.txt\n2022080701\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080702.txt\n2022080702\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080703.txt\n2022080703\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080704.txt\n2022080704\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080705.txt\n2022080705\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080706.txt\n2022080706\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080707.txt\n2022080707\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080708.txt\n2022080708\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080709.txt\n2022080709\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080710.txt\n2022080710\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080711.txt\n2022080711\n",
  "history_begin_time" : 1660786792797,
  "history_end_time" : 1660786902447,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "owffklmsmvp",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 8, in <module>\n    cmaq = pd.concat(df_from_each_hourly_file)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 346, in concat\n    op = _Concatenator(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 403, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n",
  "history_begin_time" : 1660786815797,
  "history_end_time" : 1660786902448,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "m90ga6knqaz",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786902448,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "aquaie8jcp3",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786902450,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "5ri50befevm",
  "history_input" : "# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 6)   # end date\ndays = get_days_list(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  df_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\"_extracted.nc\")\n  \n  # read mcip results \n  df_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\")\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n#     print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n#     print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n#     print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n#     cmaq_O3=list(np.ravel(o3).transpose())\n#     print(\"o3 shape: \", o3.shape)\n#     print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv',index=False)\n\nprint('Done with preparing testing data!')",
  "history_output" : "Getting data for: 20220805\nlat shape (265, 442)\nTraceback (most recent call last):\n  File \"processing_test_data.py\", line 112, in <module>\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\nNameError: name 'cmaq_O3' is not defined\n",
  "history_begin_time" : 1660786883643,
  "history_end_time" : 1660786906365,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "p6wvf2",
  "indicator" : "Failed"
},{
  "history_id" : "jgxmctru2yp",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786903455,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "fqd4s61bfhh",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786903457,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "znxeniqoulg",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786903459,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "b2em6k14x41",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786903460,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "ilxf27r7me8",
  "history_input" : "# All the utility functions that most steps in CMAQ-AI need\n# this file should not contain any direct call of function\n# it should be dedicated to define functions or variables\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import date, datetime, timedelta\n\n\n# home directory\nhome = str(Path.home())\ncmaq_folder = \"/groups/ESS/aalnaim/cmaq/\" # change if you want to use your own folder\n\ndef get_days_list(sdate, edate):\n  days=[]\n  \n  delta = edate - sdate       # as timedelta\n\n  for i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n  # add one more day\n  one_more_day = sdate + timedelta(days=delta.days + 1)\n  list_day=one_more_day.strftime('%Y%m%d')\n  days.append(list_day)\n  \n  return days\n\ndef create_and_clean_folder(folder_path):\n  os.makedirs(folder_path, exist_ok=True)\n  # clean all files inside the folder\n  for f in os.listdir(folder_path):\n    os.remove(os.path.join(folder_path, f))\n\ndef remove_file(file_path):\n  print(f'remove old files{file_path}')\n  if os.path.exists(file_path):\n    os.remove(file_path)\n    \ndef turn_2_digits(a):\n  return f\"{a:02}\"",
  "history_output" : "Running",
  "history_begin_time" : 1660786750601,
  "history_end_time" : 1660786903462,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "i3xoxv4vkdw",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom cmaq_ai_utils import *\n\n# importing data\nfinal=pd.read_csv(f'{cmaq_folder}/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n\ncreate_and_clean_folder(f\"{cmaq_folder}/models/\")\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x','CMAQ12KM_NO2(ppb)', 'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'CO(moles/s)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)','WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC'],axis=1)\nprint(\"input shape:\", X.shape)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = f'{cmaq_folder}/models/rf_pycaret_o3_only.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   30.260300   -81.453300       14.0  ...      8    5     12\n1   42.770802   -71.102798       40.0  ...      8    5     12\n2   34.528599   -86.970596        8.0  ...      8    5     12\n3   42.194401   -72.555603       23.0  ...      8    5     12\n4   34.751801   -82.256699        8.0  ...      8    5     12\n\n[5 rows x 18 columns]\ninput shape: (55900, 4)\n/home/aalnaim/CMAQAI/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n  warn(\n",
  "history_begin_time" : 1660786836196,
  "history_end_time" : 1660786903466,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
}]
