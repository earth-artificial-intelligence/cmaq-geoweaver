[{
  "history_id" : "2g6awtvp3ha",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608194,
  "history_end_time" : 1707942608194,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ov4oizt1u06",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608237,
  "history_end_time" : 1707942608237,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t6t8r1xxgrf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608239,
  "history_end_time" : 1707942608239,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gxxx13bqwcf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608242,
  "history_end_time" : 1707942608242,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g30wz5ydrhe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608243,
  "history_end_time" : 1707942608243,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jthrc21wasc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608246,
  "history_end_time" : 1707942608246,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gaabmb7vqr7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608248,
  "history_end_time" : 1707942608248,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "162krbqzocr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608251,
  "history_end_time" : 1707942608251,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4c5q0koraym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608253,
  "history_end_time" : 1707942608253,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "30o0v5iiiyp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608255,
  "history_end_time" : 1707942608255,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fhahbn2gj13",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608278,
  "history_end_time" : 1707942608278,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "easnxocaikw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608281,
  "history_end_time" : 1707942608281,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8fieqg39clu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608283,
  "history_end_time" : 1707942608283,
  "history_notes" : null,
  "history_process" : "pvzabv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bym7jhkomes",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608285,
  "history_end_time" : 1707942608285,
  "history_notes" : null,
  "history_process" : "8i9ptn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pxvod0a05je",
  "history_input" : "#!/bin/bash\n\ndays_back=40\n\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\ncmaq_gif_location=\"/groups/ESS/share/projects/SWUS3km/graph/12km/\"\n\necho \"start to traverse \"${cmaq_gif_location}\n\nfor i in $(seq 0 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i))\n  # Setting env variables\n  YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  #/groups/ESS/share/projects/SWUS3km/graph/12km/20221108/FORECAST_O3_20221108.gif\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/\"FORECAST_O3_\"$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/obsoverlay/gif/OBS-FORECAST_O3_$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  \ndone\n",
  "history_output" : "start to traverse /groups/ESS/share/projects/SWUS3km/graph/12km/\n0 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240214/FORECAST_O3_20240214.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240214/obsoverlay/gif/OBS-FORECAST_O3_20240214.gif': No such file or directory\n1 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240213/obsoverlay/gif/OBS-FORECAST_O3_20240213.gif': No such file or directory\n2 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240212/obsoverlay/gif/OBS-FORECAST_O3_20240212.gif': No such file or directory\n3 days ago\n4 days ago\n5 days ago\n6 days ago\n7 days ago\n8 days ago\n9 days ago\n10 days ago\n11 days ago\n12 days ago\n13 days ago\n14 days ago\n15 days ago\n16 days ago\n17 days ago\n18 days ago\n19 days ago\n20 days ago\n21 days ago\n22 days ago\n23 days ago\n24 days ago\n25 days ago\n26 days ago\n27 days ago\n28 days ago\n29 days ago\n30 days ago\n31 days ago\n32 days ago\n33 days ago\n34 days ago\n35 days ago\n36 days ago\n37 days ago\n38 days ago\n39 days ago\n40 days ago\n",
  "history_begin_time" : 1707942609130,
  "history_end_time" : 1707942611721,
  "history_notes" : null,
  "history_process" : "nndpw6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "voes8wfea0x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707942608316,
  "history_end_time" : 1707942608316,
  "history_notes" : null,
  "history_process" : "slsirb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3pitdrxtwic",
  "history_input" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"rf_prediction_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J rf_prediction       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 8               # Number of CPUs per task (threads)\n#SBATCH --mem=150G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\nprint(\"create and clean the prediction folder\")\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      file_path = os.path.join(testing_path, file_name)\n      testing_df = pd.read_csv(file_path)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      print(\"adding month, day, and hours\")\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[4:6], errors='coerce', downcast='integer')\n      testing_df['day'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[6:8], errors='coerce', downcast='integer')\n      testing_df['hours'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[8:10], errors='coerce', downcast='integer')\n\n      print(testing_df['YYYYMMDDHH'].values[0])\n      print(testing_df['month'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      print(f\"file_dateTime={file_dateTime}\")\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)\n        \nprint(\"Prediction is all done.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "Running",
  "history_begin_time" : 1707942652938,
  "history_end_time" : 1707942652751,
  "history_notes" : null,
  "history_process" : "xa1jxg",
  "host_id" : "100001",
  "indicator" : "Running"
},{
  "history_id" : "p3y8p6456f9",
  "history_input" : "#!/bin/bash\n\n\necho \"start to run processing_test_data_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"processing_test_data_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J processing_test_data       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)  # don't clean folder anymore\n#os.makedirs(test_folder, exist_ok=True)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file)\n  \n  # read mcip results \n  mcip_file = \"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file)\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    hourly_target_file = f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    if os.path.exists(hourly_target_file):\n      print(f\"Hourly csv file {hourly_target_file} already exist\")\n      continue\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(hourly_target_file, index=False)\n\nprint('Done with preparing testing data!')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "start to run processing_test_data_generated.sh\n/home/zsun/gw-workspace/p3y8p6456f9\nwrite the slurm script into processing_test_data_generated.sh\nsbatch processing_test_data_generated.sh\njob_id=1570147\nJob 1570147 has finished with state: JobState=COMPLETED\nSlurm job (1570147) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1570147      processin+  COMPLETED      0:0            2024-02-14T15:30:11 2024-02-14T15:30:49 \n1570147.bat+      batch  COMPLETED      0:0          0 2024-02-14T15:30:11 2024-02-14T15:30:49 \n1570147.ext+     extern  COMPLETED      0:0          0 2024-02-14T15:30:11 2024-02-14T15:30:49 \nGetting data for: 20240105\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105_ML_extracted.nc already exists\nGetting data for: 20240106\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106_ML_extracted.nc already exists\nGetting data for: 20240107\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240107_ML_extracted.nc already exists\nGetting data for: 20240108\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240108_ML_extracted.nc already exists\nGetting data for: 20240109\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240109_ML_extracted.nc already exists\nGetting data for: 20240110\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240110_ML_extracted.nc already exists\nGetting data for: 20240111\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240111_ML_extracted.nc already exists\nGetting data for: 20240112\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240112_ML_extracted.nc already exists\nGetting data for: 20240113\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240113_ML_extracted.nc already exists\nGetting data for: 20240114\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240114_ML_extracted.nc already exists\nGetting data for: 20240115\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240115_ML_extracted.nc already exists\nGetting data for: 20240116\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240116_ML_extracted.nc already exists\nGetting data for: 20240117\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240117_ML_extracted.nc already exists\nGetting data for: 20240118\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240118_ML_extracted.nc already exists\nGetting data for: 20240119\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240119_ML_extracted.nc already exists\nGetting data for: 20240120\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240120_ML_extracted.nc already exists\nGetting data for: 20240121\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240121_ML_extracted.nc already exists\nGetting data for: 20240122\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240122_ML_extracted.nc already exists\nGetting data for: 20240123\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240123_ML_extracted.nc already exists\nGetting data for: 20240124\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240124_ML_extracted.nc already exists\nGetting data for: 20240125\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240125_ML_extracted.nc already exists\nGetting data for: 20240126\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240126_ML_extracted.nc already exists\nGetting data for: 20240127\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240127_ML_extracted.nc already exists\nGetting data for: 20240128\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240128_ML_extracted.nc already exists\nGetting data for: 20240129\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240129_ML_extracted.nc already exists\nGetting data for: 20240130\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240130_ML_extracted.nc already exists\nGetting data for: 20240131\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240131_ML_extracted.nc already exists\nGetting data for: 20240201\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240201_ML_extracted.nc already exists\nGetting data for: 20240202\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240202_ML_extracted.nc already exists\nGetting data for: 20240203\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240203_ML_extracted.nc already exists\nGetting data for: 20240204\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240204_ML_extracted.nc already exists\nGetting data for: 20240205\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240205_ML_extracted.nc already exists\nGetting data for: 20240206\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240206_ML_extracted.nc already exists\nGetting data for: 20240207\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240207_ML_extracted.nc already exists\nGetting data for: 20240208\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240208_ML_extracted.nc already exists\nGetting data for: 20240209\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240209_ML_extracted.nc already exists\nGetting data for: 20240210\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240210_ML_extracted.nc already exists\nGetting data for: 20240211\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240211_ML_extracted.nc already exists\nGetting data for: 20240212\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240212_ML_extracted.nc already exists\nGetting data for: 20240213\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240213_ML_extracted.nc already exists\nGetting data for: 20240214\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240214.nc doesn't exist\nGetting data for: 20240215\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240215.nc doesn't exist\nGetting data for: 20240216\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240216.nc doesn't exist\nGetting data for: 20240217\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240217.nc doesn't exist\nGetting data for: 20240218\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240218.nc doesn't exist\nGetting data for: 20240219\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240219.nc doesn't exist\nGetting data for: 20240220\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240220.nc doesn't exist\nDone with preparing testing data!\nAll slurm job for processing_test_data_generated.sh finishes.\n",
  "history_begin_time" : 1707942609130,
  "history_end_time" : 1707942650943,
  "history_notes" : null,
  "history_process" : "z4du0c",
  "host_id" : "100001",
  "indicator" : "Done"
}]
