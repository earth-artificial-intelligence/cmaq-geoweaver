[{
  "history_id" : "fv7ns7y5r2j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408612,
  "history_end_time" : 1717529408612,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tk0jn4du1mi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408643,
  "history_end_time" : 1717529408643,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h9dnz5ri9at",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408645,
  "history_end_time" : 1717529408645,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h5l5jjvvj61",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408646,
  "history_end_time" : 1717529408646,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "seobdoa0ysr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408647,
  "history_end_time" : 1717529408647,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6qy8t6jvcvd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408650,
  "history_end_time" : 1717529408650,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jja7uf3gc81",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408651,
  "history_end_time" : 1717529408651,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uhe53sbw9pc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408652,
  "history_end_time" : 1717529408652,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "siiygi7slzd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408654,
  "history_end_time" : 1717529408654,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5b7g2oorv67",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408659,
  "history_end_time" : 1717529408659,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n3z1qsrqcal",
  "history_input" : "#!/bin/bash\n\n\n# generate images and gif from the prediction NetCDF files and overlay the AirNow station observation on the top\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\nmkdir $cmaq_folder\"/plots\"\npermanent_location=/groups/ESS3/zsun/cmaq/ai_results/\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport graph_dir=\"/groups/ESS/zsun/cmaq/plots\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n\nsource /home/zsun/.bashrc\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\ncat <<EOF >>$cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\") \nd1 = getenv(\"stdate_post\") \nd2 = getenv(\"eddate_post\") \n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\nobs_dir = getenv(\"obs_dir_NCL\")\nplot_dir = getenv(\"graph_dir\") \n\nhr=new(24,\"string\")\nhr=(/\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"/)\n\nprint(plot_dir)\naconc_dir = getenv(\"postdata_dir\") \ngrid_dir = getenv(\"mcip_dir\")\n\nprint(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\n\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\ncdf_file2= addfile(grid_dir+\"/METCRO2D_\"+date+\".nc\",\"r\")\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\nwspd10=cdf_file2->WSPD10(:,0,:,:)\nwdir10=cdf_file2->WDIR10(:,0,:,:)\n\ntemp = cdf_file2->TEMP2\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(max(temp))\nprint(min(temp))\nprint(avg(temp))\n\n\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\no3@unit = \"ppbv\"\n\nUV10=wind_component(wspd10,wdir10,0)\nUV10@lat2d = lat\nUV10@lon2d = lon\n\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; dont advance frame\nres@gsnDraw = False\nres@gsnLeftString  = \"\"\nres@gsnRightString = \"\"\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n;res@tiMainFontHeightF = 0.02\n;res@vpWidthF        = 0.7\n;res@vpHeightF       = 0.7\n\n;;set map;;\nmpres                             = res\nmpres@mpLimitMode = \"LatLon\"\nmpres@mpDataSetName               = \"Earth..4\"\nmpres@mpDataBaseVersion           = \"MediumRes\"\nmpres@mpOutlineOn                 = True\nmpres@mpGeophysicalLineThicknessF = 1.5\nmpres@mpFillDrawOrder             = \"PostDraw\"\nmpres@mpFillOn                    = False\nmpres@mpAreaMaskingOn         = True\nmpres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nmpres@mpOutlineSpecifiers         = \"United States:States\"\nmpres@mpProjection           = \"LambertConformal\"\nmpres@mpLambertParallel1F    = 33.\nmpres@mpLambertParallel2F    = 45.\nmpres@mpLambertMeridianF     = -98.\nmpres@mpMinLonF = -120 ;min(lon)+0.2\nmpres@mpMaxLonF = -70 ;max(lon)-0.2\nmpres@mpMinLatF = 25 ;min(lat)+0.05\nmpres@mpMaxLatF = 50 ;max(lat)-0.05\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@mpLandFillColor         = \"white\"\nmpres@mpInlandWaterFillColor  = \"white\"\nmpres@mpOceanFillColor        = \"white\"\nmpres@mpGeophysicalLineColor    = \"Black\"\n\n;mpres@lbLabelAutoStride       = True\nmpres@tiXAxisFont             = 25\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@tmXBLabelFont           = 25\nmpres@tmXBLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmYLLabelFont           = 25\nmpres@tmYLLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmXTLabelsOn            = False\nmpres@tmXTLabelFont           = 25\nmpres@tmXTLabelFontHeightF    = 0.013\nmpres@tmYRLabelsOn            = False\nmpres@tmYRLabelFont           = 25\nmpres@tmYRLabelFontHeightF    = 0.013\n\n;;set contour;;\ncnres                         = res\ncnres@cnFillDrawOrder         = \"PreDraw\"\ncnres@cnFillOn                = True\ncnres@cnLinesOn               = False\ncnres@cnLineLabelsOn          = False\ncnres@lbLabelFont             = 25\ncnres@lbLabelFontHeightF      = 0.013\ncnres@tiXAxisFont             = 25\ncnres@pmLabelBarWidthF        = 0.5\ncnres@pmLabelBarHeightF       = 0.1\n;cnres@pmLabelBarOrthogonalPosF = -0.02\ncnres@lbLabelAutoStride       = True\n\n;set vector;;\nres_vc                        = res\nres_vc@vcGlyphStyle           = \"LineArrow\"\nres_vc@vcLineArrowThicknessF  = 3\nres_vc@vcMinDistanceF         = 0.03\nres_vc@vcRefLengthF           = 0.03\nres_vc@vcRefAnnoOn            = True\nres_vc@vcRefMagnitudeF           = 16\nres_vc@vcRefAnnoString1          = \"16m/s\"\nres_vc@vcRefAnnoSide             = \"Top\"\nres_vc@vcRefAnnoString2On        = False\nres_vc@vcRefAnnoPerimOn          = False\nres_vc@vcRefAnnoOrthogonalPosF   = -0.02\nres_vc@vcRefAnnoParallelPosF     = 0.999\n;res_vc@vcRefAnnoBackgroundColor = \"White\"\nres_vc@vcVectorDrawOrder         = \"PostDraw\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  ;print(time(it,0)+\" \"+time(it,1))\n  rundate = yyyyddd_to_yyyymmdd( time(it,0) )\n  runtime = hr( tointeger(time(it,1)/10000) )\n\n  site = readAsciiTable(obs_dir+\"/AQF5X_Hourly_\"+rundate+runtime+\".dat\",1,\"string\",1)\n  nrows = dimsizes(site)\n  sitename = str_get_field(site,1,\",\")\n  sitelat = stringtofloat(str_get_field(site,2,\",\"))\n  sitelon = stringtofloat(str_get_field(site,3,\",\"))\n  O3_obs = stringtofloat(str_get_field(site,4,\",\"))\n\n  obslon = sitelon(:,0)\n  obslat = sitelat(:,0)\n  obsO3 = O3_obs(:,0)\n\n  npts = nrows(0)\n\n  obsO3@_FillValue = -999.\n\n;--- levels for dividing\n  levels_O3  = ispan(0,80,4)\n\n  nlevels = dimsizes(levels_O3)\n\n  colors  = span_color_rgba(\"WhiteBlueGreenYellowRed\",nlevels+1)\n\n  num_distinct_markers = nlevels+1        ; number of distinct markers\n  lat_O3 = new((/num_distinct_markers,npts/),float)\n  lon_O3 = new((/num_distinct_markers,npts/),float)\n  lat_O3 = -999\n  lon_O3 = -999\n\n\n;\n; Group the points according to which range they fall in. At the\n; same time, create the label that we will use later in the labelbar\n;\n  do i = 0, num_distinct_markers-1\n    if (i.eq.0) then\n      indexes_O3 = ind(obsO3(:).lt.levels_O3(0))\n    end if\n    if (i.eq.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.max(levels_O3))\n    end if\n    if (i.gt.0.and.i.lt.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.levels_O3(i-1).and.obsO3(:).lt.levels_O3(i))\n    end if\n\n;\n; Now that we have the set of indexes whose values fall within\n; the given range, take the corresponding lat/lon values and store\n; them, so later we can color this set of markers with the appropriate\n; color.\n;\n    if (.not.any(ismissing(indexes_O3))) then\n      npts_range_O3 = dimsizes(indexes_O3)   ; # of points in this range.\n\n      lat_O3(i,0:npts_range_O3-1) = obslat(indexes_O3)\n      lon_O3(i,0:npts_range_O3-1) = obslon(indexes_O3)\n  ;print(\"O3: \"+npts_range_O3)\n    end if\n\n\n    delete(indexes_O3)            ; Necessary b/c \"indexes\" may be a different\n  end do\n\n  lat_O3@_FillValue = -999\n  lon_O3@_FillValue = -999\n\n  gsres               = True\n  gsres@gsMarkerIndex = 16          ; Use filled dots for markers.\n\n  hollowres           = True\n  hollowres@gsMarkerIndex    = 4\n  hollowres@gsMarkerColor    = \"black\"\n  hollowres@gsMarkerSizeF    = 0.008\n\n;;;;;;;;;   Plot Ozone\n  pname=plot_dir+\"/OBS-FORECAST_O3_\"+rundate+runtime\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  pmid_O3 = new(num_distinct_markers,graphic)\n  hollow_O3 = new(num_distinct_markers,graphic)\n\n  cnres@tiMainString =  pdate+\" \"+runtime+\" UTC O~B~3~N~ (ppbV)\"\n  cnres@cnLevelSelectionMode = \"ManualLevels\"\n  cnres@cnMinLevelValF          = 0.\n  cnres@cnMaxLevelValF          = 80\n  cnres@cnLevelSpacingF         = 4\n\n  ;plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  map = gsn_csm_map(wks,mpres)\n  contour = gsn_csm_contour(wks,o3(it,:,:),cnres)\n  vector  = gsn_csm_vector(wks,UV10(0,it,:,:),UV10(1,it,:,:),res_vc)\n  overlay(map,contour)\n  overlay(map,vector)\n\n  pmid = new(num_distinct_markers,graphic)\n  hollow = new(num_distinct_markers,graphic)\n  do i = 0, num_distinct_markers-1\n    if (.not.ismissing(lat_O3(i,0)))\n      gsres@gsMarkerColor      = colors(i,:)\n      gsres@gsMarkerSizeF      = 0.008\n      gsres@gsMarkerThicknessF = 1\n       pmid(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),gsres)\n       hollow(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),hollowres)\n    end if\n  end do\n\n  draw(map)\n  frame(wks)\n  delete(wks)\n  delete(pmid_O3)\n  delete(hollow_O3)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\n\n\n  delete(pmid)\n  delete(hollow)\n  delete(site)\n  delete(sitename)\n  delete(sitelat)\n  delete(sitelon)\n  delete(O3_obs)\n  delete(obslon)\n  delete(obslat)\n  delete(obsO3)\n  delete([/lon_O3,lat_O3/])\n\nend do\ndelete(res)\n\n;/\n\nend\nprint(\"ncl business is done\")\nexit\nEOF\n\ndays_back=7\n# for regular workflow run, assign false\n# for refreshing all the previous generated gifs, assign true\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') \n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d')\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') \n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d')\n  \n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/\"Airnow_\"$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\n\n  convert -delay 100 $cmaq_folder/plots/OBS*.png $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif\n\n  # copy the result files to permanent location\n  echo \"Copy Airnow gif to permanent location\"\n  cp $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n\ndone\n\nif [ $? -eq 0 ]; then\n    echo \"Generating AirNow images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nelse\n    echo \"Generating AirNow images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nfi\n\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1717542443269,
  "history_end_time" : 1717542443074,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "100001",
  "indicator" : "Running"
},{
  "history_id" : "3c6cdr0us1c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408684,
  "history_end_time" : 1717529408684,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9ms2cs9brrt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408686,
  "history_end_time" : 1717529408686,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "395fup4strq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408688,
  "history_end_time" : 1717529408688,
  "history_notes" : null,
  "history_process" : "pvzabv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zbtxtr3xj7g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408692,
  "history_end_time" : 1717529408692,
  "history_notes" : null,
  "history_process" : "8i9ptn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "shzwzsw1uba",
  "history_input" : "#!/bin/bash\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\n\n# generate images and gif from the NetCDF files\n\necho \"cmaq_folder=\"${cmaq_folder}\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\nmkdir \"${cmaq_folder}/plots\"\n\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport dir_graph=\"${cmaq_folder}/plots\"\n\necho \"Loading NCL\"\nsource /home/zsun/.bashrc\nmodule load ncl\necho \"Loaded NCL\"\n\nrm ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\n\necho \"Drafting \"${cmaq_folder}/geoweaver_plot_daily_O3.ncl\ncat <<EOF >> ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\nprint(\"NCL script successfully begin: \")\n\ndate = getenv(\"YYYYMMDD_POST\")\nd1 = getenv(\"stdate_post\")\nd2 = getenv(\"eddate_post\")\n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\n;print(\"Passed Date: \"+date)\n\n;aconc_dir = getenv(\"postdata_dir\")\ngrid_dir = getenv(\"mcip_dir\")\nplot_dir = getenv(\"dir_graph\")\n\nprint(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\ncdf_file1 = addfile(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\n\nptime = (/\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"/)\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\n;pm25 = cdf_file1->PM25_TOT(:,0,:,:)\n\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\n;print(max(pm25))\n;print(min(pm25))\n;print(avg(pm25))\n\n;print(time)\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; don't advance frame\nres@gsnDraw = False\n;res@gsnSpreadColors = True\nres@lbLabelAutoStride = True\n;res@lbBoxLinesOn = False\nres@pmLabelBarHeightF = 0.1\nres@pmLabelBarWidthF = 0.5\nres@cnFillOn=True\n;res@cnMonoFillPattern=True\n;res@cnMonoLineColor=True\nres@cnLinesOn=False\n;res@pmLabelBarDisplayMode=\"never\"\nres@gsnLeftString  = \"\";\nres@gsnRightString = \"\"\n\nres@mpLimitMode = \"LatLon\"\nres@mpMinLonF = -120 ;min(lon)+0.2\nres@mpMaxLonF = -70 ;max(lon)-0.2\nres@mpMinLatF = 25 ;min(lat)+0.05\nres@mpMaxLatF = 50 ;max(lat)-0.05\nres@mpDataBaseVersion = \"MediumRes\"\n;res@tiMainString = times(it)\nres@mpDataBaseVersion       = \"MediumRes\"\nres@mpDataSetName           = \"Earth..4\"\nres@mpAreaMaskingOn         = True\nres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nres@mpOutlineSpecifiers=\"United States : States\"\nres@mpLandFillColor         = \"white\"\nres@mpInlandWaterFillColor  = \"white\"\nres@mpOceanFillColor        = \"white\"\nres@mpGeophysicalLineColor    = \"Black\"\nres@mpGeophysicalLineThicknessF = 1.5\n\n;res@gsnSpreadColors         = True\nres@lbLabelAutoStride       = True\nres@lbLabelFont             = 25\nres@tiXAxisFont             = 25\nres@pmTickMarkDisplayMode   = \"Always\"\nres@tmXBLabelFont           = 25\nres@tmXBLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmYLLabelFont           = 25\nres@tmYLLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmXTLabelsOn            = False\nres@tmXTLabelFont           = 25\nres@tmXTLabelFontHeightF    = 0.013\nres@tmYRLabelsOn            = False\nres@tmYRLabelFont           = 25\nres@tmYRLabelFontHeightF    = 0.013\n\n\nres@mpProjection           = \"LambertConformal\" ;\"CylindricalEquidistant\"\nres@mpLambertParallel1F    = 33.\nres@mpLambertParallel2F    = 45.\nres@mpLambertMeridianF     = -98.\n\nres@cnLevelSelectionMode = \"ManualLevels\"\nres@cnMinLevelValF          = 0.\nres@cnMaxLevelValF          = 80\nres@cnLevelSpacingF         = 4\n\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  pname=plot_dir+\"/testPlot_\"+pdate+\"_\"+ptime(it)\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  res@tiMainString = pdate+\" \"+ptime(it)+\" UTC O~B~3~N~ Forecast (ppbV)\"\n  plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  draw(plot)\n  frame(wks)\n  delete(wks)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\nend do\ndelete(res)\n\nend\nexit\nEOF\n\necho \"Start to run the NCL script: \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho \"ncl \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho $(date -d '1 day ago' '+%Y%m%d')\n\ndays_back=7\n\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date `date -d \"-2 day ${1}\" +%Y%m%d`\n  #export YYYYMMDD_POST='20220806'\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export stdate_post='2022-08-06'\n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export eddate_post='2022-08-08'\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export stdate_file='20220806'\n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export eddate_file='20220808'\n  stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d')\n  echo \"stdate_file=\"$stdate_file\n  # determine if the prediction netcdf is there\n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/Map_$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything in the folder first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3.ncl\n\n  echo \"Finished \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\n  # convert -delay 100 *.png 20220613_20220614.gif\n  convert -delay 100 $cmaq_folder/plots/testPlot*.png $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif\n  echo \"Converted images to gif\"\n\n  # cp the results to permanent location\n  cp $predict_nc_file $permanent_location/netcdfs/\n  cp $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n  echo \"Moved the generated netcdfs and gifs to permanent locations\"\n  \ndone\n\n\n\nif [ $? -eq 0 ]; then\n    echo \"Generating images/gif Completed Successfully\"\nelse\n    echo \"Generating images/gif Failed!\"\nfi\n\n",
  "history_output" : "cmaq_folder=/groups/ESS/zsun/cmaq\nmkdir: cannot create directory ‘/groups/ESS/zsun/cmaq/plots’: File exists\nLoading NCL\nLoaded NCL\nDrafting /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\nStart to run the NCL script: /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\nncl /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\n20240603\n1 days ago\nstdate_file=20240602\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240602_ML_extracted.nc doesn't exist. Skipping...\n2 days ago\nstdate_file=20240601\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240601_ML_extracted.nc doesn't exist. Skipping...\n3 days ago\nstdate_file=20240531\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240531_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240531.gif exists. Skipping...\n4 days ago\nstdate_file=20240530\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240530_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240530.gif exists. Skipping...\n5 days ago\nstdate_file=20240529\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240529_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240529.gif exists. Skipping...\n6 days ago\nstdate_file=20240528\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240528_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240528.gif exists. Skipping...\n7 days ago\nstdate_file=20240527\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240527_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240527.gif exists. Skipping...\nGenerating images/gif Completed Successfully\n",
  "history_begin_time" : 1717542438955,
  "history_end_time" : 1717542441964,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rf8aumzzxc7",
  "history_input" : "#!/bin/bash\n\ndays_back=40\n\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\ncmaq_gif_location=\"/groups/ESS/share/projects/SWUS3km/graph/12km/\"\n\necho \"start to traverse \"${cmaq_gif_location}\n\nfor i in $(seq 0 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i))\n  # Setting env variables\n  YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  #/groups/ESS/share/projects/SWUS3km/graph/12km/20221108/FORECAST_O3_20221108.gif\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/\"FORECAST_O3_\"$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/obsoverlay/gif/OBS-FORECAST_O3_$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  \ndone\n",
  "history_output" : "start to traverse /groups/ESS/share/projects/SWUS3km/graph/12km/\n0 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240604/FORECAST_O3_20240604.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240604/obsoverlay/gif/OBS-FORECAST_O3_20240604.gif': No such file or directory\n1 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240603/FORECAST_O3_20240603.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240603/obsoverlay/gif/OBS-FORECAST_O3_20240603.gif': No such file or directory\n2 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240602/FORECAST_O3_20240602.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240602/obsoverlay/gif/OBS-FORECAST_O3_20240602.gif': No such file or directory\n3 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240601/FORECAST_O3_20240601.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240601/obsoverlay/gif/OBS-FORECAST_O3_20240601.gif': No such file or directory\n4 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240531/obsoverlay/gif/OBS-FORECAST_O3_20240531.gif': No such file or directory\n5 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240530/obsoverlay/gif/OBS-FORECAST_O3_20240530.gif': No such file or directory\n6 days ago\n7 days ago\n8 days ago\n9 days ago\n10 days ago\n11 days ago\n12 days ago\n13 days ago\n14 days ago\n15 days ago\n16 days ago\n17 days ago\n18 days ago\n19 days ago\n20 days ago\n21 days ago\n22 days ago\n23 days ago\n24 days ago\n25 days ago\n26 days ago\n27 days ago\n28 days ago\n29 days ago\n30 days ago\n31 days ago\n32 days ago\n33 days ago\n34 days ago\n35 days ago\n36 days ago\n37 days ago\n38 days ago\n39 days ago\n40 days ago\n",
  "history_begin_time" : 1717529409545,
  "history_end_time" : 1717529411140,
  "history_notes" : null,
  "history_process" : "nndpw6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "x6ld6p92hlg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717529408722,
  "history_end_time" : 1717529408722,
  "history_notes" : null,
  "history_process" : "slsirb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0vmrsp5man3",
  "history_input" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"rf_prediction_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J rf_prediction       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 8               # Number of CPUs per task (threads)\n#SBATCH --mem=150G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\nprint(\"create and clean the prediction folder\")\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      file_path = os.path.join(testing_path, file_name)\n      testing_df = pd.read_csv(file_path)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      print(\"adding month, day, and hours\")\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[4:6], errors='coerce', downcast='integer')\n      testing_df['day'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[6:8], errors='coerce', downcast='integer')\n      testing_df['hours'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[8:10], errors='coerce', downcast='integer')\n\n      print(testing_df['YYYYMMDDHH'].values[0])\n      print(testing_df['month'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      print(f\"file_dateTime={file_dateTime}\")\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)\n        \nprint(\"Prediction is all done.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/0vmrsp5man3\nwrite the slurm script into rf_prediction_slurm_generated.sh\nsbatch rf_prediction_slurm_generated.sh\njob_id=1943542\nJob 1943542 has finished with state: JobState=COMPLETED\nSlurm job (1943542) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1943542      rf_predic+  COMPLETED      0:0            2024-06-04T19:00:24 2024-06-04T19:06:46 \n1943542.bat+      batch  COMPLETED      0:0 110510020K 2024-06-04T19:00:24 2024-06-04T19:06:46 \n1943542.ext+     extern  COMPLETED      0:0          0 2024-06-04T19:00:24 2024-06-04T19:06:46 \ncreate and clean the prediction folder\nstart to load model\nmodel is loaded\nPrediction is all done.\nAll slurm job for rf_prediction_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1717542021934,
  "history_end_time" : 1717542413933,
  "history_notes" : null,
  "history_process" : "xa1jxg",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dzafz4lmamd",
  "history_input" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"preprocessing_test_netcdf.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J preprocessing_test_netcdf       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# load the prediction_rf.csv into a NetCDF file for visualization\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\ntoday = datetime.today()\nedate = today\nsdate = today - timedelta(days=days_back)\ndays = get_days_list_for_prediction(sdate, edate)\n\nprediction_path = f\"{cmaq_folder}/prediction_files/\"\n\nall_hourly_files = sorted(glob.glob(os.path.join(prediction_path, \"*.csv\")))\n# print(\"overall hourly files: \", all_hourly_files)\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor i in range(len(days)-1):\n  print(days[i])\n  current_day = days[i]\n  next_day = days[i+1]\n  \n  cmaq_cdf_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n  \n  if not os.path.exists(cmaq_cdf_file):\n    print(f\"{cmaq_cdf_file} doesn't exist\")\n    continue\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cdf = xr.open_dataset(cmaq_cdf_file, engine='netcdf4')\n  daily_hourly_files = []\n  for k in real_hour_list:\n    real_hour_value = real_hour_list[k]\n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    #daily_hourly_files.append(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    daily_hourly_files.append(f'{cmaq_folder}/prediction_files/prediction_rf_{day}{turn_2_digits(real_hour_value)}.csv')\n  \n  daily_hourly_files = sorted(daily_hourly_files)\n  #print(\"single day hourly files: \", all_hourly_files[i*24:(i+1)*24])\n  print(\"single day hourly files: \", daily_hourly_files)\n  df_from_each_hourly_file = (pd.read_csv(f) for f in daily_hourly_files)\n  \n  df_csv = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n  reshaped_prediction = df_csv['prediction'].to_numpy().reshape(24, 1, 265, 442).astype(np.float32)\n  print(reshaped_prediction.shape)\n  \n  # retain only two essential variables\n  clean_df_cdf = df_cdf[['O3', 'TFLAG']]\n  print(\"O3 attrs is: \", df_cdf.O3.attrs)\n  \n  # reduce VAR dim to 1\n  new_tflag = df_cdf['TFLAG'].to_numpy()\n  new_tflag = new_tflag[:, 0, :].reshape(24, 1, 2)\n  \n  # Apply changes to data variable in nc file\n  clean_df_cdf['O3'] = (['TSTEP', 'LAY', 'ROW', 'COL'], reshaped_prediction)\n  clean_df_cdf['TFLAG'] = (['TSTEP', 'VAR', 'DATE-TIME'], new_tflag)\n\n  clean_df_cdf.O3.attrs = df_cdf.O3.attrs\n  clean_df_cdf.TFLAG.attrs = df_cdf.TFLAG.attrs\n  clean_df_cdf.attrs['VGLVLS'] = \"1.f, 0.9941f\"\n  clean_df_cdf.attrs['VAR-LIST'] = \"O3              \"\n#   create_and_clean_folder(f\"{cmaq_folder}/prediction_nc_files\")\n  clean_df_cdf.to_netcdf(target_cdf_file,)\n\n  print(f'Saved updated netCDF file: {target_cdf_file}')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"",
  "history_output" : "write the slurm script into preprocessing_test_netcdf.sh\nsbatch preprocessing_test_netcdf.sh\njob_id=1943548\nJob 1943548 has finished with state: JobState=COMPLETED\nSlurm job (1943548) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1943548      preproces+  COMPLETED      0:0            2024-06-04T19:06:57 2024-06-04T19:07:04 \n1943548.bat+      batch  COMPLETED      0:0          0 2024-06-04T19:06:57 2024-06-04T19:07:04 \n1943548.ext+     extern  COMPLETED      0:0          0 2024-06-04T19:06:57 2024-06-04T19:07:04 \n20240405\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240405_ML_extracted.nc already exists\n20240406\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240406_ML_extracted.nc already exists\n20240407\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240407_ML_extracted.nc already exists\n20240408\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240408_ML_extracted.nc already exists\n20240409\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240409_ML_extracted.nc already exists\n20240410\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240410_ML_extracted.nc already exists\n20240411\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240411_ML_extracted.nc already exists\n20240412\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240412_ML_extracted.nc already exists\n20240413\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240413_ML_extracted.nc already exists\n20240414\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240414_ML_extracted.nc already exists\n20240415\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240415_ML_extracted.nc already exists\n20240416\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240416_ML_extracted.nc already exists\n20240417\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240417_ML_extracted.nc already exists\n20240418\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240418_ML_extracted.nc already exists\n20240419\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240419_ML_extracted.nc already exists\n20240420\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240420_ML_extracted.nc already exists\n20240421\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240421_ML_extracted.nc already exists\n20240422\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240422_ML_extracted.nc already exists\n20240423\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240423_ML_extracted.nc already exists\n20240424\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240424_ML_extracted.nc already exists\n20240425\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240425_ML_extracted.nc already exists\n20240426\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240426_ML_extracted.nc already exists\n20240427\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240427_ML_extracted.nc already exists\n20240428\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240428_ML_extracted.nc already exists\n20240429\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240429_ML_extracted.nc already exists\n20240430\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240430_ML_extracted.nc already exists\n20240501\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240501_ML_extracted.nc already exists\n20240502\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240502_ML_extracted.nc already exists\n20240503\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240503_ML_extracted.nc already exists\n20240504\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240504_ML_extracted.nc already exists\n20240505\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240505_ML_extracted.nc already exists\n20240506\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240506_ML_extracted.nc already exists\n20240507\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240507_ML_extracted.nc already exists\n20240508\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240508_ML_extracted.nc already exists\n20240509\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240509_ML_extracted.nc already exists\n20240510\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240510_ML_extracted.nc already exists\n20240511\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240511_ML_extracted.nc already exists\n20240512\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240512_ML_extracted.nc already exists\n20240513\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240513_ML_extracted.nc already exists\n20240514\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240514_ML_extracted.nc already exists\n20240515\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240515_ML_extracted.nc already exists\n20240516\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240516_ML_extracted.nc already exists\n20240517\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240517_ML_extracted.nc already exists\n20240518\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240518_ML_extracted.nc already exists\n20240519\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240519_ML_extracted.nc already exists\n20240520\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240520_ML_extracted.nc already exists\n20240521\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240521_ML_extracted.nc already exists\n20240522\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240522_ML_extracted.nc already exists\n20240523\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240523_ML_extracted.nc already exists\n20240524\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240524_ML_extracted.nc already exists\n20240525\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240525_ML_extracted.nc already exists\n20240526\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240526_ML_extracted.nc already exists\n20240527\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240527_ML_extracted.nc already exists\n20240528\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240528_ML_extracted.nc already exists\n20240529\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240529_ML_extracted.nc already exists\n20240530\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240530_ML_extracted.nc already exists\n20240531\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240531_ML_extracted.nc already exists\n20240601\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240601.nc doesn't exist\n20240602\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240602.nc doesn't exist\n20240603\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240603.nc doesn't exist\nAll slurm job for preprocessing_test_netcdf.sh finishes.\n",
  "history_begin_time" : 1717542415643,
  "history_end_time" : 1717542437061,
  "history_notes" : null,
  "history_process" : "klx3sc",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p841v288qd4",
  "history_input" : "#!/bin/bash\n\n\necho \"start to run processing_test_data_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"processing_test_data_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J processing_test_data       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)  # don't clean folder anymore\n#os.makedirs(test_folder, exist_ok=True)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file, engine='netcdf4')\n  \n  # read mcip results \n  mcip_file = \"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file, engine='netcdf4')\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\", engine='netcdf4')\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    hourly_target_file = f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    if os.path.exists(hourly_target_file):\n      print(f\"Hourly csv file {hourly_target_file} already exist\")\n      continue\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D', engine='netcdf4')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(hourly_target_file, index=False)\n\nprint('Done with preparing testing data!')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "start to run processing_test_data_generated.sh\n/home/zsun/gw-workspace/p841v288qd4\nwrite the slurm script into processing_test_data_generated.sh\nsbatch processing_test_data_generated.sh\njob_id=1943496\nJob 1943496 has finished with state: JobState=COMPLETED\nSlurm job (1943496) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1943496      processin+  COMPLETED      0:0            2024-06-04T19:00:02 2024-06-04T19:00:12 \n1943496.bat+      batch  COMPLETED      0:0        96K 2024-06-04T19:00:02 2024-06-04T19:00:12 \n1943496.ext+     extern  COMPLETED      0:0          0 2024-06-04T19:00:02 2024-06-04T19:00:12 \nGetting data for: 20240405\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240405_ML_extracted.nc already exists\nGetting data for: 20240406\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240406_ML_extracted.nc already exists\nGetting data for: 20240407\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240407_ML_extracted.nc already exists\nGetting data for: 20240408\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240408_ML_extracted.nc already exists\nGetting data for: 20240409\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240409_ML_extracted.nc already exists\nGetting data for: 20240410\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240410_ML_extracted.nc already exists\nGetting data for: 20240411\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240411_ML_extracted.nc already exists\nGetting data for: 20240412\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240412_ML_extracted.nc already exists\nGetting data for: 20240413\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240413_ML_extracted.nc already exists\nGetting data for: 20240414\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240414_ML_extracted.nc already exists\nGetting data for: 20240415\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240415_ML_extracted.nc already exists\nGetting data for: 20240416\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240416_ML_extracted.nc already exists\nGetting data for: 20240417\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240417_ML_extracted.nc already exists\nGetting data for: 20240418\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240418_ML_extracted.nc already exists\nGetting data for: 20240419\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240419_ML_extracted.nc already exists\nGetting data for: 20240420\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240420_ML_extracted.nc already exists\nGetting data for: 20240421\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240421_ML_extracted.nc already exists\nGetting data for: 20240422\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240422_ML_extracted.nc already exists\nGetting data for: 20240423\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240423_ML_extracted.nc already exists\nGetting data for: 20240424\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240424_ML_extracted.nc already exists\nGetting data for: 20240425\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240425_ML_extracted.nc already exists\nGetting data for: 20240426\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240426_ML_extracted.nc already exists\nGetting data for: 20240427\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240427_ML_extracted.nc already exists\nGetting data for: 20240428\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240428_ML_extracted.nc already exists\nGetting data for: 20240429\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240429_ML_extracted.nc already exists\nGetting data for: 20240430\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240430_ML_extracted.nc already exists\nGetting data for: 20240501\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240501_ML_extracted.nc already exists\nGetting data for: 20240502\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240502_ML_extracted.nc already exists\nGetting data for: 20240503\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240503_ML_extracted.nc already exists\nGetting data for: 20240504\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240504_ML_extracted.nc already exists\nGetting data for: 20240505\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240505_ML_extracted.nc already exists\nGetting data for: 20240506\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240506_ML_extracted.nc already exists\nGetting data for: 20240507\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240507_ML_extracted.nc already exists\nGetting data for: 20240508\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240508_ML_extracted.nc already exists\nGetting data for: 20240509\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240509_ML_extracted.nc already exists\nGetting data for: 20240510\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240510_ML_extracted.nc already exists\nGetting data for: 20240511\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240511_ML_extracted.nc already exists\nGetting data for: 20240512\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240512_ML_extracted.nc already exists\nGetting data for: 20240513\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240513_ML_extracted.nc already exists\nGetting data for: 20240514\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240514_ML_extracted.nc already exists\nGetting data for: 20240515\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240515_ML_extracted.nc already exists\nGetting data for: 20240516\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240516_ML_extracted.nc already exists\nGetting data for: 20240517\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240517_ML_extracted.nc already exists\nGetting data for: 20240518\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240518_ML_extracted.nc already exists\nGetting data for: 20240519\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240519_ML_extracted.nc already exists\nGetting data for: 20240520\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240520_ML_extracted.nc already exists\nGetting data for: 20240521\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240521_ML_extracted.nc already exists\nGetting data for: 20240522\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240522_ML_extracted.nc already exists\nGetting data for: 20240523\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240523_ML_extracted.nc already exists\nGetting data for: 20240524\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240524_ML_extracted.nc already exists\nGetting data for: 20240525\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240525_ML_extracted.nc already exists\nGetting data for: 20240526\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240526_ML_extracted.nc already exists\nGetting data for: 20240527\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240527_ML_extracted.nc already exists\nGetting data for: 20240528\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240528_ML_extracted.nc already exists\nGetting data for: 20240529\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240529_ML_extracted.nc already exists\nGetting data for: 20240530\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240530_ML_extracted.nc already exists\nGetting data for: 20240531\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240531_ML_extracted.nc already exists\nGetting data for: 20240601\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240601.nc doesn't exist\nGetting data for: 20240602\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240602.nc doesn't exist\nGetting data for: 20240603\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240603.nc doesn't exist\nGetting data for: 20240604\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240604.nc doesn't exist\nGetting data for: 20240605\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240605.nc doesn't exist\nGetting data for: 20240606\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240606.nc doesn't exist\nGetting data for: 20240607\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240607.nc doesn't exist\nGetting data for: 20240608\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240608.nc doesn't exist\nGetting data for: 20240609\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240609.nc doesn't exist\nGetting data for: 20240610\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240610.nc doesn't exist\nDone with preparing testing data!\nAll slurm job for processing_test_data_generated.sh finishes.\n",
  "history_begin_time" : 1717529409545,
  "history_end_time" : 1717542020575,
  "history_notes" : null,
  "history_process" : "z4du0c",
  "host_id" : "100001",
  "indicator" : "Done"
}]
