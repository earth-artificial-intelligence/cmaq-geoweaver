[{
  "history_id" : "krzaxxh4vvf",
  "history_input" : "# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\n# Required packages to run this process.\nrequired = {'pandas','pathlib','sklearn','numpy','keras','tensorflow','tensorflow-gpu','autokeras','kaleido','glob2','scipy','netCDF4','xarray'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n    print(\"Packages missing and will be installed: \", missing)\n    python = sys.executable\n    subprocess.check_call(\n        [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models','cmaq/observation']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "history_output" : "Packages missing and will be installed:  {'netCDF4'}\n",
  "history_begin_time" : 1648912204905,
  "history_end_time" : 1648912261426,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "qoa2163pnso",
  "history_input" : "# Write first python in Geoweaver\n# Write first python in Geoweaver\n# Write first python in Geoweaver\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 4, 5)   # start date\nedate = date(2021, 4, 6)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\naa,bb,cc,dd,ee,ff,gg,hh,ii,jj,kk,ll,mm,nn,oo1,pp,qq,rr,ss=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n#ff=[]\n# k = time dimension - start from 12 to match with data\nt = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in days:\n  # read cmaq results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20210315 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  else:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# O3 variable\n  \t# O3 variable\n      oo=df.variables['O3'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      aa.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      bb.append(o3tp)\n      # CO\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      cc.append(o3tp)\n       # PM25_EC\n      oo=df.variables['PM25_EC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      dd.append(o3tp)\n      # PM25_CO\n      oo=df.variables['PM25_OC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ee.append(o3tp)\n      \n      \n  # read emission results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20191231 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_nobeis_2016fh_16j.ncf\")\n  elif int(i)==20220303:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n\n# set todays date if they don't change dataformate    \n#  else if int(i)>=20220313 and int(i)<=int(today):\n  elif int(i)>=20220313 and int(i)<=20220331:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ff.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      gg.append(o3tp)\n      # NO\n      oo=df.variables['NO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      hh.append(o3tp)  \n      \n# read mcip results \n# date must be later of 20210101\n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/\"+\"METCRO2D_\"+i+\".nc\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['PRSFC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ii.append(o3tp)\n  \t# NO2\n      oo=df.variables['PBL'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      jj.append(o3tp)\n      # NO\n      oo=df.variables['TEMP2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      kk.append(o3tp)\n            # NO\n      oo=df.variables['WSPD10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ll.append(o3tp)\n            # NO\n      oo=df.variables['WDIR10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      mm.append(o3tp)\n            # NO\n      oo=df.variables['WSTAR'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      nn.append(o3tp)\n            # NO\n      oo=df.variables['RGRND'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      oo1.append(o3tp)\n            # NO\n      oo=df.variables['RN'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      pp.append(o3tp)\n        \t# NO2\n      oo=df.variables['RC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      qq.append(o3tp)\n        \t# NO2\n      oo=df.variables['CFRAC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      rr.append(o3tp)\n      \n      \ncmaq_O3=list(np.concatenate(aa).flat) \ncmaq_NO2=list(np.concatenate(bb).flat) \ncmaq_CO=list(np.concatenate(cc).flat) \ncmaq_PM25_EC=list(np.concatenate(dd).flat) \ncmaq_PM25_CO=list(np.concatenate(ee).flat)\nCO_emi=list(np.concatenate(ff).flat) \nNO2_emi=list(np.concatenate(gg).flat) \nNO_emi=list(np.concatenate(hh).flat) \nPRSFC=list(np.concatenate(ii).flat) \nPBL=list(np.concatenate(jj).flat) \nTEMP2=list(np.concatenate(kk).flat) \nWSPD10=list(np.concatenate(ll).flat) \nWDIR10=list(np.concatenate(mm).flat) \nWSTAR=list(np.concatenate(nn).flat) \nRGRND=list(np.concatenate(oo1).flat) \nRN=list(np.concatenate(pp).flat)\nRC=list(np.concatenate(qq).flat)\nCFRAC=list(np.concatenate(rr).flat)\n\n## selecting lat and long\ndf = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\nlat_1 = df.variables['LAT'][:].values[0,0]\nlat_flt=np.ravel(lat_1)\n# need to manipulate 48 values if the next day data is available\nLAT=np.tile(lat_flt,48)\n# long\nlon_1 = df.variables['LON'][:].values[0,0]\nlon_flt=np.ravel(lon_1)\n# need to manipulate 48 values if the next day data is available\nLON=np.tile(lon_flt,48)\n# creating dataframe\n\n## creatime date-time dimension\n# date-time dimension for today\ntime0=[]\nt = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n  for j in t:\n    time_0=np.full((265,442),i+j)\n    time0.append(time_0)\nYYMMDDHH=list(np.concatenate(time0).flat)  \n\n\n\n# saving variables\ndat=pd.DataFrame({'Latitude':LAT,'Longitude':LON,'YYYYMMDDHH':YYMMDDHH,'CMAQ12KM_O3(ppb)':cmaq_O3,'CMAQ12KM_NO2(ppb)':cmaq_NO2,'CMAQ12KM_CO(ppm)':cmaq_CO,'CMAQ_EC(ug/m3)':cmaq_PM25_EC,'CMAQ_OC(ug/m3)':cmaq_PM25_CO,'NO2(moles/s)':NO2_emi,'CO(moles/s)':CO_emi,'NO(moles/s)':NO_emi,'PRSFC(Pa)':PRSFC,'PBL(m)':PBL,'TEMP2(K)':TEMP2,'WSPD10(m/s)':WSPD10,'WDIR10(degree)':WDIR10,'WSTAR(m/s)':WSTAR,'RGRND(W/m2)':RGRND,'RN(cm)':RN,'RC(cm)':RC,'CFRAC':CFRAC})\nprint(dat.head())\ndat.to_csv(home+'/cmaq/training_data.csv',index=False)\n",
  "history_output" : "    Latitude   Longitude  YYYYMMDDHH  ...  RN(cm)  RC(cm)  CFRAC\n0  21.829086 -120.620789  2021040512  ...     0.0     0.0    0.0\n1  21.855751 -120.512497  2021040512  ...     0.0     0.0    0.0\n2  21.882309 -120.404144  2021040512  ...     0.0     0.0    0.0\n3  21.908745 -120.295715  2021040512  ...     0.0     0.0    0.0\n4  21.935051 -120.187225  2021040512  ...     0.0     0.0    0.0\n\n[5 rows x 21 columns]\n",
  "history_begin_time" : 1648912261899,
  "history_end_time" : 1648912683463,
  "history_notes" : null,
  "history_process" : "hhy7tc",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "mtt7cuvkt7m",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(home+\"/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "/home/mislam25/gw-workspace/mtt7cuvkt7m/training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n/home/mislam25/gw-workspace/mtt7cuvkt7m/training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n/home/mislam25/gw-workspace/mtt7cuvkt7m/training_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n/home/mislam25/gw-workspace/mtt7cuvkt7m/training_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n/home/mislam25/gw-workspace/mtt7cuvkt7m/training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1648912763277,
  "history_end_time" : 1648912804520,
  "history_notes" : null,
  "history_process" : "tzvs1h",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "8qcp8zjhtoc",
  "history_input" : "# Write first python in Geoweaver\n# Write first python in Geoweaver\n# Write first python in Geoweaver\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 4, 5)   # start date\nedate = date(2021, 4, 6)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n  for t in time:\n    files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n    with open(files, 'r') as file:\n      text = file.read()\n    new_string = text.replace('\"', '')\n\n    outF = open(\"/home/mislam25/cmaq/observation/AQF5X_Hourly_\"+i+t+\".txt\", \"w\")\n    for line in new_string:\n      # write line to output file\n      outF.write(line)\n    outF.close()\n\n",
  "history_output" : "",
  "history_begin_time" : 1648912683810,
  "history_end_time" : 1648912739100,
  "history_notes" : null,
  "history_process" : "q43bxd",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "2ch244fmk4o",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 4, 5)   # start date\nedate = date(2021, 4, 6)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      df=pd.read_csv(file, delimiter=\" \",header=0,skiprows=1)\n      df['YYYYMMDDHH']=d+t\n      merged.append(df)\ndata_frame = pd.concat(merged)\ndata_frame = data_frame.replace(',','', regex=True)\n\n\n# dropping unnecessary variables\ndata_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\n# Changing columns name with index number\n#mapping = {data_frame.columns[0]: 'StationID', data_frame.columns[1]: 'Latitude',data_frame.columns[2]: 'Longitude',data_frame.columns[3]: 'AirNOW_O3'}\nmapping = {data_frame.columns[0]: 'StationID',data_frame.columns[1]: 'Latitude',data_frame.columns[2]: 'Longitude',data_frame.columns[3]: 'AirNOW_O3'}\ndf = data_frame.rename(columns=mapping)\n\ndf.to_csv(home+\"/cmaq/observation.csv\",index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1648912739792,
  "history_end_time" : 1648912763139,
  "history_notes" : null,
  "history_process" : "fya6lw",
  "host_id" : "vneyuq",
  "indicator" : "Done"
}]
