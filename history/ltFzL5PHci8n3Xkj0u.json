[{
  "history_id" : "wu7d7a615kc",
  "history_input" : "# get all of the cmaq model output variables and save to hourly csvs\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 5)   # end date\n\ndays = get_days_list(sdate, edate)\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntraining_input_folder = f\"{cmaq_folder}/training_input_hourly/\"\ncreate_and_clean_folder(training_input_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  df_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\"_extracted.nc\")\n  \n  # read mcip results \n  df_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\")\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value >= 0 and real_hour_value <=11:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n    # CMAQ data\n    # O3 variable\n    \n    o3=df_cmaq.variables['O3'][:].values[k,0]\n    cmaq_O3=list(np.ravel(o3).transpose())  \n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k,0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k,0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k,0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k,0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k,0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    lat_flt=np.ravel(lat)\n    LAT=np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    \n    filename = f'{cmaq_folder}training_input_hourly/train_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    print(f'Saving file: train_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(filename,index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220805\nSaving file: train_data_20220805_12.csv\nSaving file: train_data_20220805_13.csv\nSaving file: train_data_20220805_14.csv\nSaving file: train_data_20220805_15.csv\nSaving file: train_data_20220805_16.csv\nSaving file: train_data_20220805_17.csv\nSaving file: train_data_20220805_18.csv\nSaving file: train_data_20220805_19.csv\nSaving file: train_data_20220805_20.csv\nSaving file: train_data_20220805_21.csv\nSaving file: train_data_20220805_22.csv\nSaving file: train_data_20220805_23.csv\nSaving file: train_data_20220806_00.csv\nSaving file: train_data_20220806_01.csv\nSaving file: train_data_20220806_02.csv\nSaving file: train_data_20220806_03.csv\nSaving file: train_data_20220806_04.csv\nSaving file: train_data_20220806_05.csv\nSaving file: train_data_20220806_06.csv\nSaving file: train_data_20220806_07.csv\nSaving file: train_data_20220806_08.csv\nSaving file: train_data_20220806_09.csv\nSaving file: train_data_20220806_10.csv\nSaving file: train_data_20220806_11.csv\nDone!\n",
  "history_begin_time" : 1660777334919,
  "history_end_time" : 1660778171056,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "decgp2l5vp9",
  "history_input" : "# get all the airnow station data and save to csvs as well\nimport glob, os\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 5)   # end date\ndays = get_days_list(sdate, edate)\n\nobservation_folder = f\"{cmaq_folder}observation/\"\ncreate_and_clean_folder(observation_folder)\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in range(len(days)-1):\n  current_day = days[i]\n  next_day = days[i+1]\n  for x in range(len(time)):\n    if x >= 12:\n      i = next_day\n    else:\n      i = current_day\n    t = time[x]\n    files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n    \n    with open(files, 'r') as file:\n      text = file.read()\n      new_string = text.replace('\"', '')\n      out_file = f\"{observation_folder}/AQF5X_Hourly_{i}{t}.txt\"\n      print(\"Saving to :\", out_file)\n      outF = open(out_file, \"w\")\n      for line in new_string:\n        outF.write(line)\n      \n      outF.close()",
  "history_output" : "Saving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080512.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080513.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080514.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080515.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080516.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080517.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080518.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080519.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080520.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080521.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080522.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080523.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080600.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080601.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080602.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080603.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080604.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080605.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080606.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080607.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080608.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080609.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080610.txt\nSaving to : /groups/ESS/aalnaim/cmaq/observation//AQF5X_Hourly_2022080611.txt\n",
  "history_begin_time" : 1660777453318,
  "history_end_time" : 1660778171058,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "jo0jgcmats5",
  "history_input" : "# take all the airnow observation csvs and merge into one observation.csv\n\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 5)   # end date\ndays = get_days_list(sdate, edate)\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  for y in range(len(time)):\n    t = time[y]\n    if y>=12:\n      d = next_day\n    else:\n      d = current_day\n    files=glob.glob(f\"{cmaq_folder}/observation/AQF5X_Hourly_{d}{t}.txt\")\n    for file in files:\n      print(file)\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # constructs an array for each hour of each day with length of data from total stations available\n      date_time.append(dt)\n      merged.append(data)\n            \ndata_frame = np.concatenate(merged)\n\n# This gets the first 4 columns in the observation file (AQSID, Latitude, Longitude, OZONE(ppb))\ndata_frame = np.delete(data_frame, np.s_[4:9], axis=1) \n\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\ndff=df.replace(',','', regex=True)\n\ndt = np.concatenate(date_time)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(f\"{cmaq_folder}/observation/observation.csv\",index=False)",
  "history_output" : "/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080512.txt\n2022080512\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080513.txt\n2022080513\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080514.txt\n2022080514\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080515.txt\n2022080515\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080516.txt\n2022080516\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080517.txt\n2022080517\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080518.txt\n2022080518\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080519.txt\n2022080519\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080520.txt\n2022080520\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080521.txt\n2022080521\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080522.txt\n2022080522\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080523.txt\n2022080523\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080600.txt\n2022080600\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080601.txt\n2022080601\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080602.txt\n2022080602\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080603.txt\n2022080603\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080604.txt\n2022080604\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080605.txt\n2022080605\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080606.txt\n2022080606\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080607.txt\n2022080607\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080608.txt\n2022080608\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080609.txt\n2022080609\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080610.txt\n2022080610\n/groups/ESS/aalnaim/cmaq//observation/AQF5X_Hourly_2022080611.txt\n2022080611\n",
  "history_begin_time" : 1660777479477,
  "history_end_time" : 1660778171059,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "tuc8us2b2n5",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660777500674,
  "history_end_time" : 1660778171060,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "qaivcw7ocoq",
  "history_input" : "# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport glob, os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\nall_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nfilename = f'{cmaq_folder}/models/rf_pycaret_o3_only.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nfor testing_df in df_from_each_hourly_file:\n#   print(testing_df['YYYYMMDDHH'].values[0])\n  file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n  X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude','CMAQ12KM_NO2(ppb)', 'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'CO(moles/s)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)','WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day', 'hours'],axis=1)\n\n# # making prediction\n  pred = loaded_model.predict(X)\n\n# adding prediction values to test dataset\n  testing_df['prediction'] = pred.tolist()\n\n  testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n# saving the dataset into local drive\n  print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n  testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)",
  "history_output" : "Saving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080608.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080601.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080606.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080610.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080521.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080514.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080513.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080607.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080600.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080609.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080512.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080515.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080611.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080520.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080516.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080518.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080523.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080603.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080604.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080522.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080519.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080517.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080605.csv\nSaving: /groups/ESS/aalnaim/cmaq//prediction_files/prediction_rf_2022080602.csv\n",
  "history_begin_time" : 1660777768472,
  "history_end_time" : 1660778171061,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "wkv5353uv4u",
  "history_input" : "# load the prediction_rf.csv into a NetCDF file for visualization\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 5)   # end date\ndays = get_days_list(sdate, edate)\n\nprediction_path = f\"{cmaq_folder}prediction_files/\"\n\nall_hourly_files = sorted(glob.glob(os.path.join(prediction_path, \"*.csv\")))\nprint(\"overall hourly files: \", all_hourly_files)\n\nfor i in range(len(days)-1):\n  print(days[i])\n  \n  df_cdf = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+days[i+1]+\"_extracted.nc\")\n  # retain O3 variable only in cdf file. Drop all others.\n#   df_cdf = df_cdf[[\"O3\"]]\n  \n  print(\"single day hourly files: \", all_hourly_files[i*24:(i+1)*24])\n  df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files[i*24:(i+1)*24])\n  \n  df_csv = pd.concat(df_from_each_hourly_file, ignore_index=True)\n  \n  # Reshape \"prediction/Latitude/Longitude\" columns to (TSTEP, ROW, COL), these lines will reshape data into (24, 265, 442)\n  reshaped_prediction = df_csv['prediction'].to_numpy().reshape(24, 265, 442)\n  print(reshaped_prediction.shape)\n  \n  # Remove \"LAY\" Dimension in O3 variable already in nc file.\n  reduced_dim = df_cdf['O3'].sel(LAY=1, drop=True)\n\n  # Swap values from original nc file with new prediction data\n  reduced_dim.values = reshaped_prediction\n\n  # Apply changes to data variable in nc file\n  df_cdf['O3'] = (['TSTEP', 'ROW', 'COL'], reshaped_prediction)\n\n#   create_and_clean_folder(f\"{cmaq_folder}/prediction_nc_files\")\n  df_cdf.to_netcdf(f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+days[i]+'_ML_extracted.nc')\n\n  print(f'Saved updated netCDF file: {cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+days[i]+'_ML_extracted.nc')",
  "history_output" : "overall hourly files:  ['/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080512.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080513.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080514.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080515.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080516.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080517.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080518.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080519.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080520.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080521.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080522.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080523.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080600.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080601.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080602.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080603.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080604.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080605.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080606.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080607.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080608.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080609.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080610.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080611.csv']\n20220805\nsingle day hourly files:  ['/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080512.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080513.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080514.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080515.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080516.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080517.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080518.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080519.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080520.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080521.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080522.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080523.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080600.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080601.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080602.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080603.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080604.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080605.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080606.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080607.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080608.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080609.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080610.csv', '/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_2022080611.csv']\n(24, 265, 442)\nSaved updated netCDF file: /groups/ESS/aalnaim/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20220805_ML_extracted.nc\n",
  "history_begin_time" : 1660777832280,
  "history_end_time" : 1660778171062,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "nh6shs45x4m",
  "history_input" : "# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n\nsdate = date(2022, 8, 5)   # start date\nedate = date(2022, 8, 5)   # end date\ndays = get_days_list(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  df_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\"_extracted.nc\")\n  \n  # read mcip results \n  df_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\")\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv',index=False)\n\nprint('Done with preparing testing data!')",
  "history_output" : "Getting data for: 20220805\nlat shape (265, 442)\nSaving file: test_data_20220805_12.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_13.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_14.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_15.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_16.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_17.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_18.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_19.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_20.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_21.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_22.csv\nlat shape (265, 442)\nSaving file: test_data_20220805_23.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_00.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_01.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_02.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_03.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_04.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_05.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_06.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_07.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_08.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_09.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_10.csv\nlat shape (265, 442)\nSaving file: test_data_20220806_11.csv\nDone with preparing testing data!\n",
  "history_begin_time" : 1660777590641,
  "history_end_time" : 1660778171063,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "pv7r6fyxiok",
  "history_input" : "# merge all hourly testing data into daily files\n\nimport pandas as pd\nimport glob\nimport os\nfrom pathlib import Path\nfrom cmaq_ai_utils import *\n\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\nall_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))   \n# advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n# dropping unnecessary variables\ncmaq['YYYYMMDDHH'] = cmaq['YYYYMMDDHH'].map(str)\ncmaq['month'] = cmaq['YYYYMMDDHH'].str[4:6]\ncmaq['day'] = cmaq['YYYYMMDDHH'].str[6:8]\ncmaq['hours'] = cmaq['YYYYMMDDHH'].str[8:10]\n\nremove_file(f\"{testing_path}/testing.csv\")\ncmaq.to_csv(f\"{testing_path}/testing.csv\",index=False)\n\nprint('Done with generating testing.csv!')",
  "history_output" : "remove old files/groups/ESS/aalnaim/cmaq//testing_input_hourly/testing.csv\nDone with generating testing.csv!\n",
  "history_begin_time" : 1660777711434,
  "history_end_time" : 1660778171064,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "7a63phro2o0",
  "history_input" : "#!/bin/bash\n# generate images and gif from the NetCDF files\n\ncmaq_folder=\"/groups/ESS/aalnaim/cmaq\"\nmkdir $cmaq_folder\"/plots\"\nrm $cmaq_folder/plots/* # clean everything first\n\necho $(date -d '2 day ago' '+%Y%m%d')\n# Setting env variables\n#export YYYYMMDD_POST=$(date -d '2 day ago' '+%Y%m%d') #This needs to be auto date `date -d \"-2 day ${1}\" +%Y%m%d`\nexport YYYYMMDD_POST='20220805'\n#export stdate_post=$(date -d '2 day ago' '+%Y-%m-%d') #This needs to be auto date\nexport stdate_post='2022-08-05'\n#export eddate_post=$(date -d '1 day ago' '+%Y-%m-%d') #This needs to be auto date\nexport eddate_post='2022-08-06'\n\n#export stdate_file=$(date -d '2 day ago' '+%Y%m%d') #This needs to be auto date\nexport stdate_file='20220805'\n#export eddate_file=$(date -d '1 day ago' '+%Y%m%d') #This needs to be auto date\nexport eddate_file='20220806'\n\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport dir_graph=$cmaq_folder\"/plots\"\n\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_plot_daily_O3.ncl\ncat <<EOF >> $cmaq_folder/geoweaver_plot_daily_O3.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\")\nd1 = getenv(\"stdate_post\")\nd2 = getenv(\"eddate_post\")\n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\n;print(\"Passed Date: \"+date)\n\naconc_dir = getenv(\"postdata_dir\")\ngrid_dir = getenv(\"mcip_dir\")\nplot_dir = getenv(\"dir_graph\")\n\nprint(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\n\nptime = (/\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"/)\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,:,:) ;ppb\n;pm25 = cdf_file1->PM25_TOT(:,0,:,:)\n\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\n;print(max(pm25))\n;print(min(pm25))\n;print(avg(pm25))\n\n;print(time)\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; don't advance frame\nres@gsnDraw = False\n;res@gsnSpreadColors = True\nres@lbLabelAutoStride = True\n;res@lbBoxLinesOn = False\nres@pmLabelBarHeightF = 0.1\nres@pmLabelBarWidthF = 0.5\nres@cnFillOn=True\n;res@cnMonoFillPattern=True\n;res@cnMonoLineColor=True\nres@cnLinesOn=False\n;res@pmLabelBarDisplayMode=\"never\"\nres@gsnLeftString  = \"\";\nres@gsnRightString = \"\"\n\nres@mpLimitMode = \"LatLon\"\nres@mpMinLonF = -120 ;min(lon)+0.2\nres@mpMaxLonF = -70 ;max(lon)-0.2\nres@mpMinLatF = 25 ;min(lat)+0.05\nres@mpMaxLatF = 50 ;max(lat)-0.05\nres@mpDataBaseVersion = \"MediumRes\"\n;res@tiMainString = times(it)\nres@mpDataBaseVersion       = \"MediumRes\"\nres@mpDataSetName           = \"Earth..4\"\nres@mpAreaMaskingOn         = True\nres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nres@mpOutlineSpecifiers=\"United States : States\"\nres@mpLandFillColor         = \"white\"\nres@mpInlandWaterFillColor  = \"white\"\nres@mpOceanFillColor        = \"white\"\nres@mpGeophysicalLineColor    = \"Black\"\nres@mpGeophysicalLineThicknessF = 1.5\n\n;res@gsnSpreadColors         = True\nres@lbLabelAutoStride       = True\nres@lbLabelFont             = 25\nres@tiXAxisFont             = 25\nres@pmTickMarkDisplayMode   = \"Always\"\nres@tmXBLabelFont           = 25\nres@tmXBLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmYLLabelFont           = 25\nres@tmYLLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmXTLabelsOn            = False\nres@tmXTLabelFont           = 25\nres@tmXTLabelFontHeightF    = 0.013\nres@tmYRLabelsOn            = False\nres@tmYRLabelFont           = 25\nres@tmYRLabelFontHeightF    = 0.013\n\n\nres@mpProjection           = \"LambertConformal\" ;\"CylindricalEquidistant\"\nres@mpLambertParallel1F    = 33.\nres@mpLambertParallel2F    = 45.\nres@mpLambertMeridianF     = -98.\n\nres@cnLevelSelectionMode = \"ManualLevels\"\nres@cnMinLevelValF          = 0.\nres@cnMaxLevelValF          = 80\nres@cnLevelSpacingF         = 4\n\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  pname=plot_dir+\"/testPlot_\"+pdate+\"_\"+ptime(it)\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  res@tiMainString = pdate+\" \"+ptime(it)+\" UTC O~B~3~N~ Forecast (ppbV) [ML]\"\n  plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  draw(plot)\n  frame(wks)\n  delete(wks)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\nend do\ndelete(res)\n\nend\nEOF\n\n\nncl $cmaq_folder/geoweaver_plot_daily_O3.ncl\n\n# convert -delay 100 *.png 20220613_20220614.gif\nconvert -delay 100 $cmaq_folder/plots/testPlot*.png $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif\n\nif [ $? -eq 0 ]; then\n    echo \"Generating images/gif Completed Successfully\"\nelse\n    echo \"Generating images/gif Failed!\"\nfi\n",
  "history_output" : "mkdir: cannot create directory ‘/groups/ESS/aalnaim/cmaq/plots’: File exists\nrm: cannot remove '/groups/ESS/aalnaim/cmaq/plots/*': No such file or directory\n20220815\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t/groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20220805_ML_extracted.nc\n(0)\t24 265 442\n(0)\t96.84999999999999\n(0)\t0.16\n(0)\t26.69785676113191\n",
  "history_begin_time" : 1660777889179,
  "history_end_time" : 1660778171066,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "pvs7ax5emzj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660778171069,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "yld1oli7j8r",
  "history_input" : "#!/bin/bash\n# generate images and gif from the prediction NetCDF files and overlay the AirNow station observation on the top\n\ncmaq_folder=\"/groups/ESS/aalnaim/cmaq\"\nmkdir $cmaq_folder\"/plots\"\n# Setting env variables\n# export YYYYMMDD_POST=$(date -d '2 day ago' '+%Y%m%d')\n# export stdate_post=$(date -d '2 day ago' '+%Y-%m-%d') \n# export eddate_post=$(date -d '1 day ago' '+%Y-%m-%d')\n\n# export stdate_file=$(date -d '2 day ago' '+%Y%m%d') \n# export eddate_file=$(date -d '1 day ago' '+%Y%m%d') \nexport YYYYMMDD_POST='20220805'\nexport stdate_post='2022-08-05'\nexport eddate_post='2022-08-06'\nexport stdate_file='20220805'\nexport eddate_file='20220806'\n\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport graph_dir=\"/groups/ESS/aalnaim/cmaq/plots\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\ncat <<EOF >>$cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\") \nd1 = getenv(\"stdate_post\") \nd2 = getenv(\"eddate_post\") \n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\nobs_dir = getenv(\"obs_dir_NCL\")\nplot_dir = getenv(\"graph_dir\") \n\nhr=new(24,\"string\")\nhr=(/\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"/)\n\nprint(plot_dir)\naconc_dir = getenv(\"postdata_dir\") \ngrid_dir = getenv(\"mcip_dir\")\n\nprint(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\n\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\ncdf_file2= addfile(grid_dir+\"/METCRO2D_\"+date+\".nc\",\"r\")\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,:,:) ;ppb\nwspd10=cdf_file2->WSPD10(:,0,:,:)\nwdir10=cdf_file2->WDIR10(:,0,:,:)\n\ntemp = cdf_file2->TEMP2\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(max(temp))\nprint(min(temp))\nprint(avg(temp))\n\n\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\no3@unit = \"ppbv\"\n\nUV10=wind_component(wspd10,wdir10,0)\nUV10@lat2d = lat\nUV10@lon2d = lon\n\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; dont advance frame\nres@gsnDraw = False\nres@gsnLeftString  = \"\"\nres@gsnRightString = \"\"\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n;res@tiMainFontHeightF = 0.02\n;res@vpWidthF        = 0.7\n;res@vpHeightF       = 0.7\n\n;;set map;;\nmpres                             = res\nmpres@mpLimitMode = \"LatLon\"\nmpres@mpDataSetName               = \"Earth..4\"\nmpres@mpDataBaseVersion           = \"MediumRes\"\nmpres@mpOutlineOn                 = True\nmpres@mpGeophysicalLineThicknessF = 1.5\nmpres@mpFillDrawOrder             = \"PostDraw\"\nmpres@mpFillOn                    = False\nmpres@mpAreaMaskingOn         = True\nmpres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nmpres@mpOutlineSpecifiers         = \"United States:States\"\nmpres@mpProjection           = \"LambertConformal\"\nmpres@mpLambertParallel1F    = 33.\nmpres@mpLambertParallel2F    = 45.\nmpres@mpLambertMeridianF     = -98.\nmpres@mpMinLonF = -120 ;min(lon)+0.2\nmpres@mpMaxLonF = -70 ;max(lon)-0.2\nmpres@mpMinLatF = 25 ;min(lat)+0.05\nmpres@mpMaxLatF = 50 ;max(lat)-0.05\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@mpLandFillColor         = \"white\"\nmpres@mpInlandWaterFillColor  = \"white\"\nmpres@mpOceanFillColor        = \"white\"\nmpres@mpGeophysicalLineColor    = \"Black\"\n\n;mpres@lbLabelAutoStride       = True\nmpres@tiXAxisFont             = 25\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@tmXBLabelFont           = 25\nmpres@tmXBLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmYLLabelFont           = 25\nmpres@tmYLLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmXTLabelsOn            = False\nmpres@tmXTLabelFont           = 25\nmpres@tmXTLabelFontHeightF    = 0.013\nmpres@tmYRLabelsOn            = False\nmpres@tmYRLabelFont           = 25\nmpres@tmYRLabelFontHeightF    = 0.013\n\n;;set contour;;\ncnres                         = res\ncnres@cnFillDrawOrder         = \"PreDraw\"\ncnres@cnFillOn                = True\ncnres@cnLinesOn               = False\ncnres@cnLineLabelsOn          = False\ncnres@lbLabelFont             = 25\ncnres@lbLabelFontHeightF      = 0.013\ncnres@tiXAxisFont             = 25\ncnres@pmLabelBarWidthF        = 0.5\ncnres@pmLabelBarHeightF       = 0.1\n;cnres@pmLabelBarOrthogonalPosF = -0.02\ncnres@lbLabelAutoStride       = True\n\n;set vector;;\nres_vc                        = res\nres_vc@vcGlyphStyle           = \"LineArrow\"\nres_vc@vcLineArrowThicknessF  = 3\nres_vc@vcMinDistanceF         = 0.03\nres_vc@vcRefLengthF           = 0.03\nres_vc@vcRefAnnoOn            = True\nres_vc@vcRefMagnitudeF           = 16\nres_vc@vcRefAnnoString1          = \"16m/s\"\nres_vc@vcRefAnnoSide             = \"Top\"\nres_vc@vcRefAnnoString2On        = False\nres_vc@vcRefAnnoPerimOn          = False\nres_vc@vcRefAnnoOrthogonalPosF   = -0.02\nres_vc@vcRefAnnoParallelPosF     = 0.999\n;res_vc@vcRefAnnoBackgroundColor = \"White\"\nres_vc@vcVectorDrawOrder         = \"PostDraw\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  ;print(time(it,0)+\" \"+time(it,1))\n  rundate = yyyyddd_to_yyyymmdd( time(it,0) )\n  runtime = hr( tointeger(time(it,1)/10000) )\n\n  site = readAsciiTable(obs_dir+\"/AQF5X_Hourly_\"+rundate+runtime+\".dat\",1,\"string\",1)\n  nrows = dimsizes(site)\n  sitename = str_get_field(site,1,\",\")\n  sitelat = stringtofloat(str_get_field(site,2,\",\"))\n  sitelon = stringtofloat(str_get_field(site,3,\",\"))\n  O3_obs = stringtofloat(str_get_field(site,4,\",\"))\n\n  obslon = sitelon(:,0)\n  obslat = sitelat(:,0)\n  obsO3 = O3_obs(:,0)\n\n  npts = nrows(0)\n\n  obsO3@_FillValue = -999.\n\n;--- levels for dividing\n  levels_O3  = ispan(0,80,4)\n\n  nlevels = dimsizes(levels_O3)\n\n  colors  = span_color_rgba(\"WhiteBlueGreenYellowRed\",nlevels+1)\n\n  num_distinct_markers = nlevels+1        ; number of distinct markers\n  lat_O3 = new((/num_distinct_markers,npts/),float)\n  lon_O3 = new((/num_distinct_markers,npts/),float)\n  lat_O3 = -999\n  lon_O3 = -999\n\n\n;\n; Group the points according to which range they fall in. At the\n; same time, create the label that we will use later in the labelbar\n;\n  do i = 0, num_distinct_markers-1\n    if (i.eq.0) then\n      indexes_O3 = ind(obsO3(:).lt.levels_O3(0))\n    end if\n    if (i.eq.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.max(levels_O3))\n    end if\n    if (i.gt.0.and.i.lt.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.levels_O3(i-1).and.obsO3(:).lt.levels_O3(i))\n    end if\n\n;\n; Now that we have the set of indexes whose values fall within\n; the given range, take the corresponding lat/lon values and store\n; them, so later we can color this set of markers with the appropriate\n; color.\n;\n    if (.not.any(ismissing(indexes_O3))) then\n      npts_range_O3 = dimsizes(indexes_O3)   ; # of points in this range.\n\n      lat_O3(i,0:npts_range_O3-1) = obslat(indexes_O3)\n      lon_O3(i,0:npts_range_O3-1) = obslon(indexes_O3)\n  ;print(\"O3: \"+npts_range_O3)\n    end if\n\n\n    delete(indexes_O3)            ; Necessary b/c \"indexes\" may be a different\n  end do\n\n  lat_O3@_FillValue = -999\n  lon_O3@_FillValue = -999\n\n  gsres               = True\n  gsres@gsMarkerIndex = 16          ; Use filled dots for markers.\n\n  hollowres           = True\n  hollowres@gsMarkerIndex    = 4\n  hollowres@gsMarkerColor    = \"black\"\n  hollowres@gsMarkerSizeF    = 0.008\n\n;;;;;;;;;   Plot Ozone\n  pname=plot_dir+\"/OBS-FORECAST_O3_\"+rundate+runtime\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  pmid_O3 = new(num_distinct_markers,graphic)\n  hollow_O3 = new(num_distinct_markers,graphic)\n\n  cnres@tiMainString =  pdate+\" \"+runtime+\" UTC O~B~3~N~ (ppbV)\"\n  cnres@cnLevelSelectionMode = \"ManualLevels\"\n  cnres@cnMinLevelValF          = 0.\n  cnres@cnMaxLevelValF          = 80\n  cnres@cnLevelSpacingF         = 4\n\n  ;plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  map = gsn_csm_map(wks,mpres)\n  contour = gsn_csm_contour(wks,o3(it,:,:),cnres)\n  vector  = gsn_csm_vector(wks,UV10(0,it,:,:),UV10(1,it,:,:),res_vc)\n  overlay(map,contour)\n  overlay(map,vector)\n\n  pmid = new(num_distinct_markers,graphic)\n  hollow = new(num_distinct_markers,graphic)\n  do i = 0, num_distinct_markers-1\n    if (.not.ismissing(lat_O3(i,0)))\n      gsres@gsMarkerColor      = colors(i,:)\n      gsres@gsMarkerSizeF      = 0.008\n      gsres@gsMarkerThicknessF = 1\n       pmid(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),gsres)\n       hollow(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),hollowres)\n    end if\n  end do\n\n  draw(map)\n  frame(wks)\n  delete(wks)\n  delete(pmid_O3)\n  delete(hollow_O3)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\n\n\n  delete(pmid)\n  delete(hollow)\n  delete(site)\n  delete(sitename)\n  delete(sitelat)\n  delete(sitelon)\n  delete(O3_obs)\n  delete(obslon)\n  delete(obslat)\n  delete(obsO3)\n  delete([/lon_O3,lat_O3/])\n\nend do\ndelete(res)\n\n;/\n\nend\nEOF\n\n\nncl $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\n\nconvert -delay 100 $cmaq_folder/plots/OBS*.png $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif\n\nif [ $? -eq 0 ]; then\n    echo \"Generating AirNow images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nelse\n    echo \"Generating AirNow images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nfi\n",
  "history_output" : "mkdir: cannot create directory ‘/groups/ESS/aalnaim/cmaq/plots’: File exists\nrm: cannot remove '/groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3_Airnow.ncl': No such file or directory\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n\n\nVariable: plot_dir\nType: string\nTotal Size: 8 bytes\n            1 values\nNumber of Dimensions: 1\nDimensions and sizes:\t[1]\nCoordinates: \n(0)\t/groups/ESS/aalnaim/cmaq/plots\n(0)\t/groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20220805_ML_extracted.nc\n(0)\t314.3372\n(0)\t271.3267\n(0)\t295.8239\n(0)\t24 265 442\n(0)\t96.84999999999999\n(0)\t0.16\n(0)\t26.69785676113191\n",
  "history_begin_time" : 1660777889862,
  "history_end_time" : 1660778171072,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "6qmmjzdi530",
  "history_input" : "# All the utility functions that most steps in CMAQ-AI need\n# this file should not contain any direct call of function\n# it should be dedicated to define functions or variables\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import date, datetime, timedelta\n\n\n# home directory\nhome = str(Path.home())\ncmaq_folder = \"/groups/ESS/aalnaim/cmaq/\" # change if you want to use your own folder\n\ndef get_days_list(sdate, edate):\n  days=[]\n  \n  delta = edate - sdate       # as timedelta\n\n  for i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n  # add one more day\n  one_more_day = sdate + timedelta(days=delta.days + 1)\n  list_day=one_more_day.strftime('%Y%m%d')\n  days.append(list_day)\n  \n  return days\n\ndef create_and_clean_folder(folder_path):\n  os.makedirs(folder_path, exist_ok=True)\n  # clean all files inside the folder\n  for f in os.listdir(folder_path):\n    os.remove(os.path.join(folder_path, f))\n\ndef remove_file(file_path):\n  print(f'remove old files{file_path}')\n  if os.path.exists(file_path):\n    os.remove(file_path)\n    \ndef turn_2_digits(a):\n  return f\"{a:02}\"",
  "history_output" : "Running",
  "history_begin_time" : 1660777336896,
  "history_end_time" : 1660778171079,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "i4ck934cb2d",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom cmaq_ai_utils import *\n\n# importing data\nfinal=pd.read_csv(f'{cmaq_folder}/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\ncreate_and_clean_folder(f\"{cmaq_folder}/models/\")\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x','CMAQ12KM_NO2(ppb)', 'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'CO(moles/s)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)','WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day', 'hours'],axis=1)\nprint(\"input shape:\", X.shape)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = f'{cmaq_folder}/models/rf_pycaret_o3_only.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   30.260300   -81.453300       14.0  ...      8    5     12\n1   42.770802   -71.102798       40.0  ...      8    5     12\n2   34.528599   -86.970596        8.0  ...      8    5     12\n3   42.194401   -72.555603       23.0  ...      8    5     12\n4   34.751801   -82.256699        8.0  ...      8    5     12\n\n[5 rows x 18 columns]\ninput shape: (28161, 1)\n/home/aalnaim/CMAQAI/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n  warn(\n",
  "history_begin_time" : 1660777540352,
  "history_end_time" : 1660778171079,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "m6w7iv9ndlh",
  "history_input" : "{\"operation\":\"ShowResultMap\",\"params\":[{\"name\":\"FilePath\",\"value\":\"/groups/ESS/zsun/cmaq/plots/Map_20220810.gif\"}]}",
  "history_output" : null,
  "history_begin_time" : 1660777335066,
  "history_end_time" : 1660778171080,
  "history_notes" : null,
  "history_process" : "pvzabv",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
}]
