[{
  "history_id" : "h337uz1dr19",
  "history_input" : "# get all of the cmaq model output variables\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2022, 6, 1)   # start date\nedate = date(2022, 8, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\n\nfor day in days:\n\tprint(\"Getting data for: \"+day)\n    \n\t# read cmaq results\n\tdf_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+day+\"_extracted.nc\")\n    \n\t# read mcip results \n\tdf_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+day+\".nc\")\n        \n\t# read emissions results \n\tdf_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n        \n\tfor k in time_step_in_netcdf_list:\n\t\treal_hour_value = real_hour_list[k]\n            \n\t\tdf_hourly = pd.DataFrame()\n            \n\t\t# CMAQ data\n            \n\t\t# O3 variable\n\t\to3=df_cmaq.variables['O3'][:].values[k,0]\n\t\tcmaq_O3=list(np.ravel(o3).transpose().round())  \n            \n\t\t# NO2\n\t\tno2=df_cmaq.variables['NO2'][:].values[k,0]\n\t\tcmaq_NO2=list(np.ravel(no2).transpose().round())\n      \n\t\t# CO\n\t\tco=df_cmaq.variables['CO'][:].values[k,0]\n\t\tcmaq_CO=list(np.ravel(co).transpose().round())\n\n\t\t# PM25_CO\n\t\tpm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n\t\tcmaq_PM25_CO=list(np.ravel(pm25).transpose().round())\n            \n                              \n                              \n\t\t# EMIS data\n                              \n\t\tco_emis=df_emis.variables['CO'][:].values[k,0]\n\t\tCO_emi=list(np.ravel(co_emis).transpose().round())    \n                \n                              \n                              \n\t\t# MCIP data\n            \n\t\t# CO variable\n\t\tprsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n\t\tPRSFC=list(np.ravel(prsfc).transpose().round())\n            \n\t\t# NO2\n\t\tpbl=df_mcip.variables['PBL'][:].values[k,0]\n\t\tPBL=list(np.ravel(pbl).transpose().round())\n            \n\t\t# TEMP2\n\t\ttemp2=df_mcip.variables['TEMP2'][:].values[k,0]\n\t\tTEMP2=list(np.ravel(temp2).transpose().round())\n            \n\t\t# WSPD10\n\t\twspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n\t\tWSPD10=list(np.ravel(wspd10).transpose().round())\n            \n\t\t# WDIR10\n\t\twdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n\t\tWDIR10=list(np.ravel(wdir10).transpose().round())\n\n\t\t# RGRND\n\t\trgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n\t\tRGRND=list(np.ravel(rgrnd).transpose().round())\n\n\t\t# CFRAC\n\t\tcfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n\t\tCFRAC=list(np.ravel(cfrac).transpose().round())\n\n            \n\t\t## LAT/LON data\n\t\tdf_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n            \n\t\tlat = df_coords.variables['LAT'][:].values[0,0]\n\t\tlat_flt=np.ravel(lat)\n\t\tLAT=np.tile(lat_flt,1)\n\n\t\tlon = df_coords.variables['LON'][:].values[0,0]\n\t\tlon_flt=np.ravel(lon)\n\t\tLON=np.tile(lon_flt,1)\n            \n\t\tdf_hourly['Latitude'] = LAT\n\t\tdf_hourly['Longitude'] = LON\n\t\tdf_hourly['YYYYMMDDHH'] = day+str(real_hour_value)\n\t\tdf_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n\t\tdf_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n\t\tdf_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n\t\tdf_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n\t\tdf_hourly['CO(moles/s)'] = CO_emi\n\t\tdf_hourly['PRSFC(Pa)'] = PRSFC\n\t\tdf_hourly['PBL(m)'] = PBL\n\t\tdf_hourly['TEMP2(K)'] = TEMP2\n\t\tdf_hourly['WSPD10(m/s)'] = WSPD10\n\t\tdf_hourly['WDIR10(degree)'] = WDIR10\n\t\tdf_hourly['RGRND(W/m2)'] = RGRND\n\t\tdf_hourly['CFRAC'] = CFRAC\n\t\tdf_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n\t\tdf_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n\t\tdf_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n            \n            \n\t\tprint('Saving file: train_data_'+day+'_'+str(real_hour_value)+'.csv')\n\t\tdf_hourly.to_csv('/groups/ESS/aalnaim/cmaq/training_input_hourly/train_data_'+day+'_'+str(real_hour_value)+'.csv',index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220601\nTraceback (most recent call last):\n  File \"preparing_cmaq_training_data.py\", line 40, in <module>\n    df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/xarray/backends/api.py\", line 479, in open_dataset\n    engine = plugins.guess_engine(filename_or_obj)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/xarray/backends/plugins.py\", line 148, in guess_engine\n    raise ValueError(error_msg)\nValueError: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html\n",
  "history_begin_time" : 1660166193079,
  "history_end_time" : 1660166318912,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "2rw4narobtx",
  "history_input" : "# get all the airnow station data\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta, date\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\n\nsdate = date(2022, 6, 1)   # start date\nedate = date(2022, 8, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n    for t in time:\n        files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n        with open(files, 'r') as file:\n            text = file.read()\n            new_string = text.replace('\"', '')\n\n        outF = open(\"/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_\"+i+t+\".txt\", \"w\")\n        for line in new_string:\n          # write line to output file\n          outF.write(line)\n        outF.close()",
  "history_output" : "Traceback (most recent call last):\n  File \"observation.py\", line 27, in <module>\n    with open(files, 'r') as file:\nFileNotFoundError: [Errno 2] No such file or directory: '/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/AQF5X_Hourly_2022060112.dat'\n",
  "history_begin_time" : 1660166215969,
  "history_end_time" : 1660166318914,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "qdd1c2gbre0",
  "history_input" : "# take all the airnow observations and merge into one observation.csv\n\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import datetime, date, timedelta\n# home directory\nhome = str(Path.home())\n\ndays=[]\n\n\nsdate = date(2022, 6, 1)   # start date\nedate = date(2022, 8, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\n\nfor d in days:\n    for t in time:\n\n        files=glob.glob(\"/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n        for file in files:\n            print(file)\n\n            data = np.loadtxt(file, skiprows=1,dtype='str')\n            dt=d+t\n            print(dt)\n            dt=np.tile(dt,len(data)) # constructs an array for each hour of each day with length of data from total stations available\n            date_time.append(dt)\n            merged.append(data)\n            \ndata_frame = np.concatenate(merged)\n\n# This gets the first 4 columns in the observation file (AQSID, Latitude, Longitude, OZONE(ppb))\ndata_frame = np.delete(data_frame, np.s_[4:9], axis=1) \n\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\ndff=df.replace(',','', regex=True)\n\ndt = np.concatenate(date_time)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 45, in <module>\n    data_frame = np.concatenate(merged)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nValueError: need at least one array to concatenate\n",
  "history_begin_time" : 1660166224677,
  "history_end_time" : 1660166318916,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "xb9ej24udfl",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 13, in <module>\n    cmaq = pd.concat(df_from_each_hourly_file)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 346, in concat\n    op = _Concatenator(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 403, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n",
  "history_begin_time" : 1660166239049,
  "history_end_time" : 1660166318918,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "9kaw4qahcfq",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166318919,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "q4bdh7f933n",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_two_months.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 676556\n",
  "history_begin_time" : 1660166253901,
  "history_end_time" : 1660166318926,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "eou67shbk7y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166318927,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "rq9zhts4i7i",
  "history_input" : "# install all dependencies\n# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\nwith open('requirements.txt','w') as out:\n  out.write('''\nabsl-py==1.0.0\naffine==2.3.1\nasttokens==2.0.5\nastunparse==1.6.3\nattrs==21.4.0\nautokeras==1.0.18\nbackcall==0.2.0\ncachetools==5.0.0\ncertifi==2021.10.8\ncftime==1.6.0\ncharset-normalizer==2.0.12\nclick==8.1.3\nclick-plugins==1.1.1\ncligj==0.7.2\ncmaps==1.0.5\ncycler==0.11.0\ndecorator==5.1.1\nearthpy==0.9.4\nexecuting==0.8.3\nFiona==1.8.21\nflatbuffers==2.0\nfonttools==4.29.1\ngast==0.5.3\ngeopandas==0.10.2\nglob2==0.7\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.44.0\nh5py==3.6.0\nidna==3.3\nimageio==2.19.0\nimageio-ffmpeg==0.4.7\nimportlib-metadata==4.11.2\nipython==8.1.1\njedi==0.18.1\njoblib==1.1.0\nkaleido==0.2.1\nkeras==2.8.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.0\nkiwisolver==1.3.2\nkt-legacy==1.0.4\nlibclang==13.0.0\nMarkdown==3.3.6\nmatplotlib==3.5.1\nmatplotlib-inline==0.1.3\nmunch==2.5.0\nnetCDF4==1.5.8\nnetworkx==2.8\nnumpy==1.22.2\noauthlib==3.2.0\nopencv-python==4.5.5.64\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.1\nparso==0.8.3\npathlib==1.0.1\npathlib2==2.3.7.post1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.0.1\nplotly==5.7.0\nprompt-toolkit==3.0.28\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\nPygments==2.11.2\npyparsing==3.0.7\npyproj==3.3.1\npython-dateutil==2.8.2\npytz==2021.3\nPyWavelets==1.3.0\nrasterio==1.2.10\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscikit-image==0.19.2\nscikit-learn==1.0.2\nscipy==1.8.0\nseaborn==0.11.2\nShapely==1.8.2\nsix==1.16.0\nsklearn==0.0\nsnuggs==1.4.7\nstack-data==0.2.0\ntenacity==8.0.1\ntensorboard==2.8.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.8.0\ntensorflow-gpu==2.8.0\ntensorflow-io-gcs-filesystem==0.24.0\ntermcolor==1.1.0\ntf-estimator-nightly==2.8.0.dev2021122109\nthreadpoolctl==3.1.0\ntifffile==2022.5.4\ntraitlets==5.1.1\ntyping_extensions==4.1.1\nurllib3==1.26.8\nwcwidth==0.2.5\nWerkzeug==2.0.3\nwrapt==1.13.3\nxarray==2022.3.0\nxgboost==1.6.0\nzipp==3.7.0''')\n  \npython = sys.executable\nsubprocess.check_call([python, '-m', 'pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.DEVNULL)\n    #subprocess.check_call(\n        #[python, '-m', 'conda', 'install', '-c','conda-forge','xgboost'],\n      #stdout=subprocess.DEVNULL)\n\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models','cmaq/observation']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "history_output" : "\u001B[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\nYou should consider upgrading via the '/home/aalnaim/CMAQAI/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
  "history_begin_time" : 1660166153932,
  "history_end_time" : 1660166318932,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "3phz6kba9cm",
  "history_input" : "# get hourly CMAQ data into csv for prediction\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nfrom datetime import timedelta\n# home directory\nhome = str(Path.home())\n\nbase = datetime.datetime.today() - timedelta(days=2)\ndate_list = [base + timedelta(days=x) for x in range(2)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\ndays = ['20220728', '20220729']\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor day in days:\n\tprint(\"Getting data for: \"+day)\n    \n\t# read cmaq results\n\tdf_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+day+\"_extracted.nc\")\n    \n\t# read mcip results \n\tdf_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+day+\".nc\")\n        \n\t# read emissions results \n\tdf_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n        \n\tfor k in time_step_in_netcdf_list:\n\t\treal_hour_value = real_hour_list[k]\n            \n\t\tdf_hourly = pd.DataFrame()\n            \n\t\t# CMAQ data\n            \n\t\t# O3 variable\n\t\to3=df_cmaq.variables['O3'][:].values[k,0]\n\t\tcmaq_O3=list(np.ravel(o3).transpose().round())  \n            \n\t\t# NO2\n\t\tno2=df_cmaq.variables['NO2'][:].values[k,0]\n\t\tcmaq_NO2=list(np.ravel(no2).transpose().round())\n      \n\t\t# CO\n\t\tco=df_cmaq.variables['CO'][:].values[k,0]\n\t\tcmaq_CO=list(np.ravel(co).transpose().round())\n\n\t\t# PM25_CO\n\t\tpm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n\t\tcmaq_PM25_CO=list(np.ravel(pm25).transpose().round())\n            \n                              \n                              \n\t\t# EMIS data\n                              \n\t\tco_emis=df_emis.variables['CO'][:].values[k,0]\n\t\tCO_emi=list(np.ravel(co_emis).transpose().round())    \n                \n                              \n                              \n\t\t# MCIP data\n            \n\t\t# CO variable\n\t\tprsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n\t\tPRSFC=list(np.ravel(prsfc).transpose().round())\n            \n\t\t# NO2\n\t\tpbl=df_mcip.variables['PBL'][:].values[k,0]\n\t\tPBL=list(np.ravel(pbl).transpose().round())\n            \n\t\t# TEMP2\n\t\ttemp2=df_mcip.variables['TEMP2'][:].values[k,0]\n\t\tTEMP2=list(np.ravel(temp2).transpose().round())\n            \n\t\t# WSPD10\n\t\twspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n\t\tWSPD10=list(np.ravel(wspd10).transpose().round())\n            \n\t\t# WDIR10\n\t\twdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n\t\tWDIR10=list(np.ravel(wdir10).transpose().round())\n\n\t\t# RGRND\n\t\trgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n\t\tRGRND=list(np.ravel(rgrnd).transpose().round())\n\n\t\t# CFRAC\n\t\tcfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n\t\tCFRAC=list(np.ravel(cfrac).transpose().round())\n\n            \n\t\t## LAT/LON data\n\t\tdf_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n            \n\t\tlat = df_coords.variables['LAT'][:].values[0,0]\n\t\tlat_flt=np.ravel(lat)\n\t\tLAT=np.tile(lat_flt,1)\n\n\t\tlon = df_coords.variables['LON'][:].values[0,0]\n\t\tlon_flt=np.ravel(lon)\n\t\tLON=np.tile(lon_flt,1)\n            \n\t\tdf_hourly['Latitude'] = LAT\n\t\tdf_hourly['Longitude'] = LON\n\t\tdf_hourly['YYYYMMDDHH'] = day+str(real_hour_value)\n\t\tdf_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n\t\tdf_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n\t\tdf_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n\t\tdf_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n\t\tdf_hourly['CO(moles/s)'] = CO_emi\n\t\tdf_hourly['PRSFC(Pa)'] = PRSFC\n\t\tdf_hourly['PBL(m)'] = PBL\n\t\tdf_hourly['TEMP2(K)'] = TEMP2\n\t\tdf_hourly['WSPD10(m/s)'] = WSPD10\n\t\tdf_hourly['WDIR10(degree)'] = WDIR10\n\t\tdf_hourly['RGRND(W/m2)'] = RGRND\n\t\tdf_hourly['CFRAC'] = CFRAC\n\t\tdf_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n\t\tdf_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n\t\tdf_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n            \n            \n\t\tprint('Saving file: test_data_'+day+'_'+str(real_hour_value)+'.csv')\n\t\tdf_hourly.to_csv('/groups/ESS/aalnaim/cmaq/input_hourly/test_data_'+day+'_'+str(real_hour_value)+'.csv',index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220728\n",
  "history_begin_time" : 1660166278573,
  "history_end_time" : 1660166318936,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "p6wvf2",
  "indicator" : "Running"
},{
  "history_id" : "01xvs5vv6tv",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166319945,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "86z1kvn5rku",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166319958,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "sz23u02numb",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166319962,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "a4sn4yn0jbs",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660166319969,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
}]
