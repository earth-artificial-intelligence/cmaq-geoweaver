[{
  "history_id" : "yu9c2s4m7ct",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Poocessing the data - shifting columns of NO2\n\n# Checking required packages are installed or not\n\nimport sys\nimport subprocess\nprint('running_preprocess')\n#import pkg_resources\n\n# Required packages to run this process.\n#required = {'pandas','pathlib'}\n#installed = {pkg.key for pkg in pkg_resources.working_set}\n#missing = required - installed\n\n#if missing:\n #   print(\"Packages missing and will be installed: \", missing)\n #   python = sys.executable\n #   subprocess.check_call(\n    #    [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES VALIDATION  #\n################################\n\n## importing necessary libraries\n\nimport pandas as pd\nimport datetime\n\n# defining function for shifting data\nmonth_i=[1,2,3,4,5,6,7,8,9,10,11,12]\ndef shift(file):\n    station=file['Station.ID'].unique()\n    file['date']=pd.to_datetime(file[[\"year\", \"month\", \"day\",\"hours\"]]) # creating date\n    file['dayofyear'] = pd.to_datetime(file['date']).dt.dayofyear # converting monthly days to yearly dasy\n    dfs = dict(tuple(file.groupby('Station.ID'))) # grouping the data by station\n#    print(dfs)\n    list_final=[]\n    for site in station:\n        list1=dfs[site]  # selecting dataset for each station\n        o3_max=list1.loc[list1.groupby(\"dayofyear\")[\"AirNOW_O3\"].idxmax()] # daily max values\n        o3_month=pd.DataFrame(o3_max.groupby('month',as_index=False)['hours'].mean()) # monthly average hour\n        no2_max=list1.loc[list1.groupby(\"dayofyear\")[\"CMAQ12KM_NO2\"].idxmax()] # daily amx values\n        no2_month=pd.DataFrame(no2_max.groupby('month',as_index=False)['hours'].mean()) # montly average hour\n        merge_hour=pd.concat([o3_month,no2_month],axis = 1) # merging two table\n        merge_hour.columns = ['month1','hours1', 'month2','hours2'] # renaming column name\n        merge_hour['hours_diff']=merge_hour['hours1']-merge_hour['hours2']\n        merge_hour.hours_diff = merge_hour.hours_diff.astype(int) # converting hours_diff to int\n        months = dict(tuple(list1.groupby('month')))   # grouping the data by month\n        diff=merge_hour['hours_diff'] # extracting hours difference field\n        mon=merge_hour['month1'] # extracting month field\n\n        for (m,n) in zip(mon,diff):\n            list2= months[m] # selecting dataset for each month and for each station\n            list3=list2.loc[list2['month'] == m] # subsetting dataset for each month\n            list3['CMAQ12KM_NO2_new'] = list3['CMAQ12KM_NO2'].shift(n) # shifting rows for each month\n            list_final.append(list3)\n    return list_final\n\n  \n# Importing and merging 2020 and 2021 dataset\ndf1 = pd.read_csv('/home/mislam25/cmaq/2020.csv')\ndf2 = pd.read_csv('/home/mislam25/cmaq/2021.csv')\n#df1 = pd.read_csv('D:/Research/CMAQ/local_test/2020.csv')\n#df2 = pd.read_csv('D:/Research/CMAQ/local_test/2021.csv')\n#merging two dataframe vertically\nmrg=df1.append(df2, ignore_index=True)\n# Changing columns name with index number\nmapping = {mrg.columns[0]: 'Station.ID', mrg.columns[4]: 'AirNOW_O3',mrg.columns[5]: 'AirNOW_NO2',mrg.columns[6]: 'AirNOW_CO',mrg.columns[8]: 'CMAQ12KM_NO2'}\nmrg_rename = mrg.rename(columns=mapping)\n\n# dropping unnecessary columns\nmrg_rename.drop(su.columns[[5,6]], axis = 1, inplace = True)\n\n# ignoring tropomi remote sensing data\n#df3_rs=pd.read_csv('/home/mislam25/cmaq/merged_rs.csv')\n\n#final=pd.merge(mrg,df3_rs, on=['year', 'month','day','hours','Station.ID'])\n\n#shifting CMAQ NO2\nshift_df=shift(mrg_rename)\nagg_data = pd.concat(shift_df) # concatening the list\n# droping no data from all column and AirNOW_O3)\ndata_new=agg_data.dropna() \nfinal_df = data_new[data_new.AirNOW_O3!= -999]\n\n# saving the file into local drive\n#final_df.to_csv('D:/Research/CMAQ/local_test/merged_2020_2021.csv',index=False)\nfinal_df.to_csv('/home/mislam25/cmaq/merged_2020_2021.csv',index=False)\n",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\nprediction_xgboost.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest_hello.py\nmodel_comparison.py\nnetcdf_processing.py\ntext_to_csv_2020.py\ntext_to_csv_2021.py\ndirectory_test.py\nexploratory_data_analysis.py\ntest_2.py\nrunning_preprocess\nTraceback (most recent call last):\n  File \"/home/mislam25/GfkqfM14TnxTPqOCme5FHeqDXp/pre_processing.py\", line 74, in <module>\n    mrg_rename.drop(su.columns[[5,6]], axis = 1, inplace = True)\nNameError: name 'su' is not defined\n",
  "history_begin_time" : 1645586432211,
  "history_end_time" : 1645586462859,
  "history_notes" : null,
  "history_process" : "im9rct",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "10ribqy5ksb",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Poocessing the data - shifting columns of NO2\n\n# Checking required packages are installed or not\n\nimport sys\nimport subprocess\nprint('running_preprocess')\n#import pkg_resources\n\n# Required packages to run this process.\n#required = {'pandas','pathlib'}\n#installed = {pkg.key for pkg in pkg_resources.working_set}\n#missing = required - installed\n\n#if missing:\n #   print(\"Packages missing and will be installed: \", missing)\n #   python = sys.executable\n #   subprocess.check_call(\n    #    [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES VALIDATION  #\n################################\n\n## importing necessary libraries\nimport glob\nimport pandas as pd\nprint('running text_2020')\nfiles=glob.glob(r'/groups/ESS/pmakkaro/az/2020/all/sep22/*.txt')\n#files=glob.glob(r'D:/Research/CMAQ/2020/text/*.txt')\ndata_frame = pd.DataFrame()\nmerged=[]\nfor file in files:\n    df=pd.read_csv(file)\n    merged.append(df)\ndata_frame = pd.concat(merged)\ndata_frame['YYYYMMDDHH'] = data_frame['YYYYMMDDHH'].map(str)\ndata_frame['year'] = data_frame['YYYYMMDDHH'].str[:4]\ndata_frame['month'] = data_frame['YYYYMMDDHH'].str[4:6]\ndata_frame['day'] = data_frame['YYYYMMDDHH'].str[6:8]\ndata_frame['hours'] = data_frame['YYYYMMDDHH'].str[8:10]\ndata_frame.to_csv('/home/mislam25/cmaq/2020.csv',index=False)\n#data_frame.to_csv('D:/Research/CMAQ/local_test/2020.csv',index=False)",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\nprediction_xgboost.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest_hello.py\nmodel_comparison.py\nnetcdf_processing.py\ntext_to_csv_2020.py\ntext_to_csv_2021.py\ndirectory_test.py\nexploratory_data_analysis.py\ntest_2.py\nrunning_preprocess\nrunning text_2020\n",
  "history_begin_time" : 1645586380108,
  "history_end_time" : 1645586431982,
  "history_notes" : null,
  "history_process" : "hmg5ef",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "34swffaxb9z",
  "history_input" : null,
  "history_output" : "C:\\Users\\didar\\gw-workspace\\temp\\GfkqfM14TnxTPqOCme5FHeqDXp.tar is not a regular file or directory",
  "history_begin_time" : 1645586380108,
  "history_end_time" : 1645586389875,
  "history_notes" : null,
  "history_process" : "tf0wia",
  "host_id" : "w6jk5o",
  "indicator" : "Failed"
},{
  "history_id" : "z0ixxvsm365",
  "history_input" : "\n# NASA GEOWEAVER\n# CMAQ-AI Model: Poocessing the data - shifting columns of NO2\n\n# Checking required packages are installed or not\n\nimport sys\nimport subprocess\nprint('running_preprocess')\n#import pkg_resources\n\n# Required packages to run this process.\n#required = {'pandas','pathlib'}\n#installed = {pkg.key for pkg in pkg_resources.working_set}\n#missing = required - installed\n\n#if missing:\n #   print(\"Packages missing and will be installed: \", missing)\n #   python = sys.executable\n #   subprocess.check_call(\n    #    [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES VALIDATION  #\n################################\n\n## importing necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# importing data\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n#final=pd.read_csv('D:/Research/CMAQ/local_test/merged_2020_2021.csv')\n\n# defining training variables\nyear_2020=final.loc[final['year']==2020]\n\n# Processing training  data\nselected_vars = year_2020.drop(['Station.ID','YYYYMMDDHH','year','date','dayofyear'],axis=1)\n\n########################################################\n## Correlation matrix heatmap\n# Correlation between different variables\ncorr = selected_vars.corr()\n# Set up the matplotlib plot configuration\nf, ax = plt.subplots(figsize=(18, 10))\n# Generate a mask for upper traingle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Configure a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top + 0.5)\n#ax.set(xlim=(0, 12))\nplt.savefig('/home/mislam25/cmaq/EDA/correlation.png')\n########################################################\n\n# plotting r2 values of different variables vs AirNOW_O3\n########################################################\ncolumns=list(selected_vars)\nfor i in columns:\n  try:\n  \tsns.lmplot(x=i, y=\"AirNOW_O3\", data=selected_vars);\n  \tplt.savefig('/home/mislam25/cmaq/EDA/'+i+'_AirNOW_O3.png')\n  except FileNotFoundError:\n    pass\n    \n########################################################",
  "history_output" : "Running",
  "history_begin_time" : 1645586471490,
  "history_end_time" : 1645586463190,
  "history_notes" : null,
  "history_process" : "smwp7r",
  "host_id" : "w6jk5o",
  "indicator" : "Running"
}]
