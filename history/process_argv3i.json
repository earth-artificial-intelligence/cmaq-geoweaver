[{
  "history_id" : "io5e1ax79ta",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667925180738,
  "history_end_time" : 1667950923698,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "igbs51kudho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667892171012,
  "history_end_time" : 1667892171012,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "vmg38t95qjd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666685317286,
  "history_end_time" : 1666685918671,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "s8sxbprm345",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666682218473,
  "history_end_time" : 1666683406476,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "qz29r17cb3h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681701596,
  "history_end_time" : 1666681701596,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "wni2oihrb0u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681684443,
  "history_end_time" : 1667950920480,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "hsfm4prlqja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681592841,
  "history_end_time" : 1666681655024,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "7f4u804c3cy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666678879110,
  "history_end_time" : 1666678879110,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "px48yi3lh2g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666576238783,
  "history_end_time" : 1666577006606,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "rmxsar9evv4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666553421654,
  "history_end_time" : 1666553446561,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "wgej0j9iaak",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666542247436,
  "history_end_time" : 1666553383592,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "z4eu82055lc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665934797892,
  "history_end_time" : 1665935665907,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "pya8ce9odin",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665932107414,
  "history_end_time" : 1665934029279,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "lzd92z56twr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930832272,
  "history_end_time" : 1665930832272,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "g75r3tf32dv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930614410,
  "history_end_time" : 1665930705189,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "k8sdm66lezs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930050047,
  "history_end_time" : 1665930589617,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "9dyxqc6srej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665928489267,
  "history_end_time" : 1665928489267,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "vrv2n95nra2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665926308968,
  "history_end_time" : 1665926308968,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "n0rrzkxuq5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665023826541,
  "history_end_time" : 1665032721238,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "6rhf30s4pw6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665021841877,
  "history_end_time" : 1665023784708,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "plpahtoophc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665007808645,
  "history_end_time" : 1665007808645,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "kftw6kt8i4v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664983201506,
  "history_end_time" : 1665009008625,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "3e0qov4w2pl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664949775855,
  "history_end_time" : 1664949775855,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "9hcncbnnron",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664505829132,
  "history_end_time" : 1664505829132,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "661220vskn5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664428239188,
  "history_end_time" : 1664428239188,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "zv2ohd2t23d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664427014314,
  "history_end_time" : 1664427014314,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "iozu3hdjgn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664426709292,
  "history_end_time" : 1664426992849,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "JWFeeyrEqYOi",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nobs['YYYYMMDDHH'] = obs['YYYYMMDDHH'].astype(str)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nref_stations = ref_stations.astype(float)\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    new_df['YYYYMMDDHH'] = new_df['YYYYMMDDHH'].astype(str)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    #print(\"final_chunk_df\", final_chunk_df)\n    #count += 1\n    #if count == 10:\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid_test.csv\",mode='a',index=False)\n    break\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid_test.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training_new_one_year_valid_test.csv\n",
  "history_begin_time" : 1664134571151,
  "history_end_time" : 1664134644278,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LO6XeP8l8i3T",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nobs['YYYYMMDDHH'] = obs['YYYYMMDDHH'].astype(str)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nref_stations = ref_stations.astype(float)\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    new_df['YYYYMMDDHH'] = new_df['YYYYMMDDHH'].astype(str)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    #print(\"final_chunk_df\", final_chunk_df)\n    #count += 1\n    #if count == 10:\n    break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid_test.csv\",mode='a',index=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid_test.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training_new_one_year_valid_test.csv\n",
  "history_begin_time" : 1664134415821,
  "history_end_time" : 1664134518662,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uRjxpOtiMYw2",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nobs['YYYYMMDDHH'] = obs['YYYYMMDDHH'].astype(str)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nref_stations = ref_stations.astype(float)\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    new_df['YYYYMMDDHH'] = new_df['YYYYMMDDHH'].astype(str)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    #print(\"final_chunk_df\", final_chunk_df)\n    #count += 1\n    #if count == 10:\n    #  break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n",
  "history_begin_time" : 1664128844798,
  "history_end_time" : 1664134444436,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "0ZcUWh8Ya91Q",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nobs['YYYYMMDDHH'] = obs['YYYYMMDDHH'].astype(str)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    new_df['YYYYMMDDHH'] = new_df['YYYYMMDDHH'].astype(str)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    #chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ... Longitude_y AirNOW_O3\n1     36.327705   -86.684080  ...  -86.652496      17.0\n3     36.035057   -83.887270  ...  -83.876099      28.0\n5     35.557697   -80.328460  ...  -80.395042      23.0\n7     34.920830   -76.563020  ...  -76.620300      20.0\n10    34.513260  -120.026520  ... -120.045799      26.0\n..          ...          ...  ...         ...       ...\n314   38.474270  -109.865340  ... -109.821098      50.0\n315   38.928993  -104.752335  ... -104.816704      35.0\n316   39.163296   -94.690125  ...  -94.635597      13.0\n318   39.092290   -92.313446  ...  -92.316322       8.0\n319   39.028034   -90.918150  ...  -90.849190      19.0\n\n[148 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n1     37.508420   -77.358610  ...   -77.400269       34.0\n15    36.983610  -119.986680  ...  -120.034103       19.0\n16    37.323597  -118.269620  ...  -118.330833       22.0\n17    39.276980   -94.966890  ...   -94.951302       15.0\n18    39.265930   -94.406310  ...   -94.376389       18.0\n..          ...          ...  ...          ...        ...\n494   40.741936  -112.031250  ...  -112.008667       36.0\n495   40.759712  -111.889984  ...  -111.945000       44.0\n497   40.759712  -111.889984  ...  -111.930901       31.0\n498   40.759712  -111.889984  ...  -111.871696       55.0\n500   41.333050  -106.186615  ...  -106.239899       56.0\n\n[210 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     41.633568   -93.590210  ...   -93.643303       18.0\n3     41.510098   -90.550964  ...   -90.587502       19.0\n4     41.510098   -90.550964  ...   -90.517197       20.0\n8     41.094402   -85.092440  ...   -85.101814       15.0\n11    40.687600   -81.545410  ...   -81.598335       19.0\n..          ...          ...  ...          ...        ...\n429   43.861904  -104.131850  ...  -104.191902       47.0\n430   43.980347   -92.415070  ...   -92.449699       19.0\n431   43.725853   -87.778900  ...   -87.716103       24.0\n432   42.390995   -76.659300  ...   -76.653801       30.0\n434   41.418490   -71.580720  ...   -71.524849       17.0\n\n[218 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     43.833443   -87.763730  ...   -87.791939       23.0\n9     41.757412   -72.658080  ...   -72.629700        7.0\n11    41.552605   -71.679990  ...   -71.719704        9.0\n12    41.492640   -71.401610  ...   -71.421898       26.0\n16    43.918697  -106.550200  ...  -106.509743       35.0\n..          ...          ...  ...          ...        ...\n257   45.171486   -78.896850  ...   -78.931396       24.0\n259   43.917873   -71.678925  ...   -71.700798        8.0\n261   43.612170   -70.242310  ...   -70.268967       13.0\n262   43.612170   -70.242310  ...   -70.207321       19.0\n263   43.870820  -123.083466  ...  -123.035378        1.0\n\n[105 rows x 24 columns]\nmerge_training_data.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     43.974533  -123.125990  ...  -123.083771        4.0\n9     46.684097   -92.530550  ...   -92.511719       16.0\n11    46.607346   -90.653900  ...   -90.698601       18.0\n12    46.299446   -85.989290  ...   -85.950302       15.0\n14    44.491653   -72.864440  ...   -72.868973       11.0\n..          ...          ...  ...          ...        ...\n224   46.726097   -75.384674  ...   -75.431900        3.0\n226   46.402090   -72.921050  ...   -72.892799       11.0\n241   48.363087  -107.900910  ...  -107.862473       29.0\n242   48.689632  -102.429720  ...  -102.402199       20.0\n249   47.446537  -117.568390  ...  -117.529800       31.0\n\n[72 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training_new_one_year_valid.csv\n",
  "history_begin_time" : 1664128314202,
  "history_end_time" : 1664128466777,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PJPXEbyclLpX",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:42: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nmerge_training_data.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 29, in <module>\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n    op = _MergeOperation(\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 704, in __init__\n    self._maybe_coerce_merge_keys()\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 1257, in _maybe_coerce_merge_keys\n    raise ValueError(msg)\nValueError: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat\n",
  "history_begin_time" : 1664128112357,
  "history_end_time" : 1664128207999,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GTUe96huyJCl",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(\"chunk :\", chunk)\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nchunk :        Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n10000  26.692100  -88.971070  2021080112  ...      8    1     12\n10001  26.682660  -88.852750  2021080112  ...      8    1     12\n10002  26.673100  -88.734470  2021080112  ...      8    1     12\n10003  26.663387  -88.616210  2021080112  ...      8    1     12\n10004  26.653545  -88.497986  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n19995  28.691060 -109.645080  2021080112  ...      8    1     12\n19996  28.705803 -109.524445  2021080112  ...      8    1     12\n19997  28.720413 -109.403780  2021080112  ...      8    1     12\n19998  28.734886 -109.283050  2021080112  ...      8    1     12\n19999  28.749210 -109.162290  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:38: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n20000  28.763401 -109.041504  2021080112  ...      8    1     12\n20001  28.777440 -108.920654  2021080112  ...      8    1     12\n20002  28.791344 -108.799774  2021080112  ...      8    1     12\n20003  28.805122 -108.678830  2021080112  ...      8    1     12\n20004  28.818733 -108.557860  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n29995  29.627544  -75.544680  2021080112  ...      8    1     12\n29996  29.602425  -75.424930  2021080112  ...      8    1     12\n29997  29.577171  -75.305240  2021080112  ...      8    1     12\n29998  29.551777  -75.185610  2021080112  ...      8    1     12\n29999  29.526234  -75.066070  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:38: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nchunk :         Latitude  Longitude  YYYYMMDDHH  ...  month  day  hours\n30000  29.500584 -74.946594  2021080112  ...      8    1     12\n30001  29.474789 -74.827210  2021080112  ...      8    1     12\n30002  29.448845 -74.707920  2021080112  ...      8    1     12\n30003  29.422783 -74.588684  2021080112  ...      8    1     12\n30004  29.396576 -74.469540  2021080112  ...      8    1     12\n...          ...        ...         ...  ...    ...  ...    ...\n39995  34.304348 -96.148895  2021080112  ...      8    1     12\n39996  34.303250 -96.017975  2021080112  ...      8    1     12\n39997  34.302010 -95.887054  2021080112  ...      8    1     12\n39998  34.300617 -95.756134  2021080112  ...      8    1     12\n39999  34.299060 -95.625210  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:38: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nchunk :         Latitude  Longitude  YYYYMMDDHH  ...  month  day  hours\n40000  34.297337  -95.49429  2021080112  ...      8    1     12\n40001  34.295464  -95.36340  2021080112  ...      8    1     12\n40002  34.293440  -95.23248  2021080112  ...      8    1     12\n40003  34.291264  -95.10162  2021080112  ...      8    1     12\n40004  34.288920  -94.97073  2021080112  ...      8    1     12\n...          ...        ...         ...  ...    ...  ...    ...\n49995  34.773323 -118.20764  2021080112  ...      8    1     12\n49996  34.798280 -118.07941  2021080112  ...      8    1     12\n49997  34.823086 -117.95108  2021080112  ...      8    1     12\n49998  34.847736 -117.82269  2021080112  ...      8    1     12\n49999  34.872257 -117.69420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:38: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n50000   Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n50001  34.896618  -117.56563  2021080112  ...      8    1     12\n50002   34.92083  -117.43698  2021080112  ...      8    1     12\n50003    34.9449  -117.30823  2021080112  ...      8    1     12\n50004  34.968807  -117.17941  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n59995   38.07941   -80.99509  2021080112  ...      8    1     12\n59996  38.060326  -80.859375  2021080112  ...      8    1     12\n59997  38.041073  -80.723755  2021080112  ...      8    1     12\n59998   38.02166   -80.58823  2021080112  ...      8    1     12\n59999  38.002098   -80.45276  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 25, in <module>\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n    op = _MergeOperation(\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 704, in __init__\n    self._maybe_coerce_merge_keys()\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 1257, in _maybe_coerce_merge_keys\n    raise ValueError(msg)\nValueError: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat\n",
  "history_begin_time" : 1664127804956,
  "history_end_time" : 1664127898438,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mi7JFk4UYArh",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(\"chunk :\", chunk)\n    \n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nchunk :        Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n10000  26.692100  -88.971070  2021080112  ...      8    1     12\n10001  26.682660  -88.852750  2021080112  ...      8    1     12\n10002  26.673100  -88.734470  2021080112  ...      8    1     12\n10003  26.663387  -88.616210  2021080112  ...      8    1     12\n10004  26.653545  -88.497986  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n19995  28.691060 -109.645080  2021080112  ...      8    1     12\n19996  28.705803 -109.524445  2021080112  ...      8    1     12\n19997  28.720413 -109.403780  2021080112  ...      8    1     12\n19998  28.734886 -109.283050  2021080112  ...      8    1     12\n19999  28.749210 -109.162290  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n20000  28.763401 -109.041504  2021080112  ...      8    1     12\n20001  28.777440 -108.920654  2021080112  ...      8    1     12\n20002  28.791344 -108.799774  2021080112  ...      8    1     12\n20003  28.805122 -108.678830  2021080112  ...      8    1     12\n20004  28.818733 -108.557860  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n29995  29.627544  -75.544680  2021080112  ...      8    1     12\n29996  29.602425  -75.424930  2021080112  ...      8    1     12\n29997  29.577171  -75.305240  2021080112  ...      8    1     12\n29998  29.551777  -75.185610  2021080112  ...      8    1     12\n29999  29.526234  -75.066070  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nchunk :         Latitude  Longitude  YYYYMMDDHH  ...  month  day  hours\n30000  29.500584 -74.946594  2021080112  ...      8    1     12\n30001  29.474789 -74.827210  2021080112  ...      8    1     12\n30002  29.448845 -74.707920  2021080112  ...      8    1     12\n30003  29.422783 -74.588684  2021080112  ...      8    1     12\n30004  29.396576 -74.469540  2021080112  ...      8    1     12\n...          ...        ...         ...  ...    ...  ...    ...\n39995  34.304348 -96.148895  2021080112  ...      8    1     12\n39996  34.303250 -96.017975  2021080112  ...      8    1     12\n39997  34.302010 -95.887054  2021080112  ...      8    1     12\n39998  34.300617 -95.756134  2021080112  ...      8    1     12\n39999  34.299060 -95.625210  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nchunk :         Latitude  Longitude  YYYYMMDDHH  ...  month  day  hours\n40000  34.297337  -95.49429  2021080112  ...      8    1     12\n40001  34.295464  -95.36340  2021080112  ...      8    1     12\n40002  34.293440  -95.23248  2021080112  ...      8    1     12\n40003  34.291264  -95.10162  2021080112  ...      8    1     12\n40004  34.288920  -94.97073  2021080112  ...      8    1     12\n...          ...        ...         ...  ...    ...  ...    ...\n49995  34.773323 -118.20764  2021080112  ...      8    1     12\n49996  34.798280 -118.07941  2021080112  ...      8    1     12\n49997  34.823086 -117.95108  2021080112  ...      8    1     12\n49998  34.847736 -117.82269  2021080112  ...      8    1     12\n49999  34.872257 -117.69420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nchunk :         Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n50000   Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n50001  34.896618  -117.56563  2021080112  ...      8    1     12\n50002   34.92083  -117.43698  2021080112  ...      8    1     12\n50003    34.9449  -117.30823  2021080112  ...      8    1     12\n50004  34.968807  -117.17941  2021080112  ...      8    1     12\n...          ...         ...         ...  ...    ...  ...    ...\n59995   38.07941   -80.99509  2021080112  ...      8    1     12\n59996  38.060326  -80.859375  2021080112  ...      8    1     12\n59997  38.041073  -80.723755  2021080112  ...      8    1     12\n59998   38.02166   -80.58823  2021080112  ...      8    1     12\n59999  38.002098   -80.45276  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 18, in <module>\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5920, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\", line 419, in astype\n    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\", line 304, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\", line 580, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1292, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1237, in astype_array\n    values = astype_nansafe(values, dtype, copy=copy)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1181, in astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'Latitude'\n",
  "history_begin_time" : 1664127207277,
  "history_end_time" : 1664127300367,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kp12VOjwhjXn",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nmerge_training_data.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nmerge_training_data.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nmerge_training_data.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nmerge_training_data.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:36: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 17, in <module>\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5920, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\", line 419, in astype\n    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\", line 304, in apply\n    applied = getattr(b, f)(**kwargs)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\", line 580, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1292, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1237, in astype_array\n    values = astype_nansafe(values, dtype, copy=copy)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1181, in astype_nansafe\n    return arr.astype(dtype, copy=True)\nValueError: could not convert string to float: 'Latitude'\n",
  "history_begin_time" : 1664127044152,
  "history_end_time" : 1664127136759,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wchFIUKUMs8D",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n3     25.628311   -80.315060  ...   -80.326897        7.0\n4     25.712883   -80.178250  ...   -80.162201       19.0\n12    26.285812   -81.739590  ...   -81.720001       18.0\n13    26.063736   -80.344910  ...   -80.338600       12.0\n14    26.044365   -80.229004  ...   -80.257607       12.0\n21    26.128952   -80.091400  ...   -80.111397       13.0\n23    26.530144   -81.933750  ...   -81.979698       14.0\n26    26.233055   -80.069550  ...   -80.096901       17.0\n30    27.752789   -97.421265  ...   -97.434250        9.0\n39    27.858750   -97.542360  ...   -97.555801        7.0\n50    26.565243   -80.120180  ...   -80.058914       15.0\n54    27.123196   -82.306366  ...   -82.362503       20.0\n55    27.262135   -82.523254  ...   -82.570297       23.0\n57    27.367142   -82.504240  ...   -82.479698       11.0\n58    27.191406   -81.324100  ...   -81.340302       13.0\n60    27.472190   -82.485200  ...   -82.521896       13.0\n61    27.126205   -80.244660  ...   -80.240692       13.0\n63    27.594150   -82.584690  ...   -82.545799       17.0\n66    27.354744   -80.318480  ...   -80.311035       14.0\n67    28.820580   -97.060974  ...   -97.005600        7.0\n68    27.837933   -82.784580  ...   -82.739998       14.0\n70    27.787518   -82.427734  ...   -82.465599       14.0\n74    27.943192   -82.765780  ...   -82.736099       13.0\n75    27.909664   -82.527530  ...   -82.538300       14.0\n79    27.892704   -82.408480  ...   -82.454697       13.0\n80    29.023174   -95.471310  ...   -95.473602        1.0\n82    28.048492   -82.746950  ...   -82.700798       11.0\n85    27.980778   -82.270020  ...   -82.230614        7.0\n86    27.946102   -82.031890  ...   -82.000298       10.0\n91    28.153816   -82.728030  ...   -82.757797        0.0\n92    28.051258   -82.012085  ...   -81.971901        8.0\n93    27.812168   -80.467410  ...   -80.455399       15.0\n95    29.240390   -98.287600  ...   -98.311699        3.0\n97    29.226238   -94.854250  ...   -94.856400       18.0\n104   28.313725   -82.330960  ...   -82.305603        8.0\n105   28.060188   -80.661350  ...   -80.628304       12.0\n\n[36 rows x 24 columns]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     29.277943  -103.200320  ...  -103.178101       23.0\n7     28.313519   -81.593690  ...   -81.636703        9.0\n8     29.556270   -98.662140  ...   -98.620003        7.0\n13    29.556270   -95.337860  ...   -95.392502        0.0\n18    29.549755   -94.968600  ...   -95.015602        3.0\n..          ...          ...  ...          ...        ...\n192   30.248066   -81.461365  ...   -81.453300       28.0\n193   30.892830   -85.637210  ...   -85.604401       17.0\n194   30.408016   -81.808105  ...   -81.840790       22.0\n199   31.609673   -97.063200  ...   -97.070602        8.0\n201   30.477814   -81.541720  ...   -81.586899       29.0\n\n[86 rows x 24 columns]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n4     31.634686  -106.308914  ...  -106.287743       26.0\n7     32.038914   -96.428070  ...   -96.399200        9.0\n11    31.730682  -106.447784  ...  -106.402802       22.0\n13    31.730682  -106.447784  ...  -106.501114       30.0\n17    31.730682  -106.447784  ...  -106.455002       33.0\n..          ...          ...  ...          ...        ...\n173   32.616135  -115.503204  ...  -115.504204        0.0\n177   33.256145  -111.328400  ...  -111.292801       43.0\n179   33.960266   -89.758514  ...   -89.799004       15.0\n180   33.448830   -84.205930  ...   -84.161697       19.0\n187   33.293674  -111.859070  ...  -111.884300       17.0\n\n[68 rows x 24 columns]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0     33.841225    -86.89383  ...   -86.942497       11.0\n1     33.540535    -84.05939  ...   -84.066704       16.0\n3     32.912480    -79.60388  ...   -79.656998       34.0\n6     32.578003   -116.93384  ...  -116.929489       10.0\n15    33.382553   -112.00775  ...  -112.073097       14.0\n..          ...          ...  ...          ...        ...\n336   36.219997    -86.69931  ...   -86.744400       20.0\n338   35.958614    -84.17151  ...   -84.223099       26.0\n345   35.181824    -78.65912  ...   -78.728035       22.0\n346   34.435528   -119.86667  ...  -119.827797        5.0\n347   34.462390   -119.73958  ...  -119.690910        8.0\n\n[171 rows x 24 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 17, in <module>\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n    op = _MergeOperation(\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 704, in __init__\n    self._maybe_coerce_merge_keys()\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 1257, in _maybe_coerce_merge_keys\n    raise ValueError(msg)\nValueError: You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat\n",
  "history_begin_time" : 1664126833799,
  "history_end_time" : 1664126926260,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "V9fGhn6ZmZIm",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    #print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nmerge_training_data.py:34: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 17, in <module>\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n    op = _MergeOperation(\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 704, in __init__\n    self._maybe_coerce_merge_keys()\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 1257, in _maybe_coerce_merge_keys\n    raise ValueError(msg)\nValueError: You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat\n",
  "history_begin_time" : 1664126692129,
  "history_end_time" : 1664126783519,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ng3fUeooftYO",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    #print(\"chunk :\", chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    #print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    #print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    #test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    #print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    #print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    #print(\"final_chunk_df\", final_chunk_df)\n    count += 1\n    if count == 10:\n      break\n    final_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nmerge_training_data.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 40, in <module>\n    final_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid.csv\",mode='a',index=False, header=False)\nNameError: name 'final_df' is not defined\n",
  "history_begin_time" : 1664126602789,
  "history_end_time" : 1664126668449,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "67LHDXM8eSoy",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(\"chunk :\", chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nchunk :        Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nmerge_training_data.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\nnew_df after merge ref shape:         Lat_cmaq   Latitude  Lat_airnow  YYYYMMDDHH\n4599  25.677399  25.677399   25.674999  2021080112\n4600  25.681358  25.681358   25.675550  2021080112\n4601  25.685196  25.685196   25.668329  2021080112\n4602  25.688873  25.688873   25.646111  2021080112\n5040  25.778976  25.778976   25.783430  2021080112 (14, 21)\nnew_df after merge all three shape:     Lat_airnow  Lon_airnow  YYYYMMDDHH\n0   25.674999 -100.458328  2021080112\n1   25.675550 -100.338333  2021080112\n2   25.668329 -100.248329  2021080112\n3   25.646111 -100.095558  2021080112\n4   25.783430 -100.585869  2021080112 (14, 25)\ntest_obs_merge shape: (15497669, 9)     Latitude  Lat_airnow\n0  31.168888   31.168888\n1  40.277802   40.277802\n2  40.277802   40.277802\n3  47.808228   47.808228\n4  47.663963   47.663963\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   25.677399  -100.461880  ...  -100.458328     -999.0\n1   25.681358  -100.344635  ...  -100.338333     -999.0\n2   25.685196  -100.227390  ...  -100.248329     -999.0\n3   25.688873  -100.110110  ...  -100.095558     -999.0\n4   25.778976  -100.583740  ...  -100.585869     -999.0\n\n[5 rows x 24 columns]\nfinal_chunk_df     Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n10   26.157295    -97.76709  ...   -97.712502        8.0\n11   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664126230554,
  "history_end_time" : 1664126301083,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gOiWEvk9zNys",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(\"chunk :\", chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\nchunk :        Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:     Lat_cmaq   Latitude  Lat_airnow  YYYYMMDDHH\n0       NaN  21.829086         NaN  2021080112\n1       NaN  21.855751         NaN  2021080112\n2       NaN  21.882309         NaN  2021080112\n3       NaN  21.908745         NaN  2021080112\n4       NaN  21.935051         NaN  2021080112 (10000, 21)\nnew_df after merge all three shape:     Lat_airnow  Lon_airnow  YYYYMMDDHH\n0         NaN         NaN  2021080112\n1         NaN         NaN  2021080112\n2         NaN         NaN  2021080112\n3         NaN         NaN  2021080112\n4         NaN         NaN  2021080112 (10000, 25)\ntest_obs_merge shape: (15497669, 9)     Latitude  Lat_airnow\n0  31.168888   31.168888\n1  40.277802   40.277802\n2  40.277802   40.277802\n3  47.808228   47.808228\n4  47.663963   47.663963\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664126085078,
  "history_end_time" : 1664126157749,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SVECOaRq24ov",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df[['Lat_cmaq','Latitude','Lat_airnow','YYYYMMDDHH']].head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:     Lat_cmaq   Latitude  Lat_airnow  YYYYMMDDHH\n0       NaN  21.829086         NaN  2021080112\n1       NaN  21.855751         NaN  2021080112\n2       NaN  21.882309         NaN  2021080112\n3       NaN  21.908745         NaN  2021080112\n4       NaN  21.935051         NaN  2021080112 (10000, 21)\nnew_df after merge all three shape:     Lat_airnow  Lon_airnow  YYYYMMDDHH\n0         NaN         NaN  2021080112\n1         NaN         NaN  2021080112\n2         NaN         NaN  2021080112\n3         NaN         NaN  2021080112\n4         NaN         NaN  2021080112 (10000, 25)\ntest_obs_merge shape: (15497669, 9)     Latitude  Lat_airnow\n0  31.168888   31.168888\n1  40.277802   40.277802\n2  40.277802   40.277802\n3  47.808228   47.808228\n4  47.663963   47.663963\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664125770636,
  "history_end_time" : 1664125842452,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EY3YFsf09o8n",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df.head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final[['Lat_airnow','Lon_airnow','YYYYMMDDHH']].head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[[\"Latitude\", \"Lat_airnow\"]].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:      Latitude   Longitude  YYYYMMDDHH  ...  Lon_airnow  Lat_cmaq  Lon_cmaq\n0  21.829086 -120.620790  2021080112  ...         NaN       NaN       NaN\n1  21.855751 -120.512500  2021080112  ...         NaN       NaN       NaN\n2  21.882309 -120.404144  2021080112  ...         NaN       NaN       NaN\n3  21.908745 -120.295715  2021080112  ...         NaN       NaN       NaN\n4  21.935051 -120.187225  2021080112  ...         NaN       NaN       NaN\n\n[5 rows x 21 columns] (10000, 21)\nnew_df after merge all three shape:     Lat_airnow  Lon_airnow  YYYYMMDDHH\n0         NaN         NaN  2021080112\n1         NaN         NaN  2021080112\n2         NaN         NaN  2021080112\n3         NaN         NaN  2021080112\n4         NaN         NaN  2021080112 (10000, 25)\ntest_obs_merge shape: (15497669, 9)     Latitude  Lat_airnow\n0  31.168888   31.168888\n1  40.277802   40.277802\n2  40.277802   40.277802\n3  47.808228   47.808228\n4  47.663963   47.663963\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664125100322,
  "history_end_time" : 1664125170794,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Z9NfJ8yDPqMB",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df.head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final.head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[\"Latitude\", \"Lat_airnow\"].head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:      Latitude   Longitude  YYYYMMDDHH  ...  Lon_airnow  Lat_cmaq  Lon_cmaq\n0  21.829086 -120.620790  2021080112  ...         NaN       NaN       NaN\n1  21.855751 -120.512500  2021080112  ...         NaN       NaN       NaN\n2  21.882309 -120.404144  2021080112  ...         NaN       NaN       NaN\n3  21.908745 -120.295715  2021080112  ...         NaN       NaN       NaN\n4  21.935051 -120.187225  2021080112  ...         NaN       NaN       NaN\n\n[5 rows x 21 columns] (10000, 21)\nnew_df after merge all three shape:     Latitude_x  Longitude_x  YYYYMMDDHH  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  2021080112  ...         NaN          NaN        NaN\n1   21.855751  -120.512500  2021080112  ...         NaN          NaN        NaN\n2   21.882309  -120.404144  2021080112  ...         NaN          NaN        NaN\n3   21.908745  -120.295715  2021080112  ...         NaN          NaN        NaN\n4   21.935051  -120.187225  2021080112  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns] (10000, 25)\nTraceback (most recent call last):\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3621, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: ('Latitude', 'Lat_airnow')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 22, in <module>\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge[\"Latitude\", \"Lat_airnow\"].head())\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 3505, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3623, in get_loc\n    raise KeyError(key) from err\nKeyError: ('Latitude', 'Lat_airnow')\n",
  "history_begin_time" : 1664124925164,
  "history_end_time" : 1664124975997,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "jRnHRFwauKO1",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df.head(), new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final.head(), chunk_final.shape)\n    test_obs_merge = pd.merge(obs,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_airnow','Lon_airnow'])\n    print(\"test_obs_merge shape:\", test_obs_merge.shape, test_obs_merge.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:      Latitude   Longitude  YYYYMMDDHH  ...  Lon_airnow  Lat_cmaq  Lon_cmaq\n0  21.829086 -120.620790  2021080112  ...         NaN       NaN       NaN\n1  21.855751 -120.512500  2021080112  ...         NaN       NaN       NaN\n2  21.882309 -120.404144  2021080112  ...         NaN       NaN       NaN\n3  21.908745 -120.295715  2021080112  ...         NaN       NaN       NaN\n4  21.935051 -120.187225  2021080112  ...         NaN       NaN       NaN\n\n[5 rows x 21 columns] (10000, 21)\nnew_df after merge all three shape:     Latitude_x  Longitude_x  YYYYMMDDHH  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  2021080112  ...         NaN          NaN        NaN\n1   21.855751  -120.512500  2021080112  ...         NaN          NaN        NaN\n2   21.882309  -120.404144  2021080112  ...         NaN          NaN        NaN\n3   21.908745  -120.295715  2021080112  ...         NaN          NaN        NaN\n4   21.935051  -120.187225  2021080112  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns] (10000, 25)\ntest_obs_merge shape: (15497669, 9)    StationID   Latitude   Longitude  ...  Lon_airnow   Lat_cmaq    Lon_cmaq\n0  483951076  31.168888  -96.481941  ...  -96.481941  31.177826  -96.434390\n1  080699991  40.277802 -105.545303  ... -105.545303  40.295480 -105.476900\n2  080699991  40.277802 -105.545303  ... -105.545303  40.295480 -105.476900\n3  530639995  47.808228 -117.343269  ... -117.343269  47.808144 -117.364716\n4  530639997  47.663963 -117.257652  ... -117.257652  47.598984 -117.294010\n\n[5 rows x 9 columns]\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664124772719,
  "history_end_time" : 1664124847648,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "uquiOznXcUM1",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final.shape)\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:  (10000, 21)\nnew_df after merge all three shape:  (10000, 25)\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664124560141,
  "history_end_time" : 1664124624824,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dD351XlOcWOq",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    print(\"new_df after merge ref shape: \", new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:  (10000, 21)\nnew_df after merge all three shape:     Latitude_x  Longitude_x  YYYYMMDDHH  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  2021080112  ...         NaN          NaN        NaN\n1   21.855751  -120.512500  2021080112  ...         NaN          NaN        NaN\n2   21.882309  -120.404144  2021080112  ...         NaN          NaN        NaN\n3   21.908745  -120.295715  2021080112  ...         NaN          NaN        NaN\n4   21.935051  -120.187225  2021080112  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\n   Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n0   21.829086  -120.620790  ...          NaN        NaN\n1   21.855751  -120.512500  ...          NaN        NaN\n2   21.882309  -120.404144  ...          NaN        NaN\n3   21.908745  -120.295715  ...          NaN        NaN\n4   21.935051  -120.187225  ...          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df       Latitude_x  Longitude_x  ...  Longitude_y  AirNOW_O3\n6390   26.157295    -97.76709  ...   -97.712502        8.0\n6828   26.258575    -98.24072  ...   -98.291069       10.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664124437283,
  "history_end_time" : 1664124501904,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "3SA9lWZqdbf5",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref shape: \", new_df.shape)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three shape: \", chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\nobs shape:  (15279463, 5)\nref_station shape:  (2833, 4)\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref shape:  (2833, 21)\nnew_df after merge all three shape:     Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\n   Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:30: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n115   26.226227  -98.291069  26.258575  ...   26.226227   -98.291069       10.0\n822   26.200277  -97.712502  26.157295  ...   26.200277   -97.712502        8.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664124288934,
  "history_end_time" : 1664124355263,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vDZqVfl02B2X",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", chunk_final.columns, chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    \n    print(\"final_chunk_df\", final_chunk_df)\n    \n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_x',\n       'Longitude_x', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours', 'StationID', 'Latitude_y', 'Longitude_y', 'AirNOW_O3'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\n   Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 24 columns]\nmerge_training_data.py:28: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\nfinal_chunk_df      Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n115   26.226227  -98.291069  26.258575  ...   26.226227   -98.291069       10.0\n822   26.200277  -97.712502  26.157295  ...   26.200277   -97.712502        8.0\n\n[2 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664124100061,
  "history_end_time" : 1664124167143,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oBqQzGOemwKk",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", chunk_final.columns, chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[~pd.isnull(new_chunk_df[\"AirNOW_O3\"])]\n    print(\"final_chunk_df\", final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_x',\n       'Longitude_x', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours', 'StationID', 'Latitude_y', 'Longitude_y', 'AirNOW_O3'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\n   Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 24 columns]\nfinal_chunk_df       Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n115    26.226227  -98.291069  26.258575  ...   26.226227   -98.291069       10.0\n252    25.675550 -100.338333  25.681358  ...   25.675550  -100.338333     -999.0\n359    26.073299  -97.166702  26.054161  ...   26.073299   -97.166702     -999.0\n386    25.892500  -97.493599  25.841240  ...   25.892500   -97.493599     -999.0\n658    25.756910 -100.365807  25.787056  ...   25.756910  -100.365807     -999.0\n783    25.646111 -100.095558  25.688873  ...   25.646111  -100.095558     -999.0\n822    26.200277  -97.712502  26.157295  ...   26.200277   -97.712502        8.0\n856    25.668329 -100.248329  25.685196  ...   25.668329  -100.248329     -999.0\n1326   25.745001 -100.254723  25.790886  ...   25.745001  -100.254723     -999.0\n1428   25.783430 -100.585869  25.778976  ...   25.783430  -100.585869     -999.0\n1439   26.288622  -98.152069  26.259953  ...   26.288622   -98.152069     -999.0\n1844   25.471901  -80.482803  25.439682  ...   25.471901   -80.482803     -999.0\n1845   25.674999 -100.458328  25.677399  ...   25.674999  -100.458328     -999.0\n1935   26.130800  -97.937202  26.156342  ...   26.130800   -97.937202     -999.0\n\n[14 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664122925776,
  "history_end_time" : 1664122972260,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "85gGmCMuDfeT",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", chunk_final.columns, chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    chunk_final['YYYYMMDDHH'] = chunk_final['YYYYMMDDHH'].map(str)\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3!= -999]\n    print(\"final_chunk_df\", final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_x',\n       'Longitude_x', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours', 'StationID', 'Latitude_y', 'Longitude_y', 'AirNOW_O3'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\n   Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 24 columns]\nfinal_chunk_df       Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0      31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1      42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2      39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3      40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4      43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n...          ...         ...        ...  ...         ...          ...        ...\n2828   35.913799  -81.191002  35.905716  ...         NaN          NaN        NaN\n2829   40.246990  -76.846985  40.203840  ...         NaN          NaN        NaN\n2830   39.818714  -75.413971  39.826466  ...         NaN          NaN        NaN\n2831   37.046902  -95.613297  37.006958  ...         NaN          NaN        NaN\n2832   32.791000  -79.959000  32.741245  ...         NaN          NaN        NaN\n\n[2821 rows x 24 columns]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//trainingNew.csv\n",
  "history_begin_time" : 1664122754448,
  "history_end_time" : 1664122800532,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WX4byQaxf3sT",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", chunk_final.columns, chunk_final.head())\n    #chunk_final=chunk_final.drop_duplicates(keep=False)\n    #training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    print(new_chunk_df.head())\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3!= -999]\n    print(\"final_chunk_df\", final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_x',\n       'Longitude_x', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours', 'StationID', 'Latitude_y', 'Longitude_y', 'AirNOW_O3'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 22, in <module>\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nNameError: name 'training_data' is not defined\n",
  "history_begin_time" : 1664122644865,
  "history_end_time" : 1664122691179,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ix1P8dQMUDPx",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", chunk_final.columns, chunk_final.head())\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_x',\n       'Longitude_x', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours', 'StationID', 'Latitude_y', 'Longitude_y', 'AirNOW_O3'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq  ...  Latitude_y  Longitude_y  AirNOW_O3\n0   31.169399  -81.496399  31.131332  ...         NaN          NaN        NaN\n1   42.062199  -87.673599  42.088024  ...         NaN          NaN        NaN\n2   39.758900  -86.397202  39.801613  ...         NaN          NaN        NaN\n3   40.931396  -81.123520  40.971100  ...         NaN          NaN        NaN\n4   43.435001  -88.527802  43.455270  ...         NaN          NaN        NaN\n\n[5 rows x 25 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 28, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664122399321,
  "history_end_time" : 1664122445503,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aNuYBSg3z387",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", new_df.columns, new_df.head())\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq  ...  CFRAC  month  day  hours\n0   31.169399  -81.496399  31.131332 -81.537930  ...    NaN    NaN  NaN    NaN\n1   42.062199  -87.673599  42.088024 -87.710205  ...    NaN    NaN  NaN    NaN\n2   39.758900  -86.397202  39.801613 -86.452760  ...    NaN    NaN  NaN    NaN\n3   40.931396  -81.123520  40.971100 -81.189270  ...    NaN    NaN  NaN    NaN\n4   43.435001  -88.527802  43.455270 -88.565704  ...    NaN    NaN  NaN    NaN\n\n[5 rows x 21 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 28, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664122319251,
  "history_end_time" : 1664122365634,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "DeQxkxVPUaIF",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    print(\"new_df after merge all three: \", new_df.columns, new_df.head())\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nnew_df after merge all three:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq  ...  CFRAC  month  day  hours\n0   31.169399  -81.496399  31.131332 -81.537930  ...    NaN    NaN  NaN    NaN\n1   42.062199  -87.673599  42.088024 -87.710205  ...    NaN    NaN  NaN    NaN\n2   39.758900  -86.397202  39.801613 -86.452760  ...    NaN    NaN  NaN    NaN\n3   40.931396  -81.123520  40.971100 -81.189270  ...    NaN    NaN  NaN    NaN\n4   43.435001  -88.527802  43.455270 -88.565704  ...    NaN    NaN  NaN    NaN\n\n[5 rows x 21 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 28, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121771685,
  "history_end_time" : 1664121882504,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XzzDgbhVyH0J",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(\"new_df after merge ref: \", new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nnew_df after merge ref:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 27, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121594053,
  "history_end_time" : 1664121699924,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ADS1FSHZSdYs",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\nref_stations head:  Index(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq'], dtype='object')    Lat_airnow  Lon_airnow   Lat_cmaq   Lon_cmaq\n0   31.169399  -81.496399  31.131332 -81.537930\n1   42.062199  -87.673599  42.088024 -87.710205\n2   39.758900  -86.397202  39.801613 -86.452760\n3   40.931396  -81.123520  40.971100 -81.189270\n4   43.435001  -88.527802  43.455270 -88.565704\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nIndex(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 27, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121368853,
  "history_end_time" : 1664121456207,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nauzcMXtytRu",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "obs head:  Index(['StationID', 'Latitude', 'Longitude', 'AirNOW_O3', 'YYYYMMDDHH'], dtype='object')    StationID   Latitude   Longitude  AirNOW_O3  YYYYMMDDHH\n0  483951076  31.168888  -96.481941     -999.0  2021080112\n1  080699991  40.277802 -105.545303     -999.0  2021080112\n2  530639995  47.808228 -117.343269     -999.0  2021080112\n3  530639997  47.663963 -117.257652     -999.0  2021080112\n4  530639996  47.660568 -117.084503     -999.0  2021080112\n       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nIndex(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 25, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121252144,
  "history_end_time" : 1664121339265,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ahzfjhhZewN7",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    print(new_df.columns)\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nIndex(['Lat_airnow', 'Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude',\n       'Longitude', 'YYYYMMDDHH', 'CMAQ12KM_O3(ppb)', 'CMAQ12KM_NO2(ppb)',\n       'CMAQ12KM_CO(ppm)', 'CMAQ_OC(ug/m3)', 'PRSFC(Pa)', 'PBL(m)', 'TEMP2(K)',\n       'WSPD10(m/s)', 'WDIR10(degree)', 'RGRND(W/m2)', 'CFRAC', 'month', 'day',\n       'hours'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 24, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121152678,
  "history_end_time" : 1664121241997,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eweic9Zumgf8",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10000\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    print(chunk)\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "       Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0     21.829086 -120.620790  2021080112  ...      8    1     12\n1     21.855751 -120.512500  2021080112  ...      8    1     12\n2     21.882309 -120.404144  2021080112  ...      8    1     12\n3     21.908745 -120.295715  2021080112  ...      8    1     12\n4     21.935051 -120.187225  2021080112  ...      8    1     12\n...         ...         ...         ...  ...    ...  ...    ...\n9995  26.737213  -89.563050  2021080112  ...      8    1     12\n9996  26.728450  -89.444580  2021080112  ...      8    1     12\n9997  26.719570  -89.326170  2021080112  ...      8    1     12\n9998  26.710556  -89.207760  2021080112  ...      8    1     12\n9999  26.701397  -89.089420  2021080112  ...      8    1     12\n\n[10000 rows x 17 columns]\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 23, in <module>\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 5583, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'DataFrame' object has no attribute 'AirNOW_O3'\n",
  "history_begin_time" : 1664121001346,
  "history_end_time" : 1664121097188,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "nlnjhpeIg8Yz",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10 ** 6\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:9: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n  for chunk in reader:\nTraceback (most recent call last):\n  File \"merge_training_data.py\", line 10, in <module>\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n    op = _MergeOperation(\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 704, in __init__\n    self._maybe_coerce_merge_keys()\n  File \"/home/zsun/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\", line 1257, in _maybe_coerce_merge_keys\n    raise ValueError(msg)\nValueError: You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat\n",
  "history_begin_time" : 1664120864006,
  "history_end_time" : 1664120926546,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UZjWRyxJsRLn",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10 ** 6\nwith pd.read_csv(filename, chunksize=chunksize) as reader:\n  for chunk in reader:\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\n    #new_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 8, in <module>\n    with pd.read_csv(filename, chunksize=chunksize) as reader:\nNameError: name 'filename' is not defined\n",
  "history_begin_time" : 1664120736201,
  "history_end_time" : 1664120776361,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SinzjjkOoSmT",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10 ** 6\nwith pd.read_csv(filename, chunksize=chunksize) as reader:\n  for chunk in reader:\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "  File \"merge_training_data.py\", line 12\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    ^\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1664120676606,
  "history_end_time" : 1664120687502,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Z7fP85tfjnTD",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nchunksize = 10 ** 6\nwith pd.read_csv(filename, chunksize=chunksize) as reader:\n  for chunk in reader:\n    new_df = pd.merge(ref_stations, chunk,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    chunk_final=chunk_final.drop_duplicates(keep=False)\n    training_data = chunk_final.loc[:,~chunk_final.columns.duplicated()]\n\n    training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n    training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n    training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n    training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\n    new_chunk_df=training_data.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_df.AirNOW_O3!= -999]\n    print(final_chunk_df)\n    break\n\n#final_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/trainingNew.csv\")\n\n\n",
  "history_output" : "  File \"merge_training_data.py\", line 12\n    chunk_final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n    ^\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1664120621394,
  "history_end_time" : 1664120631863,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "62OL6Ei68AAw",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Lat_cmaq','Lon_cmaq'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Lat_airnow','Lon_airnow','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "All records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1663783795202,
  "history_end_time" : 1663783845749,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "mag4nh5rs6c",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/mag4nh5rs6c/merge_training_data.py\", line 8, in <module>\n    cmaq = pd.concat(df_from_each_hourly_file)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 294, in concat\n    op = _Concatenator(\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 351, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n",
  "history_begin_time" : 1663783478563,
  "history_end_time" : 1663783480127,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nEeLn8dViyq7",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/trainingNew.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS3/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1661866041507,
  "history_end_time" : 1661866094302,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EJkwYp2fvQi8",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1661734876411,
  "history_end_time" : 1661734950808,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "odWgRsvPJBUy",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : null,
  "history_begin_time" : 1661706069937,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "JRS4zkWtjEZf",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\nfinal_df.rename(columns={\"Latitude_x\": \"Latitude\", \"Longitude_x\": \"Longitude\"}, inplace=True) \n\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nmerge_training_data.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  final_df.rename(columns={\"Latitude_x\": \"Latitude\", \"Longitude_x\": \"Longitude\"}, inplace=True)\nAll records should be incorporated into : /groups/ESS3/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1661386537896,
  "history_end_time" : 1661386871802,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7fY3A1BP5XCi",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\nfinal_df.rename(columns={\"Latitude_x\": \"Latitude\", \"Longitude_x\": \"Longitude\"}, inplace=True) \n\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nmerge_training_data.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  final_df.rename(columns={\"Latitude_x\": \"Latitude\", \"Longitude_x\": \"Longitude\"}, inplace=True)\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1661142450633,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "u16sOdU9oEpW",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660805848046,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "pFQxUqbLMw4N",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660787590967,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "owffklmsmvp",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 8, in <module>\n    cmaq = pd.concat(df_from_each_hourly_file)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 346, in concat\n    op = _Concatenator(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 403, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n",
  "history_begin_time" : 1660786815797,
  "history_end_time" : 1660786902448,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "C9DRPrjHnvor",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660784452481,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "1pjh7PuaAZGk",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660782959265,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "tuc8us2b2n5",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660777500674,
  "history_end_time" : 1660778171060,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "MPrBB8HRTixg",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660772969885,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "se4qVSF9ey4m",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1660771724571,
  "history_end_time" : 1660772899523,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "CLsPgvZtC9TS",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1660749976803,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "VKqb2G8jlgmD",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\n# final = final[(final['CMAQ12KM_O3(ppb)'] <= (final['CMAQ12KM_O3(ppb)'].mean() * 1.15)) & (final['AirNOW_O3'] <= (final['AirNOW_O3'].mean() * 1.15))]\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660704585195,
  "history_end_time" : 1660772897507,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YQ6wyrzpPu0S",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\n# final = final[(final['CMAQ12KM_O3(ppb)'] <= (final['CMAQ12KM_O3(ppb)'].mean() * 1.15)) & (final['AirNOW_O3'] <= (final['AirNOW_O3'].mean() * 1.15))]\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660678892060,
  "history_end_time" : 1660772897091,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "VSRerrcmU41G",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\nfinal = final[(final['CMAQ12KM_O3(ppb)'] <= (final['CMAQ12KM_O3(ppb)'].mean() * 1.15)) & (final['AirNOW_O3'] <= (final['AirNOW_O3'].mean() * 1.15))]\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660674210829,
  "history_end_time" : 1660772896674,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "uTwlUqd0UEex",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1660522402298,
  "history_end_time" : 1660750037203,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "kQV0fMj9UsuP",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\n\nfinal = final[(final['CMAQ12KM_O3(ppb)'] <= (final['CMAQ12KM_O3(ppb)'].mean() * 1.15)) & (final['AirNOW_O3'] <= (final['AirNOW_O3'].mean() * 1.15))]\n\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660454836618,
  "history_end_time" : 1660772896170,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "w9zJa6cgNd6n",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq//training.csv\n",
  "history_begin_time" : 1660370074766,
  "history_end_time" : 1660455344493,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "n3ZIIYF68XxR",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1660311135191,
  "history_end_time" : 1660513818630,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "9aUmXgqqblB6",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/aalnaim/cmaq/training.csv\n",
  "history_begin_time" : 1660301987033,
  "history_end_time" : 1660455342967,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "n2dN4Q6lSbiH",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1660301794120,
  "history_end_time" : 1660301931759,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "47ykuyd2h3p",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 11, in <module>\n    ref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 575, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 933, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1217, in _make_engine\n    self.handles = get_handle(  # type: ignore[call-overload]\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/common.py\", line 789, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/groups/ESS/aalnaim/cmaq/station_cmaq_location.csv'\n",
  "history_begin_time" : 1660299722614,
  "history_end_time" : 1660301768665,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "vpNxNpzwqNAu",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1660273356281,
  "history_end_time" : 1660513817718,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "aHf0i6CH52qS",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training.csv\")\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\nAll records should be incorporated into : /groups/ESS/zsun/cmaq//training.csv\n",
  "history_begin_time" : 1660273246958,
  "history_end_time" : 1660273339030,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "xLrm6FvMHrQl",
  "history_input" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = f'{cmaq_folder}/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation/observation.csv\")\nref_stations=pd.read_csv(f\"{cmaq_folder}/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(f\"{cmaq_folder}/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(f\"{cmaq_folder}/training.csv\",index=False)\n\n\n",
  "history_output" : "merge_training_data.py:15: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:22: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1660273117706,
  "history_end_time" : 1660273338546,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "mnaz0if32m9",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1660176490113,
  "history_end_time" : 1660189689429,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "xb9ej24udfl",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 13, in <module>\n    cmaq = pd.concat(df_from_each_hourly_file)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 346, in concat\n    op = _Concatenator(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/core/reshape/concat.py\", line 403, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n",
  "history_begin_time" : 1660166239049,
  "history_end_time" : 1660166318918,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "80yywkruqo3",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "merge_training_data.py:20: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1660008015542,
  "history_end_time" : 1660008066678,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "XqCfVS5DPVyO",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "merge_training_data.py:20: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1659740666203,
  "history_end_time" : 1660273337648,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "locemvcg34p",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)",
  "history_output" : "merge_training_data.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1659578466880,
  "history_end_time" : 1659578695073,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "O5mMaT2s4xVw",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)",
  "history_output" : "merge_training_data.py:18: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1659447970829,
  "history_end_time" : 1660273336671,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "JLVQ22AfFUmq",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\nimport glob\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_training_data.py\", line 9, in <module>\n    all_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \nNameError: name 'os' is not defined\n",
  "history_begin_time" : 1659447941160,
  "history_end_time" : 1660273336182,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "an1vFRu01MuN",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\nimport glob\nimport os\n\n# home directory\nhome = str(Path.home())\n\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*202207*.csv\"))) \ncmaq_df_from_each_hourly_file = (pd.read_csv(f, dtype=str) for f in all_hourly_files)\n\npath = '/groups/ESS/aalnaim/cmaq/observation'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*202207*.txt\")))\nobs_df_from_each_hourly_file = []\nfor f in all_hourly_files:\n    df = pd.read_csv(f, delim_whitespace=True)\n    df['YYYYMMDDHH'] = f[-14:-4] # get date from file name\n    obs_df_from_each_hourly_file.append(df)\n\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\", dtype=str)\n# print(\"ref_stations: \\n\",ref_stations.dtypes)\ntotalNulls = 0\nfor cmaq, obs in zip(cmaq_df_from_each_hourly_file, obs_df_from_each_hourly_file):\n    # print(\"CMAQ: \\n\",cmaq.dtypes)\n    # print(\"OBS: \\n\", obs.dtypes)\n    new_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\n    # print(\"nwe_df: \\n\", new_df.dtypes)\n    final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\n    final=final.drop_duplicates(keep=False)\n    training_data = final.loc[:,~final.columns.duplicated()]\n    training_data = training_data.rename(columns={\"OZONE(ppb)\": \"AirNOW_O3\"})\n    # print(\"Final columns: \", training_data.columns)\n\n    new_df=training_data.drop(['AQSID', 'Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\n    final_df = new_df[new_df.AirNOW_O3!= -999]\nprint(final_df)\n    # final_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)",
  "history_output" : "  File \"merge_training_data.py\", line 23\n    \",ref_stations.dtypes)\n                         ^\nSyntaxError: EOL while scanning string literal\n",
  "history_begin_time" : 1659447116167,
  "history_end_time" : 1659447183932,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "8c4jmdj1ozp",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1655072841907,
  "history_end_time" : 1655072871928,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "q6xfho4ovog",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/q6xfho4ovog/training_data.py\", line 6, in <module>\n    cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/cmaq/training_data.csv'\n",
  "history_begin_time" : 1654726158330,
  "history_end_time" : 1654726159022,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "sjun93cyvi6",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654618476619,
  "history_end_time" : 1654618513243,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Done"
},{
  "history_id" : "zdceibzqpdf",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654616662541,
  "history_end_time" : 1654616693872,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Done"
},{
  "history_id" : "zlj5hsns8b2",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654477372421,
  "history_end_time" : 1654477412005,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "rds3dk31r2m",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"training_data.py\", line 6, in <module>\n    cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 676, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 448, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 880, in __init__\n    self._make_engine(self.engine)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1114, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1891, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas\\_libs\\parsers.pyx\", line 374, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas\\_libs\\parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File C:\\Users\\JensenSun/cmaq/training_data.csv does not exist: 'C:\\\\Users\\\\JensenSun/cmaq/training_data.csv'\n",
  "history_begin_time" : 1654465997932,
  "history_end_time" : 1654465998460,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "004uo64dkrw",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654455998076,
  "history_end_time" : 1654456034671,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "roeaa3",
  "indicator" : "Failed"
},{
  "history_id" : "qlyn2mwvnqt",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654319680116,
  "history_end_time" : 1654319723096,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "owyhfo0w3kl",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654317825242,
  "history_end_time" : 1654317871335,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "of419xbamj1",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1654314622932,
  "history_end_time" : 1654314654959,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "wci4nagb4g6",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1653698191580,
  "history_end_time" : 1653698223309,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "3qpjgyglmi7",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1653698090704,
  "history_end_time" : 1653698125750,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "qnocu8u1wab",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"training_data.py\", line 6, in <module>\n    cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 676, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 448, in _read\n    parser = TextFileReader(fp_or_buf, **kwds)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 880, in __init__\n    self._make_engine(self.engine)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1114, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"C:\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1891, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas\\_libs\\parsers.pyx\", line 374, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas\\_libs\\parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File C:\\Users\\JensenSun/cmaq/training_data.csv does not exist: 'C:\\\\Users\\\\JensenSun/cmaq/training_data.csv'\n",
  "history_begin_time" : 1652934714029,
  "history_end_time" : 1652934714538,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8c139mg4j4u",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/8c139mg4j4u/training_data.py\", line 6, in <module>\n    cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/cmaq/training_data.csv'\n",
  "history_begin_time" : 1652897338645,
  "history_end_time" : 1652897339388,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "4ltasiwogfp",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1652831932017,
  "history_end_time" : 1652831959195,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "m56912soyvj",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1652831065367,
  "history_end_time" : 1652831094512,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "4k52pyetkx5",
  "history_input" : null,
  "history_output" : "Remote SCP command had error: scp: 4k52pyetkx5.tar: Disk quota exceeded",
  "history_begin_time" : 1652786135646,
  "history_end_time" : 1652786137654,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Failed"
},{
  "history_id" : "odu7qynvngj",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1652054956899,
  "history_end_time" : 1652054985072,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "xz2lii6giey",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"training_data.py\", line 7, in <module>\n    obs=pd.read_csv(home+\"/cmaq/observation.csv\")\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 680, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 575, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 933, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1217, in _make_engine\n    self.handles = get_handle(  # type: ignore[call-overload]\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/pandas/io/common.py\", line 789, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/aalnaim/cmaq/observation.csv'\n",
  "history_begin_time" : 1652047812791,
  "history_end_time" : 1652047846337,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "4cpthgcysff",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(home+\"/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/4cpthgcysff/training_data.py\", line 6, in <module>\n    cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/cmaq/training_data.csv'\n",
  "history_begin_time" : 1651361413318,
  "history_end_time" : 1651361414027,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "4lcbmdnkslv",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/4lcbmdnkslv/training_data.py\", line 7, in <module>\n    obs=pd.read_csv(home+\"/cmaq/observation.csv\")\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/mislam25/cmaq/observation.csv'\n",
  "history_begin_time" : 1650480291543,
  "history_end_time" : 1650480551470,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "7td16xnrxss",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/7td16xnrxss/training_data.py\", line 7, in <module>\n    obs=pd.read_csv(home+\"/cmaq/observation.csv\")\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/mislam25/cmaq/observation.csv'\n",
  "history_begin_time" : 1650473135420,
  "history_end_time" : 1650473497567,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "a7c91896dhw",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "/home/mislam25/gw-workspace/a7c91896dhw/training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n/home/mislam25/gw-workspace/a7c91896dhw/training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n/home/mislam25/gw-workspace/a7c91896dhw/training_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n/home/mislam25/gw-workspace/a7c91896dhw/training_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n/home/mislam25/gw-workspace/a7c91896dhw/training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1650252062981,
  "history_end_time" : 1650252114588,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "tylof4csr2r",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(home+\"/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nfinal.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "/home/mislam25/gw-workspace/tylof4csr2r/training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Longitude_y', 'Latitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n/home/mislam25/gw-workspace/tylof4csr2r/training_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\n/home/mislam25/gw-workspace/tylof4csr2r/training_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\n/home/mislam25/gw-workspace/tylof4csr2r/training_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\n/home/mislam25/gw-workspace/tylof4csr2r/training_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1650214654818,
  "history_end_time" : 1650214699383,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "rwpuseo5n4y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660294477696,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "7gemau61re7",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660854222320,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "588j1rmlbv5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660786975139,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "e3tndwc8isj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660295756074,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "uh9mj1plauf",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660777307030,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "ofbmzkhalw2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660853992767,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "h4gkz7hwtzk",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664422603415,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "5z6ohk00gcm",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664422878582,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},]
