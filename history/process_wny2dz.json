[{
  "history_id" : "n9onmfu1nal",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667925180802,
  "history_end_time" : 1667950923702,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "g7jiv7bzd2o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667892171252,
  "history_end_time" : 1667892171252,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "oi1wcmh4ogn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666685317316,
  "history_end_time" : 1666685918682,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "xyxqcoypsjd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666682218511,
  "history_end_time" : 1666683406481,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "a2ikiwzemmh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681701622,
  "history_end_time" : 1666681701622,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "wyqgpsbbvpl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681684495,
  "history_end_time" : 1667950920480,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "jc1c8ts800z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666681592875,
  "history_end_time" : 1666681655040,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "kwpsio9regc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666678879170,
  "history_end_time" : 1666678879170,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "ydvm5hzzt9w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666576238833,
  "history_end_time" : 1666577006619,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "cc5hrwisrgd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666553421688,
  "history_end_time" : 1666553446601,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "v3nr3l4jz0a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666542247517,
  "history_end_time" : 1666553383623,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "8vq0p7zulia",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665934797925,
  "history_end_time" : 1665935665908,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "d89oo8qqy5d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665932107450,
  "history_end_time" : 1665934029280,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "wn5xccyj6aw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930832296,
  "history_end_time" : 1665930832296,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "icugqpwscpa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930614449,
  "history_end_time" : 1665930705193,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "h3cafsd0qhx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665930050094,
  "history_end_time" : 1665930589621,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "oasy3jjucpb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665928489303,
  "history_end_time" : 1665928489303,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "1uavwqb6vuf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665926309048,
  "history_end_time" : 1665926309048,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "4ra82zl1b9b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665023826628,
  "history_end_time" : 1665032721246,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "2tvw1u8cosv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665021841926,
  "history_end_time" : 1665023785724,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "9g76b1m48oq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1665007808676,
  "history_end_time" : 1665007808676,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "t5pthx31you",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664983201565,
  "history_end_time" : 1665009008625,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "2pw6g991tkk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664949775897,
  "history_end_time" : 1664949775897,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "jowbo6ssxj9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664505829213,
  "history_end_time" : 1664505829213,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "9ohrr4kqj2o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664428239233,
  "history_end_time" : 1664428239233,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "v33buytvy0d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664427014335,
  "history_end_time" : 1664427014335,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Skipped"
},{
  "history_id" : "9uipdp63fr0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1664426709330,
  "history_end_time" : 1664426992849,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "tw6bgxcu6kq",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/zsu/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/zsun/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "/Users/joe/gw-workspace/tw6bgxcu6kq/gw-wB3v9thLphSxeLgJX96b3YsH4i-tw6bgxcu6kq.sh: line 5: import: command not found\n/Users/joe/gw-workspace/tw6bgxcu6kq/gw-wB3v9thLphSxeLgJX96b3YsH4i-tw6bgxcu6kq.sh: line 6: import: command not found\nfrom: can't read /var/mail/sklearn.ensemble\nfrom: can't read /var/mail/xgboost.sklearn\n/Users/joe/gw-workspace/tw6bgxcu6kq/gw-wB3v9thLphSxeLgJX96b3YsH4i-tw6bgxcu6kq.sh: line 9: import: command not found\nfrom: can't read /var/mail/pathlib\n/Users/joe/gw-workspace/tw6bgxcu6kq/gw-wB3v9thLphSxeLgJX96b3YsH4i-tw6bgxcu6kq.sh: line 13: syntax error near unexpected token `('\n/Users/joe/gw-workspace/tw6bgxcu6kq/gw-wB3v9thLphSxeLgJX96b3YsH4i-tw6bgxcu6kq.sh: line 13: `home = str(Path.home())'\n",
  "history_begin_time" : 1663783481504,
  "history_end_time" : 1663783482600,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "9uo0y6ufq4l",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/zsu/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/zsun/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "import: unable to open X server `' @ error/import.c/ImportImageCommand/344.\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-9uo0y6ufq4l.sh: line 7: from: command not found\n./geoweaver-9uo0y6ufq4l.sh: line 8: from: command not found\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-9uo0y6ufq4l.sh: line 10: from: command not found\n./geoweaver-9uo0y6ufq4l.sh: line 13: syntax error near unexpected token `('\n./geoweaver-9uo0y6ufq4l.sh: line 13: `home = str(Path.home())'\n",
  "history_begin_time" : 1660295562727,
  "history_end_time" : 1660295756091,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "ignx16d1ynm",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/zsu/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/zsun/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "import: unable to open X server `' @ error/import.c/ImportImageCommand/344.\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-ignx16d1ynm.sh: line 7: from: command not found\n./geoweaver-ignx16d1ynm.sh: line 8: from: command not found\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-ignx16d1ynm.sh: line 10: from: command not found\n./geoweaver-ignx16d1ynm.sh: line 13: syntax error near unexpected token `('\n./geoweaver-ignx16d1ynm.sh: line 13: `home = str(Path.home())'\n",
  "history_begin_time" : 1660294716962,
  "history_end_time" : 1660295761386,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "lxef5mcy9a3",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/zsu/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/zsun/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "history_output" : "import: unable to open X server `' @ error/import.c/ImportImageCommand/344.\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-lxef5mcy9a3.sh: line 7: from: command not found\n./geoweaver-lxef5mcy9a3.sh: line 8: from: command not found\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-lxef5mcy9a3.sh: line 10: from: command not found\n./geoweaver-lxef5mcy9a3.sh: line 13: syntax error near unexpected token `('\n./geoweaver-lxef5mcy9a3.sh: line 13: `home = str(Path.home())'\n",
  "history_begin_time" : 1660294392769,
  "history_end_time" : 1660294477702,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "VXjfI9amSo7g",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_one_month.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 679169\n",
  "history_begin_time" : 1660189744379,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "q4bdh7f933n",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_two_months.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 676556\n",
  "history_begin_time" : 1660166253901,
  "history_end_time" : 1660166318926,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "zu7b97v3xjx",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_fixed_two.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 644546\n",
  "history_begin_time" : 1660008066725,
  "history_end_time" : 1660008090532,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "m4ReuEhKz8Rr",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 638966\n",
  "history_begin_time" : 1659740772998,
  "history_end_time" : 1660273415698,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "rSHxUc52MZuL",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=60                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load gnu10                           \nmodule load python/3.8.6-ff\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637317\n",
  "history_begin_time" : 1659681381721,
  "history_end_time" : 1660273414616,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "F5iqH02VeLJX",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=60                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:5              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load gnu10                           \nmodule load python/3.8.6-ff\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637316\n",
  "history_begin_time" : 1659681336251,
  "history_end_time" : 1660273413516,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "LsBWhSivsCeG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=60                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:6              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637309\n",
  "history_begin_time" : 1659681043028,
  "history_end_time" : 1660273412465,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "lKcx9WrxFG2B",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:6              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637298\n",
  "history_begin_time" : 1659680608760,
  "history_end_time" : 1660273411866,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YE0PeT7b30z9",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637295\n",
  "history_begin_time" : 1659680570389,
  "history_end_time" : 1660273411364,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "HmpxxTeDLhq5",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637284\n",
  "history_begin_time" : 1659679886801,
  "history_end_time" : 1660273410832,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "USf43Kvafg7M",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659679799598,
  "history_end_time" : 1660273410165,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "6hgoU9xBaf0S",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637279\n",
  "history_begin_time" : 1659679761648,
  "history_end_time" : 1660273409548,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "XphRzej9LaLF",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=8000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637277\n",
  "history_begin_time" : 1659679721736,
  "history_end_time" : 1660273408764,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "kJcRsyU4wkH0",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659679690655,
  "history_end_time" : 1660273418299,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tKXgVAf1ArrM",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=16000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637275\n",
  "history_begin_time" : 1659679660825,
  "history_end_time" : 1660273418780,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "0IHmZO8rvvMo",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=8000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637273\n",
  "history_begin_time" : 1659679621296,
  "history_end_time" : 1660273419247,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "MQBa7yyza4Xe",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq_gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=8000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637272\n",
  "history_begin_time" : 1659679573995,
  "history_end_time" : 1660273420198,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tNX5vCs1QykG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:6              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=8000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637269\n",
  "history_begin_time" : 1659679427984,
  "history_end_time" : 1660273421114,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "ufBPIe45g8Ux",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:6              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659679324276,
  "history_end_time" : 1660273421648,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "rDlJ1uKO4qIL",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659679218190,
  "history_end_time" : 1660273422214,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "Zoio5N1T4FnT",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:8              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32000M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659679131529,
  "history_end_time" : 1660273422799,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "FZSQ6fNWIDPD",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=40G                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659678986017,
  "history_end_time" : 1660273423315,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "GUzVP3REAOjl",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32G                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "sbatch: error: Batch job submission failed: Requested node configuration is not available\n",
  "history_begin_time" : 1659678867433,
  "history_end_time" : 1660273423962,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "ARXUfGfbdn5z",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=128                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=32G                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637242\n",
  "history_begin_time" : 1659678824756,
  "history_end_time" : 1660273426081,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "eHzfABWXr3Hk",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 637239\n",
  "history_begin_time" : 1659678552013,
  "history_end_time" : 1660273426681,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "BwwVjffZm9Gf",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_one_year.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : null,
  "history_begin_time" : 1659678493416,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : null
},{
  "history_id" : "bhuykvi7l2x",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_aug3.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 626341\n",
  "history_begin_time" : 1659578695329,
  "history_end_time" : 1659578719078,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "pk9T647qn8ca",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 621786\n",
  "history_begin_time" : 1659448689525,
  "history_end_time" : 1660273427436,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "J3p7hLiykEus",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 621780\n",
  "history_begin_time" : 1659448086269,
  "history_end_time" : 1660273427964,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "JgKAzkETUJbG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_Jun14.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317724\n",
  "history_begin_time" : 1655309069523,
  "history_end_time" : 1656319034768,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "QpVpvZwzKKd8",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317275\n",
  "history_begin_time" : 1655240792433,
  "history_end_time" : 1655308889463,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YiUac61u0NQs",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317274\n",
  "history_begin_time" : 1655240750532,
  "history_end_time" : 1655308888978,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "06xaefcrFtgo",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309474\n",
  "history_begin_time" : 1655211911278,
  "history_end_time" : 1655214661551,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "iLhFC3pj2OCm",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309359\n",
  "history_begin_time" : 1655176898978,
  "history_end_time" : 1655214662184,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "vo4duplBk62q",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309343\n",
  "history_begin_time" : 1655174481120,
  "history_end_time" : 1655214662658,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "W0OalQdYgJKf",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309342\n",
  "history_begin_time" : 1655174426600,
  "history_end_time" : 1655214663796,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "HkNaF85gIEwR",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309206\n",
  "history_begin_time" : 1655161058930,
  "history_end_time" : 1655214664277,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "vp02fazn0j0",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "/Users/joe/gw-workspace/vp02fazn0j0/gw-LmIFrVaH5Og4xhr8dHoWtrFmoU-vp02fazn0j0.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: No such file or directory\n/Users/joe/gw-workspace/vp02fazn0j0/gw-LmIFrVaH5Og4xhr8dHoWtrFmoU-vp02fazn0j0.sh: line 62: sbatch: command not found\n",
  "history_begin_time" : 1655077546361,
  "history_end_time" : 1655077567487,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gesfd0y495a",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "/Users/joe/gw-workspace/gesfd0y495a/gw-LBGV0M7ZFdBi83o7GYJ5WwKXO8-gesfd0y495a.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: No such file or directory\n/Users/joe/gw-workspace/gesfd0y495a/gw-LBGV0M7ZFdBi83o7GYJ5WwKXO8-gesfd0y495a.sh: line 62: sbatch: command not found\n",
  "history_begin_time" : 1655077398675,
  "history_end_time" : 1655077419875,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "im1go54hrya",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "/Users/joe/gw-workspace/im1go54hrya/gw-PcR1mtJHXYGfOfdBRSDO4RmtPJ-im1go54hrya.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: No such file or directory\n/Users/joe/gw-workspace/im1go54hrya/gw-PcR1mtJHXYGfOfdBRSDO4RmtPJ-im1go54hrya.sh: line 62: sbatch: command not found\n",
  "history_begin_time" : 1655077235874,
  "history_end_time" : 1655077257041,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "l96UTcOSv9BW",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 294124\n",
  "history_begin_time" : 1655075899025,
  "history_end_time" : 1655214664758,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "baa04m11lwy",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 293687\n",
  "history_begin_time" : 1655073603587,
  "history_end_time" : 1655073626601,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "dfflr7qie7w",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "/Users/joe/gw-workspace/dfflr7qie7w/gw-y9QAkQeFWK0U8nYeTDEGZLPZ9Z-dfflr7qie7w.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: No such file or directory\n/Users/joe/gw-workspace/dfflr7qie7w/gw-y9QAkQeFWK0U8nYeTDEGZLPZ9Z-dfflr7qie7w.sh: line 62: sbatch: command not found\n",
  "history_begin_time" : 1655067110502,
  "history_end_time" : 1655067131706,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tr1q8l4nb5t",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "/Users/joe/gw-workspace/tr1q8l4nb5t/gw-2xbe0nof3cpAB0cUnC0XxQlQm3-tr1q8l4nb5t.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: No such file or directory\n/Users/joe/gw-workspace/tr1q8l4nb5t/gw-2xbe0nof3cpAB0cUnC0XxQlQm3-tr1q8l4nb5t.sh: line 62: sbatch: command not found\n",
  "history_begin_time" : 1654726166407,
  "history_end_time" : 1654726187587,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qhncf2pso33",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20\n",
  "history_output" : "./geoweaver-qhncf2pso33.sh: line 60: /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm: Permission denied\nSubmitted batch job 269738\n",
  "history_begin_time" : 1654618900743,
  "history_end_time" : 1654618937684,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Done"
},{
  "history_id" : "yDvdEvgMmIn4",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 269679\n",
  "history_begin_time" : 1654612002594,
  "history_end_time" : 1654619308939,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "hwrti1aWxHMv",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 269677\n",
  "history_begin_time" : 1654611898350,
  "history_end_time" : 1654619308373,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "Lep1ufXcecjC",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 269675\n",
  "history_begin_time" : 1654611695981,
  "history_end_time" : 1654619307839,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "3xBA7r0B2HkM",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268230\n",
  "history_begin_time" : 1654536599783,
  "history_end_time" : 1654619307243,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "Z5kUr2I8qzhb",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nprint('IT WORKED')\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654536360346,
  "history_end_time" : 1654536532829,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "wn2hbzhu2ay",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nprint(\"IT WORKED\")\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654535458975,
  "history_end_time" : 1654535464146,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "MsjbjCC2EwMj",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268141\n",
  "history_begin_time" : 1654491910593,
  "history_end_time" : 1654535259974,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "cvbKVxUSdrbt",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268139\n",
  "history_begin_time" : 1654491776428,
  "history_end_time" : 1654535260558,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tGQb2syaWuR0",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268137\n",
  "history_begin_time" : 1654491629015,
  "history_end_time" : 1654535261059,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "usex3SwJ6mTS",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268135\n",
  "history_begin_time" : 1654491425039,
  "history_end_time" : 1654535261590,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "xt19rQLKb5hk",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\necho \"# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\" >> /home/aalnaim/rf_pyCaret.py\n\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\necho # Write first python in Geoweaver# NASA GEOWEAVER\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-xt19rQLKb5hk.sh: line 25: from: command not found\n./geoweaver-xt19rQLKb5hk.sh: line 26: from: command not found\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-xt19rQLKb5hk.sh: line 28: from: command not found\n./geoweaver-xt19rQLKb5hk.sh: line 31: syntax error near unexpected token `('\n./geoweaver-xt19rQLKb5hk.sh: line 31: `home = str(Path.home())'\n",
  "history_begin_time" : 1654491367280,
  "history_end_time" : 1654491394414,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "q6ogNuO8i4ck",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268133\n",
  "history_begin_time" : 1654491162242,
  "history_end_time" : 1654491393867,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "nfkZqgovRjlG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268131\n",
  "history_begin_time" : 1654490903840,
  "history_end_time" : 1654491393398,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "PamN9MxUU1K4",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268130\n",
  "history_begin_time" : 1654490758893,
  "history_end_time" : 1654491392901,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "zTC26oVAvMNv",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\" >> cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654490278351,
  "history_end_time" : 1654490735341,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "VfWXgYKIY46f",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268129\n",
  "history_begin_time" : 1654490091612,
  "history_end_time" : 1654490734793,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "OsW06ht15wmz",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268128\n",
  "history_begin_time" : 1654490008108,
  "history_end_time" : 1654490081925,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "yRMgjX7O036v",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" > cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268127\n",
  "history_begin_time" : 1654489311859,
  "history_end_time" : 1654489851171,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "dUmujbU0K8pG",
  "history_input" : "echo \"\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\n\" > cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "sbatch: error: This does not look like a batch script.  The first\nsbatch: error: line must start with #! followed by the path to an interpreter.\nsbatch: error: For instance: #!/bin/sh\n",
  "history_begin_time" : 1654489261923,
  "history_end_time" : 1654489851834,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "9VEMKfOKzdSL",
  "history_input" : "cat <<EOF >>cmaq_gpu.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\nEOF\n\necho \"Done!\"\necho $(pwd)",
  "history_output" : "./geoweaver-9VEMKfOKzdSL.sh: line 59: EOF: command not found\nDone!\n/home/aalnaim/gw-workspace/9VEMKfOKzdSL\n",
  "history_begin_time" : 1654488845481,
  "history_end_time" : 1654489852569,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YZpAntSz921L",
  "history_input" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py",
  "history_output" : "./geoweaver-YZpAntSz921L.sh: line 16: nvidia-smi: command not found\nLmod has detected the following error: The following module(s) are unknown:\n\"hosts/dgx\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date; it may help to try:\n  $ module --ignore-cache load \"hosts/dgx\"\n\nAlso make sure that all modulefiles written in TCL start with the string\n#%Module\n\n\n\n   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   29.489082   -81.276833        1.0  ...      3   15     12\n1   40.580200   -74.199402       37.0  ...      3   15     12\n2   39.128860   -84.504044       36.0  ...      3   15     12\n3   41.096157   -80.658905       26.0  ...      3   15     12\n4   34.635960   -82.810669       39.0  ...      3   15     12\n\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1654488620622,
  "history_end_time" : 1654488708322,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "XCUgtFWm1NCS",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-XCUgtFWm1NCS.sh: line 61: python: command not found\n./geoweaver-XCUgtFWm1NCS.sh: line 62: EOF: command not found\nSubmitted batch job 268125\n",
  "history_begin_time" : 1654488451251,
  "history_end_time" : 1654488696580,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "eIfu6yALskJK",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\npython <<EOF \n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\n\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-eIfu6yALskJK.sh: line 62: EOF: command not found\nSubmitted batch job 268124\n",
  "history_begin_time" : 1654488395089,
  "history_end_time" : 1654488418021,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "vhhKiNPD7vzO",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-vhhKiNPD7vzO.sh: line 61: python: command not found\n./geoweaver-vhhKiNPD7vzO.sh: line 62: EOF: command not found\nSubmitted batch job 268123\n",
  "history_begin_time" : 1654488249524,
  "history_end_time" : 1654488417507,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "AuddkGqBHLQo",
  "history_input" : "cat <<EOF > cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-AuddkGqBHLQo.sh: line 61: python: command not found\n./geoweaver-AuddkGqBHLQo.sh: line 62: EOF: command not found\nSubmitted batch job 268122\n",
  "history_begin_time" : 1654487557488,
  "history_end_time" : 1654488416989,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "S4lxbt3v3lbO",
  "history_input" : "#!/bin/bash\ncat <<EOF > cmaq.slurm\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-S4lxbt3v3lbO.sh: line 61: python: command not found\n./geoweaver-S4lxbt3v3lbO.sh: line 62: EOF: command not found\nsbatch: error: This does not look like a batch script.  The first\nsbatch: error: line must start with #! followed by the path to an interpreter.\nsbatch: error: For instance: #!/bin/sh\n",
  "history_begin_time" : 1654487491532,
  "history_end_time" : 1654488416191,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "myxZd3pn5O6C",
  "history_input" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py",
  "history_output" : "./geoweaver-myxZd3pn5O6C.sh: line 16: nvidia-smi: command not found\nLmod has detected the following error: The following module(s) are unknown:\n\"hosts/dgx\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date; it may help to try:\n  $ module --ignore-cache load \"hosts/dgx\"\n\nAlso make sure that all modulefiles written in TCL start with the string\n#%Module\n\n\n\n   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   29.489082   -81.276833        1.0  ...      3   15     12\n1   40.580200   -74.199402       37.0  ...      3   15     12\n2   39.128860   -84.504044       36.0  ...      3   15     12\n3   41.096157   -80.658905       26.0  ...      3   15     12\n4   34.635960   -82.810669       39.0  ...      3   15     12\n\n[5 rows x 18 columns]\n/home/aalnaim/CMAQAI/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n  warn(\n",
  "history_begin_time" : 1654487302837,
  "history_end_time" : 1654487384411,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "9zq8twzownb",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1660189690453,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Stopped"
},{
  "history_id" : "1vv9vtc7v2t",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664422603430,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},{
  "history_id" : "z7x9epogdpb",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664422878598,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "aqt8fv",
  "indicator" : "Stopped"
},]
