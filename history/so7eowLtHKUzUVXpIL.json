[{
  "history_id" : "j9fobfrxom7",
  "history_input" : "# get all of the cmaq model output variables\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2022, 8, 1)   # start date\nedate = date(2022, 8, 5)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\n\nfor day in days:\n\tprint(\"Getting data for: \"+day)\n    \n\t# read cmaq results\n\tdf_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+day+\"_extracted.nc\")\n    \n\t# read mcip results \n\tdf_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+day+\".nc\")\n        \n\t# read emissions results \n\tdf_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n        \n\tfor k in time_step_in_netcdf_list:\n\t\treal_hour_value = real_hour_list[k]\n            \n\t\tdf_hourly = pd.DataFrame()\n            \n\t\t# CMAQ data\n            \n\t\t# O3 variable\n\t\to3=df_cmaq.variables['O3'][:].values[k,0]\n\t\tcmaq_O3=list(np.ravel(o3).transpose().round())  \n            \n\t\t# NO2\n\t\tno2=df_cmaq.variables['NO2'][:].values[k,0]\n\t\tcmaq_NO2=list(np.ravel(no2).transpose().round())\n      \n\t\t# CO\n\t\tco=df_cmaq.variables['CO'][:].values[k,0]\n\t\tcmaq_CO=list(np.ravel(co).transpose().round())\n\n\t\t# PM25_CO\n\t\tpm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n\t\tcmaq_PM25_CO=list(np.ravel(pm25).transpose().round())\n            \n                              \n                              \n\t\t# EMIS data\n                              \n\t\tco_emis=df_emis.variables['CO'][:].values[k,0]\n\t\tCO_emi=list(np.ravel(co_emis).transpose().round())    \n                \n                              \n                              \n\t\t# MCIP data\n            \n\t\t# CO variable\n\t\tprsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n\t\tPRSFC=list(np.ravel(prsfc).transpose().round())\n            \n\t\t# NO2\n\t\tpbl=df_mcip.variables['PBL'][:].values[k,0]\n\t\tPBL=list(np.ravel(pbl).transpose().round())\n            \n\t\t# TEMP2\n\t\ttemp2=df_mcip.variables['TEMP2'][:].values[k,0]\n\t\tTEMP2=list(np.ravel(temp2).transpose().round())\n            \n\t\t# WSPD10\n\t\twspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n\t\tWSPD10=list(np.ravel(wspd10).transpose().round())\n            \n\t\t# WDIR10\n\t\twdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n\t\tWDIR10=list(np.ravel(wdir10).transpose().round())\n\n\t\t# RGRND\n\t\trgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n\t\tRGRND=list(np.ravel(rgrnd).transpose().round())\n\n\t\t# CFRAC\n\t\tcfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n\t\tCFRAC=list(np.ravel(cfrac).transpose().round())\n\n            \n\t\t## LAT/LON data\n\t\tdf_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n            \n\t\tlat = df_coords.variables['LAT'][:].values[0,0]\n\t\tlat_flt=np.ravel(lat)\n\t\tLAT=np.tile(lat_flt,1)\n\n\t\tlon = df_coords.variables['LON'][:].values[0,0]\n\t\tlon_flt=np.ravel(lon)\n\t\tLON=np.tile(lon_flt,1)\n            \n\t\tdf_hourly['Latitude'] = LAT\n\t\tdf_hourly['Longitude'] = LON\n\t\tdf_hourly['YYYYMMDDHH'] = day+str(real_hour_value)\n\t\tdf_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n\t\tdf_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n\t\tdf_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n\t\tdf_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n\t\tdf_hourly['CO(moles/s)'] = CO_emi\n\t\tdf_hourly['PRSFC(Pa)'] = PRSFC\n\t\tdf_hourly['PBL(m)'] = PBL\n\t\tdf_hourly['TEMP2(K)'] = TEMP2\n\t\tdf_hourly['WSPD10(m/s)'] = WSPD10\n\t\tdf_hourly['WDIR10(degree)'] = WDIR10\n\t\tdf_hourly['RGRND(W/m2)'] = RGRND\n\t\tdf_hourly['CFRAC'] = CFRAC\n\t\tdf_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n\t\tdf_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n\t\tdf_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n            \n            \n\t\tprint('Saving file: train_data_'+day+'_'+str(real_hour_value)+'.csv')\n\t\tdf_hourly.to_csv('/groups/ESS/aalnaim/cmaq/training_input_hourly/train_data_'+day+'_'+str(real_hour_value)+'.csv',index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220801\nSaving file: train_data_20220801_12.csv\nSaving file: train_data_20220801_13.csv\nSaving file: train_data_20220801_14.csv\nSaving file: train_data_20220801_15.csv\nSaving file: train_data_20220801_16.csv\nSaving file: train_data_20220801_17.csv\nSaving file: train_data_20220801_18.csv\nSaving file: train_data_20220801_19.csv\nSaving file: train_data_20220801_20.csv\nSaving file: train_data_20220801_21.csv\nSaving file: train_data_20220801_22.csv\nSaving file: train_data_20220801_23.csv\nSaving file: train_data_20220801_0.csv\nSaving file: train_data_20220801_1.csv\nSaving file: train_data_20220801_2.csv\nSaving file: train_data_20220801_3.csv\nSaving file: train_data_20220801_4.csv\nSaving file: train_data_20220801_5.csv\nSaving file: train_data_20220801_6.csv\nSaving file: train_data_20220801_7.csv\nSaving file: train_data_20220801_8.csv\nSaving file: train_data_20220801_9.csv\nSaving file: train_data_20220801_10.csv\nSaving file: train_data_20220801_11.csv\nGetting data for: 20220802\nSaving file: train_data_20220802_12.csv\nSaving file: train_data_20220802_13.csv\nSaving file: train_data_20220802_14.csv\nSaving file: train_data_20220802_15.csv\nSaving file: train_data_20220802_16.csv\nSaving file: train_data_20220802_17.csv\nSaving file: train_data_20220802_18.csv\nSaving file: train_data_20220802_19.csv\nSaving file: train_data_20220802_20.csv\nSaving file: train_data_20220802_21.csv\nSaving file: train_data_20220802_22.csv\nSaving file: train_data_20220802_23.csv\nSaving file: train_data_20220802_0.csv\nSaving file: train_data_20220802_1.csv\nSaving file: train_data_20220802_2.csv\nSaving file: train_data_20220802_3.csv\nSaving file: train_data_20220802_4.csv\nSaving file: train_data_20220802_5.csv\nSaving file: train_data_20220802_6.csv\nSaving file: train_data_20220802_7.csv\nSaving file: train_data_20220802_8.csv\nSaving file: train_data_20220802_9.csv\nSaving file: train_data_20220802_10.csv\nSaving file: train_data_20220802_11.csv\nGetting data for: 20220803\nSaving file: train_data_20220803_12.csv\nSaving file: train_data_20220803_13.csv\nSaving file: train_data_20220803_14.csv\nSaving file: train_data_20220803_15.csv\nSaving file: train_data_20220803_16.csv\nSaving file: train_data_20220803_17.csv\nSaving file: train_data_20220803_18.csv\nSaving file: train_data_20220803_19.csv\nSaving file: train_data_20220803_20.csv\nSaving file: train_data_20220803_21.csv\nSaving file: train_data_20220803_22.csv\nSaving file: train_data_20220803_23.csv\nSaving file: train_data_20220803_0.csv\nSaving file: train_data_20220803_1.csv\nSaving file: train_data_20220803_2.csv\nSaving file: train_data_20220803_3.csv\nSaving file: train_data_20220803_4.csv\nSaving file: train_data_20220803_5.csv\nSaving file: train_data_20220803_6.csv\nSaving file: train_data_20220803_7.csv\nSaving file: train_data_20220803_8.csv\nSaving file: train_data_20220803_9.csv\nSaving file: train_data_20220803_10.csv\nSaving file: train_data_20220803_11.csv\nGetting data for: 20220804\nSaving file: train_data_20220804_12.csv\nSaving file: train_data_20220804_13.csv\nSaving file: train_data_20220804_14.csv\nSaving file: train_data_20220804_15.csv\nSaving file: train_data_20220804_16.csv\nSaving file: train_data_20220804_17.csv\nSaving file: train_data_20220804_18.csv\nSaving file: train_data_20220804_19.csv\nSaving file: train_data_20220804_20.csv\nSaving file: train_data_20220804_21.csv\nSaving file: train_data_20220804_22.csv\nSaving file: train_data_20220804_23.csv\nSaving file: train_data_20220804_0.csv\nSaving file: train_data_20220804_1.csv\nSaving file: train_data_20220804_2.csv\nSaving file: train_data_20220804_3.csv\nSaving file: train_data_20220804_4.csv\nSaving file: train_data_20220804_5.csv\nSaving file: train_data_20220804_6.csv\nSaving file: train_data_20220804_7.csv\nSaving file: train_data_20220804_8.csv\nSaving file: train_data_20220804_9.csv\nSaving file: train_data_20220804_10.csv\nSaving file: train_data_20220804_11.csv\nGetting data for: 20220805\nSaving file: train_data_20220805_12.csv\nSaving file: train_data_20220805_13.csv\nSaving file: train_data_20220805_14.csv\nSaving file: train_data_20220805_15.csv\nSaving file: train_data_20220805_16.csv\nSaving file: train_data_20220805_17.csv\nSaving file: train_data_20220805_18.csv\nSaving file: train_data_20220805_19.csv\nSaving file: train_data_20220805_20.csv\nSaving file: train_data_20220805_21.csv\nSaving file: train_data_20220805_22.csv\nSaving file: train_data_20220805_23.csv\nSaving file: train_data_20220805_0.csv\nSaving file: train_data_20220805_1.csv\nSaving file: train_data_20220805_2.csv\nSaving file: train_data_20220805_3.csv\nSaving file: train_data_20220805_4.csv\nSaving file: train_data_20220805_5.csv\nSaving file: train_data_20220805_6.csv\nSaving file: train_data_20220805_7.csv\nSaving file: train_data_20220805_8.csv\nSaving file: train_data_20220805_9.csv\nSaving file: train_data_20220805_10.csv\nSaving file: train_data_20220805_11.csv\nDone!\n",
  "history_begin_time" : 1660007420713,
  "history_end_time" : 1660007981886,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "q6yuua9vjqi",
  "history_input" : "# get all the airnow station data\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta, date\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\n\nsdate = date(2022, 8, 1)   # start date\nedate = date(2022, 8, 5)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n    for t in time:\n        files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n        with open(files, 'r') as file:\n            text = file.read()\n            new_string = text.replace('\"', '')\n\n        outF = open(\"/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_\"+i+t+\".txt\", \"w\")\n        for line in new_string:\n          # write line to output file\n          outF.write(line)\n        outF.close()",
  "history_output" : "Running",
  "history_begin_time" : 1660007983756,
  "history_end_time" : 1660007995465,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "j0bfxiespfa",
  "history_input" : "# take all the airnow observations and merge into one observation.csv\n\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import datetime, date, timedelta\n# home directory\nhome = str(Path.home())\n\ndays=[]\n\n\nsdate = date(2022, 8, 1)   # start date\nedate = date(2022, 8, 5)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\n\nfor d in days:\n    for t in time:\n\n        files=glob.glob(\"/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n        for file in files:\n            print(file)\n\n            data = np.loadtxt(file, skiprows=1,dtype='str')\n            dt=d+t\n            print(dt)\n            dt=np.tile(dt,len(data)) # constructs an array for each hour of each day with length of data from total stations available\n            date_time.append(dt)\n            merged.append(data)\n            \ndata_frame = np.concatenate(merged)\n\n# This gets the first 4 columns in the observation file (AQSID, Latitude, Longitude, OZONE(ppb))\ndata_frame = np.delete(data_frame, np.s_[4:9], axis=1) \n\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\ndff=df.replace(',','', regex=True)\n\ndt = np.concatenate(date_time)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\",index=False)",
  "history_output" : "/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080112.txt\n2022080112\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080113.txt\n2022080113\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080114.txt\n2022080114\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080115.txt\n2022080115\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080116.txt\n2022080116\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080117.txt\n2022080117\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080118.txt\n2022080118\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080119.txt\n2022080119\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080120.txt\n2022080120\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080121.txt\n2022080121\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080122.txt\n2022080122\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080123.txt\n2022080123\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080100.txt\n2022080100\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080101.txt\n2022080101\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080102.txt\n2022080102\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080103.txt\n2022080103\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080104.txt\n2022080104\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080105.txt\n2022080105\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080106.txt\n2022080106\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080107.txt\n2022080107\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080108.txt\n2022080108\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080109.txt\n2022080109\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080110.txt\n2022080110\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080111.txt\n2022080111\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080212.txt\n2022080212\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080213.txt\n2022080213\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080214.txt\n2022080214\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080215.txt\n2022080215\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080216.txt\n2022080216\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080217.txt\n2022080217\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080218.txt\n2022080218\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080219.txt\n2022080219\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080220.txt\n2022080220\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080221.txt\n2022080221\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080222.txt\n2022080222\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080223.txt\n2022080223\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080200.txt\n2022080200\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080201.txt\n2022080201\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080202.txt\n2022080202\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080203.txt\n2022080203\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080204.txt\n2022080204\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080205.txt\n2022080205\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080206.txt\n2022080206\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080207.txt\n2022080207\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080208.txt\n2022080208\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080209.txt\n2022080209\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080210.txt\n2022080210\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080211.txt\n2022080211\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080312.txt\n2022080312\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080313.txt\n2022080313\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080314.txt\n2022080314\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080315.txt\n2022080315\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080316.txt\n2022080316\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080317.txt\n2022080317\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080318.txt\n2022080318\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080319.txt\n2022080319\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080320.txt\n2022080320\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080321.txt\n2022080321\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080322.txt\n2022080322\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080323.txt\n2022080323\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080300.txt\n2022080300\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080301.txt\n2022080301\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080302.txt\n2022080302\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080303.txt\n2022080303\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080304.txt\n2022080304\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080305.txt\n2022080305\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080306.txt\n2022080306\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080307.txt\n2022080307\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080308.txt\n2022080308\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080309.txt\n2022080309\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080310.txt\n2022080310\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080311.txt\n2022080311\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080412.txt\n2022080412\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080413.txt\n2022080413\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080414.txt\n2022080414\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080415.txt\n2022080415\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080416.txt\n2022080416\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080417.txt\n2022080417\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080418.txt\n2022080418\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080419.txt\n2022080419\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080420.txt\n2022080420\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080421.txt\n2022080421\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080422.txt\n2022080422\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080423.txt\n2022080423\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080400.txt\n2022080400\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080401.txt\n2022080401\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080402.txt\n2022080402\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080403.txt\n2022080403\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080404.txt\n2022080404\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080405.txt\n2022080405\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080406.txt\n2022080406\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080407.txt\n2022080407\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080408.txt\n2022080408\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080409.txt\n2022080409\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080410.txt\n2022080410\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080411.txt\n2022080411\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080512.txt\n2022080512\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080513.txt\n2022080513\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080514.txt\n2022080514\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080515.txt\n2022080515\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080516.txt\n2022080516\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080517.txt\n2022080517\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080518.txt\n2022080518\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080519.txt\n2022080519\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080520.txt\n2022080520\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080521.txt\n2022080521\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080522.txt\n2022080522\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080523.txt\n2022080523\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080500.txt\n2022080500\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080501.txt\n2022080501\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080502.txt\n2022080502\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080503.txt\n2022080503\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080504.txt\n2022080504\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080505.txt\n2022080505\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080506.txt\n2022080506\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080507.txt\n2022080507\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080508.txt\n2022080508\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080509.txt\n2022080509\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080510.txt\n2022080510\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022080511.txt\n2022080511\n",
  "history_begin_time" : 1660007996956,
  "history_end_time" : 1660008015531,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "80yywkruqo3",
  "history_input" : "# combine cmaq and airnow into training.csv\n\nimport pandas as pd\nfrom pathlib import Path\nimport glob, os\n\n# home directory\n\n# cmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\npath = '/groups/ESS/aalnaim/cmaq/training_input_hourly'\nall_hourly_files = sorted(glob.glob(os.path.join(path, \"*.csv\"))) \ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file)\n\nobs=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/observation/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(\"/groups/ESS/aalnaim/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(\"/groups/ESS/aalnaim/cmaq/training.csv\",index=False)\n\n\n",
  "history_output" : "merge_training_data.py:20: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\nmerge_training_data.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\nmerge_training_data.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\nmerge_training_data.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\nmerge_training_data.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1660008015542,
  "history_end_time" : 1660008066678,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "nisfh1gx2c6",
  "history_input" : "# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport glob, os\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# home directory\nhome = str(Path.home())\n# importing data\nfinal=pd.read_csv(\"/groups/ESS/aalnaim/cmaq/input_hourly/testing.csv\")\nprint(final.head())\nX = final.drop(['YYYYMMDDHH','Latitude','Longitude',],axis=1)\ny= final['CMAQ12KM_O3(ppb)']\n# defining  testing variables\n# processing test data\n\n# load the model from disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_fixed_two.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\n# making prediction\npred = loaded_model.predict(X)\n\n# adding prediction values to test dataset\nfinal['prediction'] = pred.tolist()\n\nfinal = final[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n# saving the dataset into local drive\nfinal.to_csv('/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf.csv',index=False)",
  "history_output" : "    Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0  21.829086 -120.620790  2022080323  ...      8    3     23\n1  21.855751 -120.512500  2022080323  ...      8    3     23\n2  21.882309 -120.404144  2022080323  ...      8    3     23\n3  21.908745 -120.295715  2022080323  ...      8    3     23\n4  21.935051 -120.187225  2022080323  ...      8    3     23\n\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1660008357126,
  "history_end_time" : 1660008425394,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "zu7b97v3xjx",
  "history_input" : "# train the model using training.csv\n\n\necho \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/aalnaim/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_from_hourly_fixed_two.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 644546\n",
  "history_begin_time" : 1660008066725,
  "history_end_time" : 1660008090532,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "qoy6v59jglk",
  "history_input" : "# load the prediction_rf.csv into a NetCDF file for visualization\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nfrom datetime import timedelta\n# home directory\nhome = str(Path.home())\n\nbase = datetime.datetime.today() - timedelta(days=2)\ndate_list = [base + timedelta(days=x) for x in range(2)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\ndays = ['20220803', '20220804']\n\n# nc file need to correspond to the same prediction date in \"/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf_Jun13.csv\"\ndf_cdf = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+days[0]+\"_extracted.nc\")\n\ndf_csv = pd.read_csv(\"/groups/ESS/aalnaim/cmaq/prediction_files/prediction_rf.csv\")\n\ndf_csv['YYYYMMDDHH'] = df_csv['YYYYMMDDHH'].astype(str)\ndf_filt = df_csv[df_csv['YYYYMMDDHH'].str.contains(days[1]+\"|\"+days[0], case = False, regex=True)]\ndf_filt = df_filt[(df_filt['YYYYMMDDHH'] > days[0]+'11') & (df_filt['YYYYMMDDHH'] < days[1]+'12')]\n\n\n# Reshape \"prediction/Latitude/Longitude\" columns to (TSTEP, ROW, COL), these lines will reshape data into (24, 265, 442)\nreshaped_prediction = np.atleast_3d(df_filt['prediction']).reshape(-1, 265, 442)\n\n# Remove \"LAY\" Dimension in O3 variable already in nc file.\nreduced_dim = df_cdf['O3'].sel(LAY=1, drop=True)\n# Swap values from original nc file with new prediction data\nreduced_dim.values = reshaped_prediction\n\n# Apply changes to data variable in nc file\ndf_cdf['O3'] = (['TSTEP', 'ROW', 'COL'], reshaped_prediction)\n\ndf_cdf.to_netcdf('/groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+days[0]+'_'+days[1]+'_ML_extracted.nc')\n\nprint('Saved updated netCDF file: /groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+days[0]+'_'+days[1]+'_ML_extracted.nc')",
  "history_output" : "Saved updated netCDF file: /groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20220803_20220804_ML_extracted.nc\n",
  "history_begin_time" : 1660008425874,
  "history_end_time" : 1660008622120,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "dvrfr7awi6c",
  "history_input" : "# install all dependencies\n# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\nwith open('requirements.txt','w') as out:\n  out.write('''\nabsl-py==1.0.0\naffine==2.3.1\nasttokens==2.0.5\nastunparse==1.6.3\nattrs==21.4.0\nautokeras==1.0.18\nbackcall==0.2.0\ncachetools==5.0.0\ncertifi==2021.10.8\ncftime==1.6.0\ncharset-normalizer==2.0.12\nclick==8.1.3\nclick-plugins==1.1.1\ncligj==0.7.2\ncmaps==1.0.5\ncycler==0.11.0\ndecorator==5.1.1\nearthpy==0.9.4\nexecuting==0.8.3\nFiona==1.8.21\nflatbuffers==2.0\nfonttools==4.29.1\ngast==0.5.3\ngeopandas==0.10.2\nglob2==0.7\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.44.0\nh5py==3.6.0\nidna==3.3\nimageio==2.19.0\nimageio-ffmpeg==0.4.7\nimportlib-metadata==4.11.2\nipython==8.1.1\njedi==0.18.1\njoblib==1.1.0\nkaleido==0.2.1\nkeras==2.8.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.0\nkiwisolver==1.3.2\nkt-legacy==1.0.4\nlibclang==13.0.0\nMarkdown==3.3.6\nmatplotlib==3.5.1\nmatplotlib-inline==0.1.3\nmunch==2.5.0\nnetCDF4==1.5.8\nnetworkx==2.8\nnumpy==1.22.2\noauthlib==3.2.0\nopencv-python==4.5.5.64\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.1\nparso==0.8.3\npathlib==1.0.1\npathlib2==2.3.7.post1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.0.1\nplotly==5.7.0\nprompt-toolkit==3.0.28\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\nPygments==2.11.2\npyparsing==3.0.7\npyproj==3.3.1\npython-dateutil==2.8.2\npytz==2021.3\nPyWavelets==1.3.0\nrasterio==1.2.10\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscikit-image==0.19.2\nscikit-learn==1.0.2\nscipy==1.8.0\nseaborn==0.11.2\nShapely==1.8.2\nsix==1.16.0\nsklearn==0.0\nsnuggs==1.4.7\nstack-data==0.2.0\ntenacity==8.0.1\ntensorboard==2.8.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.8.0\ntensorflow-gpu==2.8.0\ntensorflow-io-gcs-filesystem==0.24.0\ntermcolor==1.1.0\ntf-estimator-nightly==2.8.0.dev2021122109\nthreadpoolctl==3.1.0\ntifffile==2022.5.4\ntraitlets==5.1.1\ntyping_extensions==4.1.1\nurllib3==1.26.8\nwcwidth==0.2.5\nWerkzeug==2.0.3\nwrapt==1.13.3\nxarray==2022.3.0\nxgboost==1.6.0\nzipp==3.7.0''')\n  \npython = sys.executable\nsubprocess.check_call([python, '-m', 'pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.DEVNULL)\n    #subprocess.check_call(\n        #[python, '-m', 'conda', 'install', '-c','conda-forge','xgboost'],\n      #stdout=subprocess.DEVNULL)\n\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models','cmaq/observation']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "history_output" : "\u001B[33mWARNING: You are using pip version 22.0.3; however, version 22.2.2 is available.\nYou should consider upgrading via the '/home/aalnaim/CMAQAI/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\n",
  "history_begin_time" : 1660007382609,
  "history_end_time" : 1660007420613,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "wi7h3me0tmp",
  "history_input" : "# get hourly CMAQ data into csv for prediction\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nfrom datetime import timedelta\n# home directory\nhome = str(Path.home())\n\nbase = datetime.datetime.today() - timedelta(days=2)\ndate_list = [base + timedelta(days=x) for x in range(2)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\ndays = ['20220803', '20220804']\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor day in days:\n\tprint(\"Getting data for: \"+day)\n    \n\t# read cmaq results\n\tdf_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+day+\"_extracted.nc\")\n    \n\t# read mcip results \n\tdf_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+day+\".nc\")\n        \n\t# read emissions results \n\tdf_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n        \n\tfor k in time_step_in_netcdf_list:\n\t\treal_hour_value = real_hour_list[k]\n            \n\t\tdf_hourly = pd.DataFrame()\n            \n\t\t# CMAQ data\n            \n\t\t# O3 variable\n\t\to3=df_cmaq.variables['O3'][:].values[k,0]\n\t\tcmaq_O3=list(np.ravel(o3).transpose().round())  \n            \n\t\t# NO2\n\t\tno2=df_cmaq.variables['NO2'][:].values[k,0]\n\t\tcmaq_NO2=list(np.ravel(no2).transpose().round())\n      \n\t\t# CO\n\t\tco=df_cmaq.variables['CO'][:].values[k,0]\n\t\tcmaq_CO=list(np.ravel(co).transpose().round())\n\n\t\t# PM25_CO\n\t\tpm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n\t\tcmaq_PM25_CO=list(np.ravel(pm25).transpose().round())\n            \n                              \n                              \n\t\t# EMIS data\n                              \n\t\tco_emis=df_emis.variables['CO'][:].values[k,0]\n\t\tCO_emi=list(np.ravel(co_emis).transpose().round())    \n                \n                              \n                              \n\t\t# MCIP data\n            \n\t\t# CO variable\n\t\tprsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n\t\tPRSFC=list(np.ravel(prsfc).transpose().round())\n            \n\t\t# NO2\n\t\tpbl=df_mcip.variables['PBL'][:].values[k,0]\n\t\tPBL=list(np.ravel(pbl).transpose().round())\n            \n\t\t# TEMP2\n\t\ttemp2=df_mcip.variables['TEMP2'][:].values[k,0]\n\t\tTEMP2=list(np.ravel(temp2).transpose().round())\n            \n\t\t# WSPD10\n\t\twspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n\t\tWSPD10=list(np.ravel(wspd10).transpose().round())\n            \n\t\t# WDIR10\n\t\twdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n\t\tWDIR10=list(np.ravel(wdir10).transpose().round())\n\n\t\t# RGRND\n\t\trgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n\t\tRGRND=list(np.ravel(rgrnd).transpose().round())\n\n\t\t# CFRAC\n\t\tcfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n\t\tCFRAC=list(np.ravel(cfrac).transpose().round())\n\n            \n\t\t## LAT/LON data\n\t\tdf_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n            \n\t\tlat = df_coords.variables['LAT'][:].values[0,0]\n\t\tlat_flt=np.ravel(lat)\n\t\tLAT=np.tile(lat_flt,1)\n\n\t\tlon = df_coords.variables['LON'][:].values[0,0]\n\t\tlon_flt=np.ravel(lon)\n\t\tLON=np.tile(lon_flt,1)\n            \n\t\tdf_hourly['Latitude'] = LAT\n\t\tdf_hourly['Longitude'] = LON\n\t\tdf_hourly['YYYYMMDDHH'] = day+str(real_hour_value)\n\t\tdf_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n\t\tdf_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n\t\tdf_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n\t\tdf_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n\t\tdf_hourly['CO(moles/s)'] = CO_emi\n\t\tdf_hourly['PRSFC(Pa)'] = PRSFC\n\t\tdf_hourly['PBL(m)'] = PBL\n\t\tdf_hourly['TEMP2(K)'] = TEMP2\n\t\tdf_hourly['WSPD10(m/s)'] = WSPD10\n\t\tdf_hourly['WDIR10(degree)'] = WDIR10\n\t\tdf_hourly['RGRND(W/m2)'] = RGRND\n\t\tdf_hourly['CFRAC'] = CFRAC\n\t\tdf_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n\t\tdf_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n\t\tdf_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n            \n            \n\t\tprint('Saving file: test_data_'+day+'_'+str(real_hour_value)+'.csv')\n\t\tdf_hourly.to_csv('/groups/ESS/aalnaim/cmaq/input_hourly/test_data_'+day+'_'+str(real_hour_value)+'.csv',index=False)\n\nprint('Done!')",
  "history_output" : "Getting data for: 20220803\nSaving file: test_data_20220803_12.csv\nSaving file: test_data_20220803_13.csv\nSaving file: test_data_20220803_14.csv\nSaving file: test_data_20220803_15.csv\nSaving file: test_data_20220803_16.csv\nSaving file: test_data_20220803_17.csv\nSaving file: test_data_20220803_18.csv\nSaving file: test_data_20220803_19.csv\nSaving file: test_data_20220803_20.csv\nSaving file: test_data_20220803_21.csv\nSaving file: test_data_20220803_22.csv\nSaving file: test_data_20220803_23.csv\nSaving file: test_data_20220803_0.csv\nSaving file: test_data_20220803_1.csv\nSaving file: test_data_20220803_2.csv\nSaving file: test_data_20220803_3.csv\nSaving file: test_data_20220803_4.csv\nSaving file: test_data_20220803_5.csv\nSaving file: test_data_20220803_6.csv\nSaving file: test_data_20220803_7.csv\nSaving file: test_data_20220803_8.csv\nSaving file: test_data_20220803_9.csv\nSaving file: test_data_20220803_10.csv\nSaving file: test_data_20220803_11.csv\nGetting data for: 20220804\nSaving file: test_data_20220804_12.csv\nSaving file: test_data_20220804_13.csv\nSaving file: test_data_20220804_14.csv\nSaving file: test_data_20220804_15.csv\nSaving file: test_data_20220804_16.csv\nSaving file: test_data_20220804_17.csv\nSaving file: test_data_20220804_18.csv\nSaving file: test_data_20220804_19.csv\nSaving file: test_data_20220804_20.csv\nSaving file: test_data_20220804_21.csv\nSaving file: test_data_20220804_22.csv\nSaving file: test_data_20220804_23.csv\nSaving file: test_data_20220804_0.csv\nSaving file: test_data_20220804_1.csv\nSaving file: test_data_20220804_2.csv\nSaving file: test_data_20220804_3.csv\nSaving file: test_data_20220804_4.csv\nSaving file: test_data_20220804_5.csv\nSaving file: test_data_20220804_6.csv\nSaving file: test_data_20220804_7.csv\nSaving file: test_data_20220804_8.csv\nSaving file: test_data_20220804_9.csv\nSaving file: test_data_20220804_10.csv\nSaving file: test_data_20220804_11.csv\nDone!\n",
  "history_begin_time" : 1660008090778,
  "history_end_time" : 1660008287870,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "utlvlq24cny",
  "history_input" : "# merge all hourly testing data into daily files\n\nimport pandas as pd\nimport glob\nimport os\nfrom pathlib import Path\n\n# home directory\n# home = str(Path.home())\n\npath = '/groups/ESS/aalnaim/cmaq/input_hourly'\nall_hourly_files = glob.glob(os.path.join(path, \"test_data_*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n# dropping unnecessary variables\ncmaq['YYYYMMDDHH'] = cmaq['YYYYMMDDHH'].map(str)\ncmaq['month'] = cmaq['YYYYMMDDHH'].str[4:6]\ncmaq['day'] = cmaq['YYYYMMDDHH'].str[6:8]\ncmaq['hours'] = cmaq['YYYYMMDDHH'].str[8:10]\n\n#new_df=cmaq.drop(['YYYYMMDDHH'],axis=1)\ncmaq.to_csv(\"/groups/ESS/aalnaim/cmaq/input_hourly/testing.csv\",index=False)\n\nprint('Done!')",
  "history_output" : "Done!\n",
  "history_begin_time" : 1660008288283,
  "history_end_time" : 1660008356626,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "euwvb175p33",
  "history_input" : "#!/bin/bash\n# generate images and gif from the NetCDF files\n\n\n# Setting env variables\nexport YYYYMMDD_POST=$(date -d '5 day ago' '+%Y%m%d') #This needs to be auto date `date -d \"-2 day ${1}\" +%Y%m%d`\nexport stdate_post=$(date -d '5 day ago' '+%Y-%m-%d') #This needs to be auto date\nexport eddate_post=$(date -d '4 day ago' '+%Y-%m-%d') #This needs to be auto date\n\nexport stdate_file=$(date -d '5 day ago' '+%Y%m%d') #This needs to be auto date\nexport eddate_file=$(date -d '4 day ago' '+%Y%m%d') #This needs to be auto date\n\n\nexport postdata_dir=\"/groups/ESS/aalnaim/cmaq/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport dir_graph=\"/groups/ESS/aalnaim/cmaq/plots\"\n\nmodule load ncl\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\")\nd1 = getenv(\"stdate_post\")\nd2 = getenv(\"eddate_post\")\n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\n;print(\"Passed Date: \"+date)\n\n;aconc_dir = getenv(\"postdata_dir\")\ngrid_dir = getenv(\"mcip_dir\")\nplot_dir = getenv(\"dir_graph\")\n\ncdf_file1 = addfile(\"/groups/ESS/aalnaim/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_\"+dFile2+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\n\nptime = (/\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"/)\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,:,:) ;ppb\n;pm25 = cdf_file1->PM25_TOT(:,0,:,:)\n\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\n;print(max(pm25))\n;print(min(pm25))\n;print(avg(pm25))\n\n;print(time)\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; don't advance frame\nres@gsnDraw = False\n;res@gsnSpreadColors = True\nres@lbLabelAutoStride = True\n;res@lbBoxLinesOn = False\nres@pmLabelBarHeightF = 0.1\nres@pmLabelBarWidthF = 0.5\nres@cnFillOn=True\n;res@cnMonoFillPattern=True\n;res@cnMonoLineColor=True\nres@cnLinesOn=False\n;res@pmLabelBarDisplayMode=\"never\"\nres@gsnLeftString  = \"\";\nres@gsnRightString = \"\"\n\nres@mpLimitMode = \"LatLon\"\nres@mpMinLonF = -120 ;min(lon)+0.2\nres@mpMaxLonF = -70 ;max(lon)-0.2\nres@mpMinLatF = 25 ;min(lat)+0.05\nres@mpMaxLatF = 50 ;max(lat)-0.05\nres@mpDataBaseVersion = \"MediumRes\"\n;res@tiMainString = times(it)\nres@mpDataBaseVersion       = \"MediumRes\"\nres@mpDataSetName           = \"Earth..4\"\nres@mpAreaMaskingOn         = True\nres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nres@mpOutlineSpecifiers=\"United States : States\"\nres@mpLandFillColor         = \"white\"\nres@mpInlandWaterFillColor  = \"white\"\nres@mpOceanFillColor        = \"white\"\nres@mpGeophysicalLineColor    = \"Black\"\nres@mpGeophysicalLineThicknessF = 1.5\n\n;res@gsnSpreadColors         = True\nres@lbLabelAutoStride       = True\nres@lbLabelFont             = 25\nres@tiXAxisFont             = 25\nres@pmTickMarkDisplayMode   = \"Always\"\nres@tmXBLabelFont           = 25\nres@tmXBLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmYLLabelFont           = 25\nres@tmYLLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmXTLabelsOn            = False\nres@tmXTLabelFont           = 25\nres@tmXTLabelFontHeightF    = 0.013\nres@tmYRLabelsOn            = False\nres@tmYRLabelFont           = 25\nres@tmYRLabelFontHeightF    = 0.013\n\n\nres@mpProjection           = \"LambertConformal\" ;\"CylindricalEquidistant\"\nres@mpLambertParallel1F    = 33.\nres@mpLambertParallel2F    = 45.\nres@mpLambertMeridianF     = -98.\n\nres@cnLevelSelectionMode = \"ManualLevels\"\nres@cnMinLevelValF          = 0.\nres@cnMaxLevelValF          = 80\nres@cnLevelSpacingF         = 4\n\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  pname=plot_dir+\"/testPlot_\"+pdate+\"_\"+ptime(it)\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  res@tiMainString = pdate+\" \"+ptime(it)+\" UTC O~B~3~N~ Forecast (ppbV)\"\n  plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  draw(plot)\n  frame(wks)\n  delete(wks)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/aalnaim/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\nend do\ndelete(res)\n\nend\nEOF\n\n\nncl /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3.ncl\n\n# convert -delay 100 *.png 20220613_20220614.gif\nconvert -delay 100 /groups/ESS/aalnaim/cmaq/plots/testPlot*.png /groups/ESS/aalnaim/cmaq/plots/\"Map_\"$YYYYMMDD_POST.gif\n\nif [ $? -eq 0 ]; then\n    echo \"Generating images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3.ncl...\"\n\trm /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3.ncl\nelse\n    echo \"Generating images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3.ncl...\"\n\trm /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3.ncl\nfi\n",
  "history_output" : " Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t24 265 442\n(0)\t107.12\n(0)\t0.6899999999999999\n(0)\t26.75160186452871\nGenerating images/gif Completed Successfully\nRemoving ncl file: geoweaver_plot_daily_O3.ncl...\n",
  "history_begin_time" : 1660008622136,
  "history_end_time" : 1660008882584,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "bo7gm60oq57",
  "history_input" : "#!/bin/bash\n# evaluate the prediction accuracy\n\n# Setting env variables\nexport YYYYMMDD_POST=$(date -d '5 day ago' '+%Y%m%d')\nexport stdate_file=$(date -d '5 day ago' '+%Y%m%d') #This needs to be auto date\nexport eddate_file=$(date -d '4 day ago' '+%Y%m%d') #This needs to be auto date\n\n\nexport wfname=\"/groups/ESS/aalnaim/cmaq/results/geoweaver_evalution_\"$YYYYMMDD_POST\"_results.txt\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\nexport ofname=\"/AQF5X_Hourly_\"\n\nexport postdata_dir=\"/groups/ESS/aalnaim/cmaq/prediction_nc_files/\"\n\nexport mfname=\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_\"$eddate_file\"_ML_extracted.nc\"\n\nexport grid_fname=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/GRIDCRO2D_\"$YYYYMMDD_POST\".nc\" #This needs to be auto date\n\nexport dx=12000\n\nmodule load ncl\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/geoweaver_eva_daily_O3.ncl\n\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\nsdate=getenv(\"YYYYMMDD_POST\")\nwfname=getenv(\"wfname\")\nobs_dir=getenv(\"obs_dir_NCL\")\nofname=getenv(\"ofname\")\nmod_dir=getenv(\"postdata_dir\")\nmfname=getenv(\"mfname\")\ndkm=tofloat(getenv(\"dx\"))\ngrid_fname=(getenv(\"grid_fname\"))\n\nmaxdist=dkm/90000.0*1.414\nmaxarea=0.25\nthd=70\n\n;-----read model lat lon------\n;read lat lon\nf1 = addfile(grid_fname,\"r\")\nmlat = f1->LAT(0,0,:,:)\nmlon = f1->LON(0,0,:,:)\ndelete(f1)\nmlat1d = ndtooned(mlat)\nmlon1d = ndtooned(mlon)\ndelete([/mlat,mlon/])\n\n;-----read cmaq results-----\nf2 = addfile(mod_dir+mfname,\"r\")\nmO3 = f2->O3(:,:,:) ;ppb\n\n\nnt = dimsizes(mO3(:,0,0))\nny = dimsizes(mO3(0,:,0))\nnx = dimsizes(mO3(0,0,:))\n\nm8O3 = new((/17,ny,nx/),\"double\")\nm8maxO3 = new((/ny,nx/),\"double\")\n\ndo ih=0,16\n  m8O3(ih,:,:)=dim_avg_n(mO3(ih:ih+7,:,:),0)\nend do\nm8maxO3 = dim_max_n(m8O3,0) ;type double\nmO31d_d=ndtooned(m8maxO3) ; type double\nmO31d=tofloat(mO31d_d)\n\ndelete([/f2,mO3,m8O3,m8maxO3/])\n\n;-----read obs-----\nsyyyy1=str_get_cols(sdate,0,3)\nsmm1=str_get_cols(sdate,4,5)\nsdd1=str_get_cols(sdate,6,7)\n\nymd=jul2greg(greg2jul(tointeger(syyyy1),tointeger(smm1),tointeger(sdd1),-1)+1)\nsyyyy2=tostring_with_format(ymd(0),\"%0.4i\")\nsmm2=tostring_with_format(ymd(1),\"%0.2i\")\nsdd2=tostring_with_format(ymd(2),\"%0.2i\")\n\ntolat=(/-999.0/) ;set the first data to 0\ntolon=tolat\ntoO3=tolat\n\ndo ih=12,35\n  if (ih.lt.24) then\n    shh=tostring_with_format(ih,\"%0.2i\")\n    syyyy=syyyy1\n    smm=smm1\n    sdd=sdd1\n  else\n    shh=tostring_with_format(ih-24,\"%0.2i\")\n    syyyy=syyyy2\n    smm=smm2\n    sdd=sdd2\n  end if\n  data=asciiread(obs_dir+ofname+syyyy+smm+sdd+shh+\".dat\",-1,\"string\")\n  xx=array_append_record(tolat,stringtofloat(str_get_field(data(1::), 2,\",\")),0)\n  yy=array_append_record(tolon,stringtofloat(str_get_field(data(1::), 3,\",\")),0)\n  zz=array_append_record(toO3,stringtofloat(str_get_field(data(1::), 4,\",\")),0)\n  delete([/tolat,tolon,toO3/])\n  tolat=xx\n  tolon=yy\n  toO3=zz\n  delete([/xx,yy,zz/])\n  delete(data)\nend do\n\ntoO3@_FillValue = -999.0\n\n;-----calculate max ave 8 hour o3-----\noflag=tolat*0+1\naa=ind((oflag.gt.0).and.(toO3.ge.0))\nii=0\nprint(\"8h start\")\nif (any(ismissing(aa))) then\n  iflag=0\nelse\n  iflag=1\n  olat=(/tolat(aa(0))/)\n  olon=(/tolon(aa(0))/)\n  oO3=(/-999.0/)\n  o8O3 = new(17,\"float\")\n  o8O3 = -999.0\nend if\ndelete(aa)\ndo while (iflag.gt.0)\n  aa=ind((tolat.eq.olat(ii)).and.(tolon.eq.olon(ii)).and.(toO3.ge.0))\n  oflag(aa)=0\n  if (dimsizes(aa).eq.24) then  ; calculate 24 h, so calculate 8hr ozone here\n    do ih = 0, 16\n      o8O3(ih) = avg(toO3(aa(ih:ih+7)))\n    end do\n    oO3(ii)=max(o8O3)\n  end if\n  o8O3 = -999.0\n  delete(aa)\n  aa=ind((oflag.gt.0).and.(toO3.ge.0))\n  if (any(ismissing(aa))) then\n    iflag=0\n  else\n    xx=array_append_record(olat,(/tolat(aa(0))/),0)\n    yy=array_append_record(olon,(/tolon(aa(0))/),0)\n    zz=array_append_record(oO3,(/-999.0/),0)\n    delete([/olat,olon,oO3/])\n    olat=xx\n    olon=yy\n    oO3=zz\n    delete([/xx,yy,zz/])\n    ii=ii+1\n  end if\n  delete(aa)\nend do\nprint(\"obs 8hour max end\")\naa=ind(oO3.ge.0)\nnobs=dimsizes(aa)\nolat24=olat(aa)\nolon24=olon(aa)\noO324=oO3(aa)\nprint(\"TYPE of oO324: \"+typeof(oO324))\ndelete([/aa,olat,olon,oO3/])\nmO324=oO324*0-999.0\nprint(\"TYPE of mO324: \"+typeof(mO324))\nprint(\"TYPE of mO31d: \"+typeof(mO31d))\nareaa=oO324*0-999.0\nareab=areaa\naread=areaa\n\n;-----find model point-----\ndo in=0,nobs-1\n  dis=sqrt((mlat1d-olat24(in))^2+(mlon1d-olon24(in))^2)\n  aa=minind(dis)\n ;print(in+\" \"+aa)\n  if (dis(aa).lt.maxdist) then\n    mO324(in)=mO31d(aa)\n    cc=ind((mlat1d.ge.(olat24(in)-maxarea)).and.(mlat1d.le.(olat24(in)+maxarea)).and.\\\n           (mlon1d.ge.(olon24(in)-maxarea)).and.(mlon1d.le.(olon24(in)+maxarea)))\n    areaa(in)=0\n    areab(in)=0\n    if (oO324(in).ge.thd) then\n      aread(in)=0\n      if (max(mO31d(cc)).ge.thd) then\n        areab(in)=1\n      else\n        aread(in)=1\n      end if\n    else\n      bb=ind((olat24.ge.(olat24(in)-maxarea)).and.(olat24.le.(olat24(in)+maxarea)).and.\\\n             (olon24.ge.(olon24(in)-maxarea)).and.(olon24.le.(olon24(in)+maxarea)))\n      if (max(mO31d(aa)).ge.thd) then\n        if (max(oO324(bb)).ge.thd) then\n          areaa(in)=0\n        else\n          areaa(in)=1\n        end if\n      else\n        areaa(in)=0\n      end if\n      delete(bb)\n    end if\n    delete(cc)\n  end if\n  delete(aa)\nend do\n\n;-----cal rmse corr nme nmb me mb-----\ntt=ind((mO324.ge.0).and.(oO324.ge.0))\n\nif (any(ismissing(tt))) then\n  rmse=-999.0\n  corr=-999.0\n  nmb=-999.0\n  nme=-999.0\n  me=-999.0\n  mb=-999.0\nelse\n  rmse=dim_rmsd_n(oO324(tt),mO324(tt),0)\n  corr=esccr(oO324(tt),mO324(tt),0)\n  nmb=sum((mO324(tt)-oO324(tt)))/sum(oO324(tt))\n  nme=sum(abs(oO324(tt)-mO324(tt)))/sum(oO324(tt))\n  me=avg(abs(oO324(tt)-mO324(tt)))\n  mb=avg((mO324(tt)-oO324(tt)))\nend if\n;-----cal ah afar-----\naa=ind((areaa+areab).gt.0)\nbb=ind((aread+areab).gt.0)\nif (any(ismissing(aa))) then\n  afar=0.\nelse\n  afar=tofloat(sum(areaa(aa)))/tofloat(sum(areab(aa))+sum(areaa(aa)))*100\nend if\ndelete(aa)\nif (any(ismissing(bb))) then\n  ah=-999.0\nelse\n  ah=tofloat(sum(areab(bb)))/tofloat(sum(areab(bb))+sum(aread(bb)))*100\nend if\ndelete(bb)\nwrite_table(wfname,\"a\",[/sdate,dimsizes(tt),avg(oO324(tt)),avg(mO324(tt)),rmse,corr,nmb,nme,mb,me,ah,afar/],\\\n            \"%s,%i,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f\")\ndelete(tt)\nend\n\nEOF\n\n\nncl /groups/ESS/aalnaim/cmaq/geoweaver_eva_daily_O3.ncl\n\nif [ $? -eq 0 ]; then\n    echo \"Evaluation Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_eva_daily_O3.ncl...\"\n\trm /groups/ESS/aalnaim/cmaq/geoweaver_eva_daily_O3.ncl\nelse\n    echo \"Evaluation Failed!\"\nfi\n",
  "history_output" : " Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t8h start\n(0)\tobs 8hour max end\n(0)\tTYPE of oO324: float\n(0)\tTYPE of mO324: float\n(0)\tTYPE of mO31d: float\nEvaluation Completed Successfully\nRemoving ncl file: geoweaver_eva_daily_O3.ncl...\n",
  "history_begin_time" : 1660008886604,
  "history_end_time" : 1660008898907,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "i0pxdm7c1n2",
  "history_input" : "#!/bin/bash\n# generate images and gif from the prediction NetCDF files and overlay the AirNow station observation on the top\n\n# Setting env variables\nexport YYYYMMDD_POST=$(date -d '5 day ago' '+%Y%m%d')\nexport stdate_post=$(date -d '5 day ago' '+%Y-%m-%d') \nexport eddate_post=$(date -d '4 day ago' '+%Y-%m-%d')\n\nexport stdate_file=$(date -d '5 day ago' '+%Y%m%d') \nexport eddate_file=$(date -d '4 day ago' '+%Y%m%d') \n\n\nexport postdata_dir=\"/groups/ESS/aalnaim/cmaq/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport graph_dir=\"/groups/ESS/aalnaim/cmaq/plots\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n\nmodule load ncl\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3_Airnow.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\") \nd1 = getenv(\"stdate_post\") \nd2 = getenv(\"eddate_post\") \n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\nobs_dir = getenv(\"obs_dir_NCL\")\nplot_dir = getenv(\"graph_dir\") \n\nhr=new(24,\"string\")\nhr=(/\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"/)\n\nprint(plot_dir)\naconc_dir = getenv(\"postdata_dir\") \ngrid_dir = getenv(\"mcip_dir\") \n\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_\"+dFile2+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\ncdf_file2= addfile(grid_dir+\"/METCRO2D_\"+date+\".nc\",\"r\")\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,:,:) ;ppb\nwspd10=cdf_file2->WSPD10(:,0,:,:)\nwdir10=cdf_file2->WDIR10(:,0,:,:)\n\ntemp = cdf_file2->TEMP2\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(max(temp))\nprint(min(temp))\nprint(avg(temp))\n\n\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\no3@unit = \"ppbv\"\n\nUV10=wind_component(wspd10,wdir10,0)\nUV10@lat2d = lat\nUV10@lon2d = lon\n\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; dont advance frame\nres@gsnDraw = False\nres@gsnLeftString  = \"\"\nres@gsnRightString = \"\"\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n;res@tiMainFontHeightF = 0.02\n;res@vpWidthF        = 0.7\n;res@vpHeightF       = 0.7\n\n;;set map;;\nmpres                             = res\nmpres@mpLimitMode = \"LatLon\"\nmpres@mpDataSetName               = \"Earth..4\"\nmpres@mpDataBaseVersion           = \"MediumRes\"\nmpres@mpOutlineOn                 = True\nmpres@mpGeophysicalLineThicknessF = 1.5\nmpres@mpFillDrawOrder             = \"PostDraw\"\nmpres@mpFillOn                    = False\nmpres@mpAreaMaskingOn         = True\nmpres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nmpres@mpOutlineSpecifiers         = \"United States:States\"\nmpres@mpProjection           = \"LambertConformal\"\nmpres@mpLambertParallel1F    = 33.\nmpres@mpLambertParallel2F    = 45.\nmpres@mpLambertMeridianF     = -98.\nmpres@mpMinLonF = -120 ;min(lon)+0.2\nmpres@mpMaxLonF = -70 ;max(lon)-0.2\nmpres@mpMinLatF = 25 ;min(lat)+0.05\nmpres@mpMaxLatF = 50 ;max(lat)-0.05\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@mpLandFillColor         = \"white\"\nmpres@mpInlandWaterFillColor  = \"white\"\nmpres@mpOceanFillColor        = \"white\"\nmpres@mpGeophysicalLineColor    = \"Black\"\n\n;mpres@lbLabelAutoStride       = True\nmpres@tiXAxisFont             = 25\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@tmXBLabelFont           = 25\nmpres@tmXBLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmYLLabelFont           = 25\nmpres@tmYLLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmXTLabelsOn            = False\nmpres@tmXTLabelFont           = 25\nmpres@tmXTLabelFontHeightF    = 0.013\nmpres@tmYRLabelsOn            = False\nmpres@tmYRLabelFont           = 25\nmpres@tmYRLabelFontHeightF    = 0.013\n\n;;set contour;;\ncnres                         = res\ncnres@cnFillDrawOrder         = \"PreDraw\"\ncnres@cnFillOn                = True\ncnres@cnLinesOn               = False\ncnres@cnLineLabelsOn          = False\ncnres@lbLabelFont             = 25\ncnres@lbLabelFontHeightF      = 0.013\ncnres@tiXAxisFont             = 25\ncnres@pmLabelBarWidthF        = 0.5\ncnres@pmLabelBarHeightF       = 0.1\n;cnres@pmLabelBarOrthogonalPosF = -0.02\ncnres@lbLabelAutoStride       = True\n\n;set vector;;\nres_vc                        = res\nres_vc@vcGlyphStyle           = \"LineArrow\"\nres_vc@vcLineArrowThicknessF  = 3\nres_vc@vcMinDistanceF         = 0.03\nres_vc@vcRefLengthF           = 0.03\nres_vc@vcRefAnnoOn            = True\nres_vc@vcRefMagnitudeF           = 16\nres_vc@vcRefAnnoString1          = \"16m/s\"\nres_vc@vcRefAnnoSide             = \"Top\"\nres_vc@vcRefAnnoString2On        = False\nres_vc@vcRefAnnoPerimOn          = False\nres_vc@vcRefAnnoOrthogonalPosF   = -0.02\nres_vc@vcRefAnnoParallelPosF     = 0.999\n;res_vc@vcRefAnnoBackgroundColor = \"White\"\nres_vc@vcVectorDrawOrder         = \"PostDraw\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  ;print(time(it,0)+\" \"+time(it,1))\n  rundate = yyyyddd_to_yyyymmdd( time(it,0) )\n  runtime = hr( tointeger(time(it,1)/10000) )\n\n  site = readAsciiTable(obs_dir+\"/AQF5X_Hourly_\"+rundate+runtime+\".dat\",1,\"string\",1)\n  nrows = dimsizes(site)\n  sitename = str_get_field(site,1,\",\")\n  sitelat = stringtofloat(str_get_field(site,2,\",\"))\n  sitelon = stringtofloat(str_get_field(site,3,\",\"))\n  O3_obs = stringtofloat(str_get_field(site,4,\",\"))\n\n  obslon = sitelon(:,0)\n  obslat = sitelat(:,0)\n  obsO3 = O3_obs(:,0)\n\n  npts = nrows(0)\n\n  obsO3@_FillValue = -999.\n\n;--- levels for dividing\n  levels_O3  = ispan(0,80,4)\n\n  nlevels = dimsizes(levels_O3)\n\n  colors  = span_color_rgba(\"WhiteBlueGreenYellowRed\",nlevels+1)\n\n  num_distinct_markers = nlevels+1        ; number of distinct markers\n  lat_O3 = new((/num_distinct_markers,npts/),float)\n  lon_O3 = new((/num_distinct_markers,npts/),float)\n  lat_O3 = -999\n  lon_O3 = -999\n\n\n;\n; Group the points according to which range they fall in. At the\n; same time, create the label that we will use later in the labelbar\n;\n  do i = 0, num_distinct_markers-1\n    if (i.eq.0) then\n      indexes_O3 = ind(obsO3(:).lt.levels_O3(0))\n    end if\n    if (i.eq.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.max(levels_O3))\n    end if\n    if (i.gt.0.and.i.lt.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.levels_O3(i-1).and.obsO3(:).lt.levels_O3(i))\n    end if\n\n;\n; Now that we have the set of indexes whose values fall within\n; the given range, take the corresponding lat/lon values and store\n; them, so later we can color this set of markers with the appropriate\n; color.\n;\n    if (.not.any(ismissing(indexes_O3))) then\n      npts_range_O3 = dimsizes(indexes_O3)   ; # of points in this range.\n\n      lat_O3(i,0:npts_range_O3-1) = obslat(indexes_O3)\n      lon_O3(i,0:npts_range_O3-1) = obslon(indexes_O3)\n  ;print(\"O3: \"+npts_range_O3)\n    end if\n\n\n    delete(indexes_O3)            ; Necessary b/c \"indexes\" may be a different\n  end do\n\n  lat_O3@_FillValue = -999\n  lon_O3@_FillValue = -999\n\n  gsres               = True\n  gsres@gsMarkerIndex = 16          ; Use filled dots for markers.\n\n  hollowres           = True\n  hollowres@gsMarkerIndex    = 4\n  hollowres@gsMarkerColor    = \"black\"\n  hollowres@gsMarkerSizeF    = 0.008\n\n;;;;;;;;;   Plot Ozone\n  pname=plot_dir+\"/OBS-FORECAST_O3_\"+rundate+runtime\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  pmid_O3 = new(num_distinct_markers,graphic)\n  hollow_O3 = new(num_distinct_markers,graphic)\n\n  cnres@tiMainString =  pdate+\" \"+runtime+\" UTC O~B~3~N~ (ppbV)\"\n  cnres@cnLevelSelectionMode = \"ManualLevels\"\n  cnres@cnMinLevelValF          = 0.\n  cnres@cnMaxLevelValF          = 80\n  cnres@cnLevelSpacingF         = 4\n\n  ;plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  map = gsn_csm_map(wks,mpres)\n  contour = gsn_csm_contour(wks,o3(it,:,:),cnres)\n  vector  = gsn_csm_vector(wks,UV10(0,it,:,:),UV10(1,it,:,:),res_vc)\n  overlay(map,contour)\n  overlay(map,vector)\n\n  pmid = new(num_distinct_markers,graphic)\n  hollow = new(num_distinct_markers,graphic)\n  do i = 0, num_distinct_markers-1\n    if (.not.ismissing(lat_O3(i,0)))\n      gsres@gsMarkerColor      = colors(i,:)\n      gsres@gsMarkerSizeF      = 0.008\n      gsres@gsMarkerThicknessF = 1\n       pmid(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),gsres)\n       hollow(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),hollowres)\n    end if\n  end do\n\n  draw(map)\n  frame(wks)\n  delete(wks)\n  delete(pmid_O3)\n  delete(hollow_O3)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/aalnaim/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\n\n\n  delete(pmid)\n  delete(hollow)\n  delete(site)\n  delete(sitename)\n  delete(sitelat)\n  delete(sitelon)\n  delete(O3_obs)\n  delete(obslon)\n  delete(obslat)\n  delete(obsO3)\n  delete([/lon_O3,lat_O3/])\n\nend do\ndelete(res)\n\n;/\n\nend\nEOF\n\n\nncl /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3_Airnow.ncl\n\nconvert -delay 100 /groups/ESS/aalnaim/cmaq/plots/OBS*.png /groups/ESS/aalnaim/cmaq/plots/\"Airnow_\"$YYYYMMDD_POST.gif\n\nif [ $? -eq 0 ]; then\n    echo \"Generating AirNow images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\trm /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3_Airnow.ncl\nelse\n    echo \"Generating AirNow images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\trm /groups/ESS/aalnaim/cmaq/geoweaver_plot_daily_O3_Airnow.ncl\nfi\n",
  "history_output" : " Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n\n\nVariable: plot_dir\nType: string\nTotal Size: 8 bytes\n            1 values\nNumber of Dimensions: 1\nDimensions and sizes:\t[1]\nCoordinates: \n(0)\t/groups/ESS/aalnaim/cmaq/plots\n(0)\t317.1743\n(0)\t274.8513\n(0)\t295.8538\n(0)\t24 265 442\n(0)\t107.12\n(0)\t0.6899999999999999\n(0)\t26.75160186452871\nGenerating AirNow images/gif Completed Successfully\nRemoving ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\n",
  "history_begin_time" : 1660008622806,
  "history_end_time" : 1660008886219,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
}]
