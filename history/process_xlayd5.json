[{
  "history_id" : "Zpu58gjxTpmO",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import datetime, date, timedelta\n# home directory\nhome = str(Path.home())\n\ndays=[]\n\nbase = datetime.today() - timedelta(days=2)\ndate_list = [base - timedelta(days=x) for x in range(3)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\n\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(\"/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(\"/groups/ESS/aalnaim/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061312.txt\n2022061312\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061313.txt\n2022061313\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061314.txt\n2022061314\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061315.txt\n2022061315\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061316.txt\n2022061316\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061317.txt\n2022061317\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061318.txt\n2022061318\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061319.txt\n2022061319\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061320.txt\n2022061320\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061321.txt\n2022061321\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061322.txt\n2022061322\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061323.txt\n2022061323\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061300.txt\n2022061300\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061301.txt\n2022061301\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061302.txt\n2022061302\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061303.txt\n2022061303\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061304.txt\n2022061304\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061305.txt\n2022061305\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061306.txt\n2022061306\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061307.txt\n2022061307\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061308.txt\n2022061308\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061309.txt\n2022061309\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061310.txt\n2022061310\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061311.txt\n2022061311\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061212.txt\n2022061212\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061213.txt\n2022061213\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061214.txt\n2022061214\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061215.txt\n2022061215\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061216.txt\n2022061216\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061217.txt\n2022061217\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061218.txt\n2022061218\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061219.txt\n2022061219\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061220.txt\n2022061220\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061221.txt\n2022061221\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061222.txt\n2022061222\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061223.txt\n2022061223\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061200.txt\n2022061200\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061201.txt\n2022061201\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061202.txt\n2022061202\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061203.txt\n2022061203\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061204.txt\n2022061204\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061205.txt\n2022061205\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061206.txt\n2022061206\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061207.txt\n2022061207\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061208.txt\n2022061208\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061209.txt\n2022061209\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061210.txt\n2022061210\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061211.txt\n2022061211\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061112.txt\n2022061112\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061113.txt\n2022061113\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061114.txt\n2022061114\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061115.txt\n2022061115\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061116.txt\n2022061116\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061117.txt\n2022061117\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061118.txt\n2022061118\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061119.txt\n2022061119\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061120.txt\n2022061120\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061121.txt\n2022061121\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061122.txt\n2022061122\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061123.txt\n2022061123\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061100.txt\n2022061100\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061101.txt\n2022061101\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061102.txt\n2022061102\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061103.txt\n2022061103\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061104.txt\n2022061104\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061105.txt\n2022061105\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061106.txt\n2022061106\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061107.txt\n2022061107\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061108.txt\n2022061108\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061109.txt\n2022061109\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061110.txt\n2022061110\n/groups/ESS/aalnaim/cmaq/observation/AQF5X_Hourly_2022061111.txt\n2022061111\n3\n(209376, 4)\n(209376,)\n",
  "history_begin_time" : 1655276461451,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "6RsY73mc0Mkz",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import datetime, date, timedelta\n# home directory\nhome = str(Path.home())\n\ndays=[]\n\nbase = datetime.today() - timedelta(days=2)\ndate_list = [base - timedelta(days=x) for x in range(7)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\n\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061012.txt\n2022061012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061013.txt\n2022061013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061014.txt\n2022061014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061015.txt\n2022061015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061016.txt\n2022061016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061017.txt\n2022061017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061018.txt\n2022061018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061019.txt\n2022061019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061020.txt\n2022061020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061021.txt\n2022061021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061022.txt\n2022061022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061023.txt\n2022061023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061000.txt\n2022061000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061001.txt\n2022061001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061002.txt\n2022061002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061003.txt\n2022061003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061004.txt\n2022061004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061005.txt\n2022061005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061006.txt\n2022061006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061007.txt\n2022061007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061008.txt\n2022061008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061009.txt\n2022061009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061010.txt\n2022061010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022061011.txt\n2022061011\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060912.txt\n2022060912\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060913.txt\n2022060913\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060914.txt\n2022060914\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060915.txt\n2022060915\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060916.txt\n2022060916\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060917.txt\n2022060917\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060918.txt\n2022060918\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060919.txt\n2022060919\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060920.txt\n2022060920\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060921.txt\n2022060921\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060922.txt\n2022060922\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060923.txt\n2022060923\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060900.txt\n2022060900\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060901.txt\n2022060901\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060902.txt\n2022060902\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060903.txt\n2022060903\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060904.txt\n2022060904\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060905.txt\n2022060905\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060906.txt\n2022060906\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060907.txt\n2022060907\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060908.txt\n2022060908\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060909.txt\n2022060909\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060910.txt\n2022060910\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060911.txt\n2022060911\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060812.txt\n2022060812\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060813.txt\n2022060813\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060814.txt\n2022060814\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060815.txt\n2022060815\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060816.txt\n2022060816\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060817.txt\n2022060817\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060818.txt\n2022060818\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060819.txt\n2022060819\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060820.txt\n2022060820\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060821.txt\n2022060821\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060822.txt\n2022060822\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060823.txt\n2022060823\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060800.txt\n2022060800\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060801.txt\n2022060801\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060802.txt\n2022060802\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060803.txt\n2022060803\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060804.txt\n2022060804\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060805.txt\n2022060805\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060806.txt\n2022060806\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060807.txt\n2022060807\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060808.txt\n2022060808\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060809.txt\n2022060809\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060810.txt\n2022060810\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060811.txt\n2022060811\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060712.txt\n2022060712\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060713.txt\n2022060713\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060714.txt\n2022060714\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060715.txt\n2022060715\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060716.txt\n2022060716\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060717.txt\n2022060717\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060718.txt\n2022060718\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060719.txt\n2022060719\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060720.txt\n2022060720\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060721.txt\n2022060721\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060722.txt\n2022060722\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060723.txt\n2022060723\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060700.txt\n2022060700\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060701.txt\n2022060701\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060702.txt\n2022060702\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060703.txt\n2022060703\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060704.txt\n2022060704\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060705.txt\n2022060705\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060706.txt\n2022060706\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060707.txt\n2022060707\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060708.txt\n2022060708\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060709.txt\n2022060709\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060710.txt\n2022060710\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060711.txt\n2022060711\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060612.txt\n2022060612\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060613.txt\n2022060613\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060614.txt\n2022060614\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060615.txt\n2022060615\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060616.txt\n2022060616\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060617.txt\n2022060617\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060618.txt\n2022060618\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060619.txt\n2022060619\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060620.txt\n2022060620\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060621.txt\n2022060621\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060622.txt\n2022060622\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060623.txt\n2022060623\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060600.txt\n2022060600\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060601.txt\n2022060601\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060602.txt\n2022060602\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060603.txt\n2022060603\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060604.txt\n2022060604\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060605.txt\n2022060605\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060606.txt\n2022060606\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060607.txt\n2022060607\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060608.txt\n2022060608\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060609.txt\n2022060609\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060610.txt\n2022060610\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060611.txt\n2022060611\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060512.txt\n2022060512\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060513.txt\n2022060513\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060514.txt\n2022060514\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060515.txt\n2022060515\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060516.txt\n2022060516\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060517.txt\n2022060517\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060518.txt\n2022060518\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060519.txt\n2022060519\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060520.txt\n2022060520\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060521.txt\n2022060521\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060522.txt\n2022060522\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060523.txt\n2022060523\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060500.txt\n2022060500\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060501.txt\n2022060501\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060502.txt\n2022060502\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060503.txt\n2022060503\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060504.txt\n2022060504\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060505.txt\n2022060505\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060506.txt\n2022060506\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060507.txt\n2022060507\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060508.txt\n2022060508\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060509.txt\n2022060509\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060510.txt\n2022060510\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060511.txt\n2022060511\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060412.txt\n2022060412\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060413.txt\n2022060413\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060414.txt\n2022060414\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060415.txt\n2022060415\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060416.txt\n2022060416\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060417.txt\n2022060417\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060418.txt\n2022060418\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060419.txt\n2022060419\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060420.txt\n2022060420\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060421.txt\n2022060421\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060422.txt\n2022060422\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060423.txt\n2022060423\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060400.txt\n2022060400\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060401.txt\n2022060401\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060402.txt\n2022060402\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060403.txt\n2022060403\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060404.txt\n2022060404\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060405.txt\n2022060405\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060406.txt\n2022060406\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060407.txt\n2022060407\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060408.txt\n2022060408\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060409.txt\n2022060409\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060410.txt\n2022060410\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2022060411.txt\n2022060411\n7\n(488466, 4)\n(488466,)\n",
  "history_begin_time" : 1655074717499,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "turTKdBIzbt4",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom datetime import date, timedelta\n# home directory\nhome = str(Path.home())\n\ndays=[]\n\nbase = datetime.today() - timedelta(days=2)\ndate_list = [base - timedelta(days=x) for x in range(7)]\ndays = [date.strftime('%Y%m%d') for date in date_list]\n\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 12, in <module>\n    base = datetime.today() - timedelta(days=2)\nNameError: name 'datetime' is not defined\n",
  "history_begin_time" : 1655074678344,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "yxzoaey43em",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 40, in <module>\n    data_frame = np.concatenate(merged)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nValueError: need at least one array to concatenate\n",
  "history_begin_time" : 1655072830354,
  "history_end_time" : 1655072841824,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Failed"
},{
  "history_id" : "k7kd97407lv",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/zsun/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1654477343854,
  "history_end_time" : 1654477372083,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "4izemcvvw2v",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 40, in <module>\n    data_frame = np.concatenate(merged)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: need at least one array to concatenate\n",
  "history_begin_time" : 1654465996825,
  "history_end_time" : 1654465997528,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "kwyv9tikhar",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/zsun/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1654455970672,
  "history_end_time" : 1654455997579,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "roeaa3",
  "indicator" : "Failed"
},{
  "history_id" : "6b6at4txbrd",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1654319660322,
  "history_end_time" : 1654319679768,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "03kb607gma6",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/zsun/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1654317797258,
  "history_end_time" : 1654317824376,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "w049npmn63k",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/zsun/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/zsun/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1654314595761,
  "history_end_time" : 1654314622913,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "roeaa3",
  "indicator" : "Done"
},{
  "history_id" : "xhs0hfj3vt1",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1653698168583,
  "history_end_time" : 1653698191547,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "yes2j510p4h",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1653698066046,
  "history_end_time" : 1653698090615,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "whpe74a67nj",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 40, in <module>\n    data_frame = np.concatenate(merged)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: need at least one array to concatenate\n",
  "history_begin_time" : 1652934712774,
  "history_end_time" : 1652934713350,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tbshp6bx5fr",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1652831918954,
  "history_end_time" : 1652831931900,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "kund9d432sq",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1652831049849,
  "history_end_time" : 1652831064507,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "jt875cy4ef5",
  "history_input" : null,
  "history_output" : "Remote SCP command had error: scp: jt875cy4ef5.tar: Disk quota exceeded",
  "history_begin_time" : 1652786133164,
  "history_end_time" : 1652786134999,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Failed"
},{
  "history_id" : "bmqxvjnjx3i",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1652054941727,
  "history_end_time" : 1652054955890,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "km8axqdh0mf",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 29)   # start date\nedate = date(2022, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"processing_observation_data.py\", line 40, in <module>\n    data_frame = np.concatenate(merged)\n  File \"<__array_function__ internals>\", line 180, in concatenate\nValueError: need at least one array to concatenate\n",
  "history_begin_time" : 1652047733077,
  "history_end_time" : 1652047812249,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "ofroggd80hp",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 9, 30)   # start date\nedate = date(2022, 1, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      dt=np.tile(dt,2822) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\n\ndt = np.concatenate(date_time)\n\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/ofroggd80hp/processing_observation_data.py\", line 48, in <module>\n    dff['YYYYMMDDHH'] = dt.tolist()\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3612, in __setitem__\n    self._set_item(key, value)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3784, in _set_item\n    value = self._sanitize_column(value)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 4509, in _sanitize_column\n    com.require_length_match(value, self.index)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/common.py\", line 531, in require_length_match\n    raise ValueError(\nValueError: Length of values (4334592) does not match length of index (4336926)\n",
  "history_begin_time" : 1650480126065,
  "history_end_time" : 1650480291055,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "b45u3e93gvh",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 9, 30)   # start date\nedate = date(2022, 1, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      dt=np.tile(dt,2822) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\n\ndt = np.concatenate(date_time)\n\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/b45u3e93gvh/processing_observation_data.py\", line 48, in <module>\n    dff['YYYYMMDDHH'] = dt.tolist()\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3612, in __setitem__\n    self._set_item(key, value)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3784, in _set_item\n    value = self._sanitize_column(value)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 4509, in _sanitize_column\n    com.require_length_match(value, self.index)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/core/common.py\", line 531, in require_length_match\n    raise ValueError(\nValueError: Length of values (4334592) does not match length of index (4336926)\n",
  "history_begin_time" : 1650472973439,
  "history_end_time" : 1650473134725,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "iqk0r94iss6",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 12, 30)   # start date\nedate = date(2022, 1, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      dt=np.tile(dt,2822) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\n\ndt = np.concatenate(date_time)\n\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1650252032069,
  "history_end_time" : 1650252061995,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},{
  "history_id" : "kvami6vyahh",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 12, 30)   # start date\nedate = date(2022, 1, 1)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      dt=np.tile(dt,2822) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\n\ndt = np.concatenate(date_time)\n\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "",
  "history_begin_time" : 1650214626715,
  "history_end_time" : 1650214654278,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "3wgogh",
  "indicator" : "Done"
},]
