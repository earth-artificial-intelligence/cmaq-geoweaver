[{
  "history_id" : "tj4jv5v6dmi",
  "history_input" : "# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\n# Required packages to run this process.\nrequired = {'pandas','pathlib','sklearn','numpy','keras','tensorflow','tensorflow-gpu','autokeras','kaleido','glob2','scipy','netCDF4','xarray','geopandas','Shapely','rasterio','earthpy','opencv-python','Pillow'}\n#required = {'xgboost'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\n#if missing:\n#    print(\"Packages missing and will be installed: \", missing)\n#    python = sys.executable\n#    subprocess.check_call(\n#        [python, '-m', 'pip', 'install', *missing],\n#      stdout=subprocess.DEVNULL)\n    #subprocess.check_call(\n        #[python, '-m', 'conda', 'install', '-c','conda-forge','xgboost'],\n      #stdout=subprocess.DEVNULL)\n\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models','cmaq/observation']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "history_output" : "",
  "history_begin_time" : 1652831757425,
  "history_end_time" : 1652831769148,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "sw6k8lmj53u",
  "history_input" : "import xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\naa,bb,cc,dd,ee,ff,gg,hh,ii,jj,kk,ll,mm,nn,oo1,pp,qq,rr,ss=(\n    [] for i in range(19))\n#ff=[]\n# k = time dimension - start from 12 to match with data\nt = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in days:\n  print(i)\n  # read cmaq results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20210315 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  else:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# O3 variable\n  \t# O3 variable\n      oo=df.variables['O3'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)      \n      aa.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      bb.append(o3tp)\n      # CO\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      cc.append(o3tp)\n\n      # PM25_CO\n      oo=df.variables['PM25_OC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ee.append(o3tp)\n      \n      \n  # read emission results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20191231 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_nobeis_2016fh_16j.ncf\")\n  elif int(i)==20220303:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n\n# set todays date if they don't change dataformate    \n#  else if int(i)>=20220313 and int(i)<=int(today):\n  elif int(i)>=20220313 and int(i)<=20220331:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ff.append(o3tp)\n\n      # NO\n#      oo=df.variables['NO'][:].values[k,0]\n#      oo3=np.ravel(oo)\n#      o3tp=np.transpose(oo3)\n#      o3tp=np.round(o3tp)\n#      hh.append(o3tp)  \n      \n# read mcip results \n# date must be later of 20210101\n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/\"+\"METCRO2D_\"+i+\".nc\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['PRSFC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ii.append(o3tp)\n  \t# NO2\n      oo=df.variables['PBL'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      jj.append(o3tp)\n      # NO\n      oo=df.variables['TEMP2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      kk.append(o3tp)\n            # NO\n      oo=df.variables['WSPD10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ll.append(o3tp)\n            # NO\n      oo=df.variables['WDIR10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      mm.append(o3tp)\n\n            # NO\n      oo=df.variables['RGRND'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      oo1.append(o3tp)\n\n        \t# NO2\n      oo=df.variables['CFRAC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      rr.append(o3tp)\n      \n      \ncmaq_O3=list(np.concatenate(aa).flat) \nprint(len(cmaq_O3))\ndel aa\ncmaq_NO2=list(np.concatenate(bb).flat) \nprint(len(cmaq_NO2))\ndel bb\ncmaq_CO=list(np.concatenate(cc).flat) \nprint(len(cmaq_CO))\ndel cc\n\ncmaq_PM25_CO=list(np.concatenate(ee).flat)\n\ndel ee\nCO_emi=list(np.concatenate(ff).flat) \nprint(len(CO_emi))\ndel ff\n\n#NO_emi=list(np.concatenate(hh).flat) \n#del hh\nPRSFC=list(np.concatenate(ii).flat) \ndel ii\nPBL=list(np.concatenate(jj).flat) \ndel jj\nTEMP2=list(np.concatenate(kk).flat) \ndel kk\nWSPD10=list(np.concatenate(ll).flat) \ndel ll\nWDIR10=list(np.concatenate(mm).flat)\ndel mm\n\nRGRND=list(np.concatenate(oo1).flat) \ndel oo1\n\nCFRAC=list(np.concatenate(rr).flat)\nprint(len(CFRAC))\ndel rr\n\n## selecting lat and long\ndf = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\nlat_1 = df.variables['LAT'][:].values[0,0]\nlat_flt=np.ravel(lat_1)\n# need to manipulate 48 values if the next day data is available\nLAT=np.tile(lat_flt,len(days)*24)\nprint(len(LAT))\n# long\nlon_1 = df.variables['LON'][:].values[0,0]\nlon_flt=np.ravel(lon_1)\n# need to manipulate 48 values if the next day data is available\nLON=np.tile(lon_flt,len(days)*24)\nprint(len(LON))\n# creating dataframe\n\n## creatime date-time dimension\n# date-time dimension for today\ntime0=[]\nt = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n  for j in t:\n    time_0=np.full((265,442),i+j)\n    time0.append(time_0)\nYYMMDDHH=list(np.concatenate(time0).flat)  \nprint(len(YYMMDDHH))\n\n\n# saving variables\ndat=pd.DataFrame({'Latitude':LAT,'Longitude':LON,'YYYYMMDDHH':YYMMDDHH,'CMAQ12KM_O3(ppb)':cmaq_O3,'CMAQ12KM_NO2(ppb)':cmaq_NO2,'CMAQ12KM_CO(ppm)':cmaq_CO,'CMAQ_OC(ug/m3)':cmaq_PM25_CO,'CO(moles/s)':CO_emi,'PRSFC(Pa)':PRSFC,'PBL(m)':PBL,'TEMP2(K)':TEMP2,'WSPD10(m/s)':WSPD10,'WDIR10(degree)':WDIR10,'RGRND(W/m2)':RGRND,'CFRAC':CFRAC})\nprint(dat.head())\ndat.to_csv(home+'/cmaq/training_data.csv',index=False)\n\n",
  "history_output" : "20211030\n2811120\n2811120\n2811120\n2811120\n2811120\n2811120\n2811120\n2811120\n    Latitude   Longitude  YYYYMMDDHH  ...  WDIR10(degree)  RGRND(W/m2)  CFRAC\n0  21.829086 -120.620789  2021103012  ...           348.0        312.0    0.0\n1  21.855751 -120.512497  2021103012  ...           346.0        310.0    0.0\n2  21.882309 -120.404144  2021103012  ...           345.0        308.0    0.0\n3  21.908745 -120.295715  2021103012  ...           344.0        306.0    0.0\n4  21.935051 -120.187225  2021103012  ...           344.0        304.0    0.0\n\n[5 rows x 15 columns]\n",
  "history_begin_time" : 1652831769607,
  "history_end_time" : 1652831897871,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "or9x6vgtr96",
  "history_input" : "\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\n# home directory\nhome = str(Path.home())\n\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n  for t in time:\n    files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n    with open(files, 'r') as file:\n      text = file.read()\n    new_string = text.replace('\"', '')\n\n    outF = open(home + \"/cmaq/observation/AQF5X_Hourly_\"+i+t+\".txt\", \"w\")\n    for line in new_string:\n      # write line to output file\n      outF.write(line)\n    outF.close()",
  "history_output" : "",
  "history_begin_time" : 1652831898630,
  "history_end_time" : 1652831917948,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "tbshp6bx5fr",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n# home directory\nhome = str(Path.home())\n\ndays=[]\nfrom datetime import date, timedelta\n\nsdate = date(2021, 10, 30)   # start date\nedate = date(2021, 10, 30)   # end date\n\ndelta = edate - sdate       # as timedelta\n\nfor i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n    \n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor d in days:\n  for t in time:\n\n    files=glob.glob(home+\"/cmaq/observation/AQF5X_Hourly_\"+d+t+\".txt\")\n    for file in files:\n      print(file)\n\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # 2822 total station number\n      date_time.append(dt)\n      merged.append(data)\ndata_frame = np.concatenate(merged)\ndata_frame =np.delete(data_frame, np.s_[4:9], axis=1) \n#data_frame.drop(data_frame.columns[[4, 5,6,7,8]], axis = 1, inplace = True)\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\nprint(len(days))\nprint(df.shape)\ndt = np.concatenate(date_time)\nprint(dt.shape)\n#dt = np.concatenate(date_time)\n#print(len(dt))\ndff=df.replace(',','', regex=True)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(home+\"/cmaq/observation.csv\",index=False)\n\n\n\n",
  "history_output" : "/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103012.txt\n2021103012\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103013.txt\n2021103013\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103014.txt\n2021103014\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103015.txt\n2021103015\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103016.txt\n2021103016\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103017.txt\n2021103017\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103018.txt\n2021103018\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103019.txt\n2021103019\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103020.txt\n2021103020\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103021.txt\n2021103021\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103022.txt\n2021103022\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103023.txt\n2021103023\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103000.txt\n2021103000\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103001.txt\n2021103001\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103002.txt\n2021103002\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103003.txt\n2021103003\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103004.txt\n2021103004\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103005.txt\n2021103005\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103006.txt\n2021103006\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103007.txt\n2021103007\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103008.txt\n2021103008\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103009.txt\n2021103009\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103010.txt\n2021103010\n/home/aalnaim/cmaq/observation/AQF5X_Hourly_2021103011.txt\n2021103011\n1\n(68592, 4)\n(68592,)\n",
  "history_begin_time" : 1652831918954,
  "history_end_time" : 1652831931900,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "4ltasiwogfp",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/training_data.csv\")\nobs=pd.read_csv(home+\"/cmaq/observation.csv\")\nref_stations=pd.read_csv(\"/groups/ESS/mislam25/station_cmaq_location.csv\")\n\nnew_df = pd.merge(ref_stations, cmaq,  how='left', left_on=['Latitude_y','Longitude_y'], right_on = ['Latitude','Longitude'])\nnew_df.to_csv(home+\"/cmaq/subset.csv\")\nfinal = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\n#final.to_csv(home+\"/cmaq/subset.csv\",index=False)\nfinal=final.drop_duplicates(keep=False)\ntraining_data = final.loc[:,~final.columns.duplicated()]\n# dropping unnecessary variables\ntraining_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n\nnew_df=training_data.drop(['StationID','Latitude_y','Longitude_y','YYYYMMDDHH'],axis=1)\nfinal_df = new_df[new_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+\"/cmaq/training.csv\",index=False)",
  "history_output" : "training_data.py:12: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Latitude_y', 'Longitude_y'} in the result is deprecated and will raise a MergeError in a future version.\n  final = pd.merge(obs, new_df,  how='left', left_on=['Latitude','Longitude','YYYYMMDDHH'], right_on = ['Latitude_x','Longitude_x','YYYYMMDDHH'])\ntraining_data.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['YYYYMMDDHH'] = training_data['YYYYMMDDHH'].map(str)\ntraining_data.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['month'] = training_data['YYYYMMDDHH'].str[4:6]\ntraining_data.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['day'] = training_data['YYYYMMDDHH'].str[6:8]\ntraining_data.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  training_data['hours'] = training_data['YYYYMMDDHH'].str[8:10]\n",
  "history_begin_time" : 1652831932017,
  "history_end_time" : 1652831959195,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "rkb9aeni5ts",
  "history_input" : "import xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nimport datetime\n# home directory\nhome = str(Path.home())\n\ntoday=datetime.datetime.today().strftime('%Y%m%d')\npday_= datetime.datetime.today() - datetime.timedelta(days=1)\npday=pday_.strftime('%Y%m%d')\nfday_= datetime.datetime.today() + datetime.timedelta(days=1)\nfday=fday_.strftime('%Y%m%d')\ndays=[today,pday]\naa,bb,cc,dd,ee,ff,gg,hh,ii,jj,kk,ll,mm,nn,oo1,pp,qq,rr,ss=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n#ff=[]\n# k = time dimension - start from 12 to match with data\nt = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\nfor i in days:\n  print(i)\n  # read cmaq results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20210315 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  else:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# O3 variable\n  \t# O3 variable\n      oo=df.variables['O3'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)      \n      aa.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      bb.append(o3tp)\n      # CO\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      cc.append(o3tp)\n\n      # PM25_CO\n      oo=df.variables['PM25_OC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ee.append(o3tp)\n      \n      \n  # read emission results\n  # old files before 20210315 are not in diractory. must choose later date.\n  if int(i)>=20191231 and int(i)<=20210902:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_nobeis_2016fh_16j.ncf\")\n  elif int(i)==20220303:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n\n# set todays date if they don't change dataformate    \n#  else if int(i)>=20220313 and int(i)<=int(today):\n  elif int(i)>=20220313 and int(i)<=20220331:\n    files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  for j in files:\n\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ff.append(o3tp)\n\n      # NO\n#      oo=df.variables['NO'][:].values[k,0]\n#      oo3=np.ravel(oo)\n#      o3tp=np.transpose(oo3)\n#      o3tp=np.round(o3tp)\n#      hh.append(o3tp)  \n      \n# read mcip results \n# date must be later of 20210101\n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/\"+\"METCRO2D_\"+i+\".nc\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in t:\n  \t# CO variable\n      oo=df.variables['PRSFC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ii.append(o3tp)\n  \t# NO2\n      oo=df.variables['PBL'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      jj.append(o3tp)\n      # NO\n      oo=df.variables['TEMP2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      kk.append(o3tp)\n            # NO\n      oo=df.variables['WSPD10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      ll.append(o3tp)\n            # NO\n      oo=df.variables['WDIR10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      mm.append(o3tp)\n\n            # NO\n      oo=df.variables['RGRND'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      oo1.append(o3tp)\n\n        \t# NO2\n      oo=df.variables['CFRAC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      o3tp=np.round(o3tp)\n      rr.append(o3tp)\n      \n      \ncmaq_O3=list(np.concatenate(aa).flat) \nprint(len(cmaq_O3))\ndel aa\ncmaq_NO2=list(np.concatenate(bb).flat) \nprint(len(cmaq_NO2))\ndel bb\ncmaq_CO=list(np.concatenate(cc).flat) \nprint(len(cmaq_CO))\ndel cc\n\ncmaq_PM25_CO=list(np.concatenate(ee).flat)\n\ndel ee\nCO_emi=list(np.concatenate(ff).flat) \nprint(len(CO_emi))\ndel ff\n\n#NO_emi=list(np.concatenate(hh).flat) \n#del hh\nPRSFC=list(np.concatenate(ii).flat) \ndel ii\nPBL=list(np.concatenate(jj).flat) \ndel jj\nTEMP2=list(np.concatenate(kk).flat) \ndel kk\nWSPD10=list(np.concatenate(ll).flat) \ndel ll\nWDIR10=list(np.concatenate(mm).flat)\ndel mm\n\nRGRND=list(np.concatenate(oo1).flat) \ndel oo1\n#RN=list(np.concatenate(pp).flat)\n#del pp\n#RC=list(np.concatenate(qq).flat)\n#del qq\nCFRAC=list(np.concatenate(rr).flat)\nprint(len(CFRAC))\ndel rr\n\n## selecting lat and long\ndf = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\nlat_1 = df.variables['LAT'][:].values[0,0]\nlat_flt=np.ravel(lat_1)\n# need to manipulate 48 values if the next day data is available\nLAT=np.tile(lat_flt,len(days)*24)\nprint(len(LAT))\n# long\nlon_1 = df.variables['LON'][:].values[0,0]\nlon_flt=np.ravel(lon_1)\n# need to manipulate 48 values if the next day data is available\nLON=np.tile(lon_flt,len(days)*24)\nprint(len(LON))\n# creating dataframe\n\n## creatime date-time dimension\n# date-time dimension for today\ntime0=[]\nt = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in days:\n  for j in t:\n    time_0=np.full((265,442),i+j)\n    time0.append(time_0)\nYYMMDDHH=list(np.concatenate(time0).flat)  \nprint(len(YYMMDDHH))\n\n\n# saving variables\ndat=pd.DataFrame({'Latitude':LAT,'Longitude':LON,'YYYYMMDDHH':YYMMDDHH,'CMAQ12KM_O3(ppb)':cmaq_O3,'CMAQ12KM_NO2(ppb)':cmaq_NO2,'CMAQ12KM_CO(ppm)':cmaq_CO,'CMAQ_OC(ug/m3)':cmaq_PM25_CO,'CO(moles/s)':CO_emi,'PRSFC(Pa)':PRSFC,'PBL(m)':PBL,'TEMP2(K)':TEMP2,'WSPD10(m/s)':WSPD10,'WDIR10(degree)':WDIR10,'RGRND(W/m2)':RGRND,'CFRAC':CFRAC})\nprint(dat.head())\ndat.to_csv(home+'/cmaq/test_data.csv',index=False)\n\n",
  "history_output" : "20220517\n20220516\n5622240\n5622240\n5622240\n5622240\n5622240\n5622240\n5622240\n5622240\n    Latitude   Longitude  YYYYMMDDHH  ...  WDIR10(degree)  RGRND(W/m2)  CFRAC\n0  21.829086 -120.620789  2022051712  ...            13.0        587.0    0.0\n1  21.855751 -120.512497  2022051712  ...            13.0        586.0    0.0\n2  21.882309 -120.404144  2022051712  ...            13.0        584.0    0.0\n3  21.908745 -120.295715  2022051712  ...            13.0        583.0    0.0\n4  21.935051 -120.187225  2022051712  ...            13.0        581.0    0.0\n\n[5 rows x 15 columns]\n",
  "history_begin_time" : 1652831960005,
  "history_end_time" : 1652832228551,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "pqwcp469fyi",
  "history_input" : "import pandas as pd\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ncmaq=pd.read_csv(home+\"/cmaq/test_data.csv\")\n\n# dropping unnecessary variables\ncmaq['YYYYMMDDHH'] = cmaq['YYYYMMDDHH'].map(str)\ncmaq['month'] = cmaq['YYYYMMDDHH'].str[4:6]\ncmaq['day'] = cmaq['YYYYMMDDHH'].str[6:8]\ncmaq['hours'] = cmaq['YYYYMMDDHH'].str[8:10]\n\n#new_df=cmaq.drop(['YYYYMMDDHH'],axis=1)\ncmaq.to_csv(home+\"/cmaq/testing.csv\",index=False)",
  "history_output" : "",
  "history_begin_time" : 1652832229164,
  "history_end_time" : 1652832313323,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "6j67tfndue7",
  "history_input" : "# importing necessary libraries\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport geopandas as gpd\nimport cmaps\nfrom datetime import datetime\n\nimport imageio\nimport glob\n\n\nmatplotlib.rcParams['font.size'] = 25\nnorm= matplotlib.colors.Normalize(vmin=0,vmax=80)\n\n\n# home directory\nhome = str(Path.home())\n\n# importing data\ndf = pd.read_csv(home + '/cmaq/prediction_files/prediction_rf.csv')\nAirnowObs = pd.read_csv(home + \"/cmaq/observation.csv\")\n\ntime_ = df['YYYYMMDDHH'].unique()\ndfs = dict(tuple(df.groupby('YYYYMMDDHH')))  # grouping the data by YYMMDDHH\n\n#fig, ax = plt.subplots(figsize=(20, 25))\n\n\n\nfor t in time_:\n    \n    df = dfs[t]\n    \n    gdf = gpd.GeoDataFrame(\n        df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n    #gdf = gdf.set_crs(\"EPSG:3762\", allow_override=True)\n    #gdf = gdf.set_crs(\"EPSG:4326\")\n    gdf.plot(column='prediction', legend=True, figsize=(20, 25),\n         cmap=cmaps.WhiteBlueGreenYellowRed, legend_kwds={'orientation': \"horizontal\", \"pad\": 0.05}, vmin=0, vmax=80 )\n    \n    # Add individual AirNow stations in US with color representation.\n    \n    # gdfAirnow = gpd.GeoDataFrame(\n        # AirnowObs, geometry=gpd.points_from_xy(AirnowObs.Longitude, AirnowObs.Latitude))\n    # gdfAirnow.plot(ax=predDF, column='AirNOW_O3', marker='o', cmap=cmaps.WhiteBlueGreenYellowRed, figsize=(20, 20))\n    \n    # Add US states boundries.\n    \n    # states = gpd.read_file('usStates/cb_2018_us_state_500k.shp')\n\t# states = states.to_crs(\"EPSG:4326\")\n\t# states.boundary.plot(ax=ax)\n    \n    dateObj = datetime.strptime(str(t), \"%Y%m%d%H\")\n    plotTitle = datetime.strftime(dateObj, \"%Y-%m-%d (Time: %-H)\")\n    plt.title(plotTitle, fontdict={'fontsize': 35})\n    plt.savefig(home + \"/cmaq/prediction_maps/\" + str(t) + \".tif\")\n\n    \nfiles=glob.glob(home + \"/cmaq/prediction_maps/*.tif\")\nimages=[]\n\nfor i in files:\n    print(i)\n    img=imageio.imread(i)\n    images.append(img)\n    \nimageio.mimsave(home+'/prediction.gif', images)",
  "history_output" : "/home/aalnaim/CMAQAI/lib/python3.8/site-packages/geopandas/plotting.py:661: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n  fig, ax = plt.subplots(figsize=figsize)\nTraceback (most recent call last):\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/PIL/Image.py\", line 2212, in save\n    save_handler(self, fp, filename)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/PIL/TiffImagePlugin.py\", line 1769, in _save\n    ImageFile._save(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/PIL/ImageFile.py\", line 496, in _save\n    fp.flush()\nOSError: [Errno 122] Disk quota exceeded\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"prediction_maps.py\", line 58, in <module>\n    plt.savefig(home + \"/cmaq/prediction_maps/\" + str(t) + \".tif\")\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/matplotlib/pyplot.py\", line 958, in savefig\n    res = fig.savefig(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/matplotlib/figure.py\", line 3019, in savefig\n    self.canvas.print_figure(fname, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/matplotlib/backend_bases.py\", line 2319, in print_figure\n    result = print_method(\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/matplotlib/backend_bases.py\", line 1648, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\", line 594, in print_tif\n    return (Image.fromarray(np.asarray(self.buffer_rgba()))\n  File \"/home/aalnaim/CMAQAI/lib/python3.8/site-packages/PIL/Image.py\", line 2216, in save\n    fp.close()\nOSError: [Errno 122] Disk quota exceeded\n",
  "history_begin_time" : 1652832428259,
  "history_end_time" : 1652832957060,
  "history_notes" : null,
  "history_process" : "6oifw4",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "4d4qd82x1ax",
  "history_input" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\n\n",
  "history_output" : "   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   44.402981   -65.202766       16.0  ...     10   30     12\n1   45.006100   -65.000000       26.0  ...     10   30     12\n2   45.957291   -66.645233       28.0  ...     10   30     12\n3   45.309200   -66.009201       20.0  ...     10   30     12\n4   45.253601   -66.080299       17.0  ...     10   30     12\n\n[5 rows x 18 columns]\n/home/aalnaim/CMAQAI/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n  warn(\n",
  "history_begin_time" : 1652832313864,
  "history_end_time" : 1652832356996,
  "history_notes" : null,
  "history_process" : "u2h4a7",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "upczcif6qx6",
  "history_input" : "\n# Importing necessary libraries\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\n\n# home directory\nhome = str(Path.home())\n# importing data\nfinal=pd.read_csv(home+'/cmaq/testing.csv')\nprint(final.head())\nX = final.drop(['YYYYMMDDHH','Latitude','Longitude',],axis=1)\n# defining  testing variables\n# processing test data\n\n# load the model from disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\n# making prediction\npred = loaded_model.predict(X)\n\n# adding prediction values to test dataset\nfinal['prediction'] = pred.tolist()\n\nfinal = final[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n# saving the dataset into local drive\nfinal.to_csv(home+'/cmaq/prediction_files/prediction_rf.csv',index=False)",
  "history_output" : "    Latitude   Longitude  YYYYMMDDHH  ...  month  day  hours\n0  21.829086 -120.620790  2022051712  ...      5   17     12\n1  21.855751 -120.512500  2022051712  ...      5   17     12\n2  21.882309 -120.404144  2022051712  ...      5   17     12\n3  21.908745 -120.295715  2022051712  ...      5   17     12\n4  21.935051 -120.187225  2022051712  ...      5   17     12\n\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1652832357494,
  "history_end_time" : 1652832428017,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
}]
