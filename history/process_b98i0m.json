[{
  "history_id" : "0g5zze1zikj",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\ntropomiDaily = tropomi.groupby(\"Date\").max()\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\", index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1667526006415,
  "history_end_time" : 1667526088990,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "uedjextsck1",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\ntropomiDaily = tropomi.groupby(\"Date\").max()\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\", index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1667525501592,
  "history_end_time" : 1667525582539,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "LnQxPfoKezBl",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\ntropomiDaily = tropomi.groupby(\"Date\").max()\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667524405554,
  "history_end_time" : 1667524493167,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZYWWLUWQBLI6",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomiDaily = tropomi.groupby(\"Date\").max()\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_data.py\", line 18, in <module>\n    tropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 3978, in __setitem__\n    self._set_item(key, value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 4172, in _set_item\n    value = self._sanitize_column(value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 4905, in _sanitize_column\n    com.require_length_match(value, self.index)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\", line 561, in require_length_match\n    raise ValueError(\nValueError: Length of values (58) does not match length of index (321)\n",
  "history_begin_time" : 1667524229079,
  "history_end_time" : 1667524230067,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TrrH6r0UehvU",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomi[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomi.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_data.py\", line 17, in <module>\n    tropomi[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 3978, in __setitem__\n    self._set_item(key, value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 4172, in _set_item\n    value = self._sanitize_column(value)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 4905, in _sanitize_column\n    com.require_length_match(value, self.index)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/common.py\", line 561, in require_length_match\n    raise ValueError(\nValueError: Length of values (58) does not match length of index (321)\n",
  "history_begin_time" : 1667523929193,
  "history_end_time" : 1667523930110,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "WSaePDUQVymg",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_data.py\", line 17, in <module>\n    tropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\nNameError: name 'tropomiDaily' is not defined\n",
  "history_begin_time" : 1667523899160,
  "history_end_time" : 1667523899960,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9lPeUhYm8J5q",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\n# Make date match CMAQ and Airnow retrieved data. This is done only becuase TROPOMI doesn't have data before 2018, so making the data the same as the other source for merging later.\n# If the data retrieved for CMAQ and Airnow is after 2018, then this below line can be deleted.\ntropomiDaily[\"Date\"] = pd.date_range(\"20170101\", \"20170227\").strftime('%Y%m%d')\ntropomiDaily.reset_index(drop=True, inplace=True)\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\", \"Unnamed: 0\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['Date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomiDaily, on=\"Date\").merge(cmaq, on=\"Date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"merge_data.py\", line 4, in <module>\n    airnow = pd.read_csv('airnow_data.csv', parse_dates=[\"Date\"])\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 317, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1729, in _make_engine\n    self.handles = get_handle(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\", line 857, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: 'airnow_data.csv'\n",
  "history_begin_time" : 1667523883221,
  "history_end_time" : 1667523884242,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "55LUQHgmDke2",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667510615957,
  "history_end_time" : 1667510693501,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9bt8NpgYAzrJ",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667510278838,
  "history_end_time" : 1667510353461,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "E3yFUrOE5b2e",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match predictors scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n# Reformat date string\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan_Feb.csv', parse_dates=[\"date\"])\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Reformat date string\ncmaq['date'] = cmaq[\"date\"].dt.strftime('%Y%m%d')\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, left_on=\"Date\", right_on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667510093377,
  "history_end_time" : 1667510174297,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vutk6dw46nw",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667311142855,
  "history_end_time" : 1667311155908,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gpe2bpvxedn",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667310963399,
  "history_end_time" : 1667310975282,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8n5zknvr68e",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "",
  "history_begin_time" : 1667310889254,
  "history_end_time" : 1667310901919,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "df2oc1vjitl",
  "history_input" : "import pandas as pd\n\n# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"ch15_merge_data.py\", line 18, in <module>\n    cmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 317, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1729, in _make_engine\n    self.handles = get_handle(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\", line 857, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv'\n",
  "history_begin_time" : 1667310729213,
  "history_end_time" : 1667310730178,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "3l1hkkd06ud",
  "history_input" : "# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"ch15_merge_data.py\", line 2, in <module>\n    airnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\nNameError: name 'pd' is not defined\n",
  "history_begin_time" : 1667310661017,
  "history_end_time" : 1667310661210,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Failed"
},]
