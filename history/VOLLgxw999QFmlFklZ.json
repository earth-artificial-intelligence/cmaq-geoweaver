[{
  "history_id" : "mspy22j10ba",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928639,
  "history_end_time" : 1720581928639,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "58e5qxz2ptx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928640,
  "history_end_time" : 1720581928640,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ybzxqddoift",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928641,
  "history_end_time" : 1720581928641,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "urrka2sryjb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928642,
  "history_end_time" : 1720581928642,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xladfktv3z0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928644,
  "history_end_time" : 1720581928644,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c0r1e776e7z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928645,
  "history_end_time" : 1720581928645,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q1dlqyiagd3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928645,
  "history_end_time" : 1720581928645,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "muaxog5256v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928646,
  "history_end_time" : 1720581928646,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d0xo0smyk7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928647,
  "history_end_time" : 1720581928647,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mlxa442hulf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928648,
  "history_end_time" : 1720581928648,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ailky6th4md",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781959,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e4uusyor5sz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928651,
  "history_end_time" : 1720581928651,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xp56m1l73ak",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928652,
  "history_end_time" : 1720581928652,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2otk7laz6to",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928653,
  "history_end_time" : 1720581928653,
  "history_notes" : null,
  "history_process" : "pvzabv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j92b4zslk0x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928654,
  "history_end_time" : 1720581928654,
  "history_notes" : null,
  "history_process" : "8i9ptn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e7jz903tap5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781953,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0quuzcva681",
  "history_input" : "#!/bin/bash\n\ndays_back=40\n\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\ncmaq_gif_location=\"/groups/ESS/share/projects/SWUS3km/graph/12km/\"\n\necho \"start to traverse \"${cmaq_gif_location}\n\nfor i in $(seq 0 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i))\n  # Setting env variables\n  YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  #/groups/ESS/share/projects/SWUS3km/graph/12km/20221108/FORECAST_O3_20221108.gif\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/\"FORECAST_O3_\"$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/obsoverlay/gif/OBS-FORECAST_O3_$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  \ndone\n",
  "history_output" : "start to traverse /groups/ESS/share/projects/SWUS3km/graph/12km/\n0 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240709/obsoverlay/gif/OBS-FORECAST_O3_20240709.gif': No such file or directory\n1 days ago\n2 days ago\n3 days ago\n4 days ago\n5 days ago\n6 days ago\n7 days ago\n8 days ago\n9 days ago\n10 days ago\n11 days ago\n12 days ago\n13 days ago\n14 days ago\n15 days ago\n16 days ago\n17 days ago\n18 days ago\n19 days ago\n20 days ago\n21 days ago\n22 days ago\n23 days ago\n24 days ago\n25 days ago\n26 days ago\n27 days ago\n28 days ago\n29 days ago\n30 days ago\n31 days ago\n32 days ago\n33 days ago\n34 days ago\n35 days ago\n36 days ago\n37 days ago\n38 days ago\n39 days ago\n40 days ago\n",
  "history_begin_time" : 1720581930033,
  "history_end_time" : 1720581934675,
  "history_notes" : null,
  "history_process" : "nndpw6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "55psbeuvuwh",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781974,
  "history_notes" : null,
  "history_process" : "gn54f0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w3deykf2rd4",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781964,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uw6d2h6phxk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720581928659,
  "history_end_time" : 1720581928659,
  "history_notes" : null,
  "history_process" : "slsirb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y3ng6ijcnfb",
  "history_input" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"rf_prediction_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J rf_prediction       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 8               # Number of CPUs per task (threads)\n#SBATCH --mem=150G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u << INNER_EOF\n\n\n# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\nfrom pathlib import Path\n\nprint(\"create and clean the prediction folder\")\n#create_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      file_path = os.path.join(testing_path, file_name)\n      testing_df = pd.read_csv(file_path)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      print(\"adding month, day, and hours\")\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[4:6], errors='coerce', downcast='integer')\n      testing_df['day'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[6:8], errors='coerce', downcast='integer')\n      testing_df['hours'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[8:10], errors='coerce', downcast='integer')\n\n      print(testing_df['YYYYMMDDHH'].values[0])\n      print(testing_df['month'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      print(f\"file_dateTime={file_dateTime}\")\n\n      final_file_path = f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv'\n\n      if Path(final_file_path).is_file():\n        print(f\"The file {file_path} exists. Skipped.\")\n        continue\n\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(,index=False)\n        \nprint(\"Prediction is all done.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        # Print the newly added content\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\ncat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\nexit $exit_code\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/y3ng6ijcnfb\nwrite the slurm script into rf_prediction_slurm_generated.sh\nsbatch rf_prediction_slurm_generated.sh\njob_id=2036774\ncat: file_name: No such file or directory\n/home/zsun/gw-workspace/y3ng6ijcnfb/gw-giKnlrgTR78MXG5V5AHoebQFR8-y3ng6ijcnfb.sh: line 136: : No such file or directory\n1c1,4\n< \n---\n>   File \"<stdin>\", line 84\n>     testing_df.to_csv(,index=False)\n>                       ^\n> SyntaxError: invalid syntax\nJob 2036774 has finished with state: JobState=FAILED\nSlurm job (2036774) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n2036774      rf_predic+     FAILED      1:0            2024-07-09T23:39:32 2024-07-09T23:39:33 \n2036774.bat+      batch     FAILED      1:0       396K 2024-07-09T23:39:32 2024-07-09T23:39:33 \n2036774.ext+     extern  COMPLETED      0:0          0 2024-07-09T23:39:32 2024-07-09T23:39:33 \n  File \"<stdin>\", line 84\n    testing_df.to_csv(,index=False)\n                      ^\nSyntaxError: invalid syntax\ncat: '/scratch/zsun/test_data_slurm-*-2036774.out': No such file or directory\nAll slurm job for rf_prediction_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1720582767445,
  "history_end_time" : 1720582781942,
  "history_notes" : null,
  "history_process" : "xa1jxg",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ktx0d9ssq19",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781947,
  "history_notes" : null,
  "history_process" : "klx3sc",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "af5zl7w9y44",
  "history_input" : "#!/bin/bash\n\n\necho \"start to run processing_test_data_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"processing_test_data_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J processing_test_data       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython -u  << INNER_EOF\n\n# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\n#create_and_clean_folder(test_folder)  # don't clean folder anymore\n#os.makedirs(test_folder, exist_ok=True)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file, engine='netcdf4')\n  \n  # read mcip results \n  mcip_file = \"/scratch/sma8/forecast/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file, engine='netcdf4')\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\", engine='netcdf4')\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    hourly_target_file = f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    if os.path.exists(hourly_target_file):\n      print(f\"Hourly csv file {hourly_target_file} already exist\")\n      continue\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D', engine='netcdf4')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(hourly_target_file, index=False)\n\nprint('Done with preparing testing data!')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 100  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\nif [[ $job_status != *\"COMPLETED\"* ]]; then\n    exit 1\nfi\n",
  "history_output" : "start to run processing_test_data_generated.sh\n/home/zsun/gw-workspace/af5zl7w9y44\nwrite the slurm script into processing_test_data_generated.sh\nsbatch processing_test_data_generated.sh\njob_id=2036768\n/home/zsun/gw-workspace/af5zl7w9y44/gw-giKnlrgTR78MXG5V5AHoebQFR8-af5zl7w9y44.sh: line 198: : No such file or directory\n/home/zsun/gw-workspace/af5zl7w9y44/gw-giKnlrgTR78MXG5V5AHoebQFR8-af5zl7w9y44.sh: line 202: : No such file or directory\n1c1,158\n< \n---\n> Getting data for: 20240510\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240510.nc doesn't exist\n> Getting data for: 20240511\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240511.nc doesn't exist\n> Getting data for: 20240512\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240512.nc doesn't exist\n> Getting data for: 20240513\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240513.nc doesn't exist\n> Getting data for: 20240514\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240514.nc doesn't exist\n> Getting data for: 20240515\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240515.nc doesn't exist\n> Getting data for: 20240516\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240516.nc doesn't exist\n> Getting data for: 20240517\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240517.nc doesn't exist\n> Getting data for: 20240518\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240518.nc doesn't exist\n> Getting data for: 20240519\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240519.nc doesn't exist\n> Getting data for: 20240520\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240520.nc doesn't exist\n> Getting data for: 20240521\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240521.nc doesn't exist\n> Getting data for: 20240522\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240522.nc doesn't exist\n> Getting data for: 20240523\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240523.nc doesn't exist\n> Getting data for: 20240524\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240524.nc doesn't exist\n> Getting data for: 20240525\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240525.nc doesn't exist\n> Getting data for: 20240526\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240526.nc doesn't exist\n> Getting data for: 20240527\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240527.nc doesn't exist\n> Getting data for: 20240528\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240528.nc doesn't exist\n> Getting data for: 20240529\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240529.nc doesn't exist\n> Getting data for: 20240530\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240530_ML_extracted.nc already exists\n> Getting data for: 20240531\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240531_ML_extracted.nc already exists\n> Getting data for: 20240601\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240601_ML_extracted.nc already exists\n> Getting data for: 20240602\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240602_ML_extracted.nc already exists\n> Getting data for: 20240603\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240603_ML_extracted.nc already exists\n> Getting data for: 20240604\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240604_ML_extracted.nc already exists\n> Getting data for: 20240605\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240605_ML_extracted.nc already exists\n> Getting data for: 20240606\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240606_ML_extracted.nc already exists\n> Getting data for: 20240607\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240607_ML_extracted.nc already exists\n> Getting data for: 20240608\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240608_ML_extracted.nc already exists\n> Getting data for: 20240609\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240609_ML_extracted.nc already exists\n> Getting data for: 20240610\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240610_ML_extracted.nc already exists\n> Getting data for: 20240611\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240611_ML_extracted.nc already exists\n> Getting data for: 20240612\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240612_ML_extracted.nc already exists\n> Getting data for: 20240613\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240613_ML_extracted.nc already exists\n> Getting data for: 20240614\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240614_ML_extracted.nc already exists\n> Getting data for: 20240615\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240615_ML_extracted.nc already exists\n> Getting data for: 20240616\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240616_ML_extracted.nc already exists\n> Getting data for: 20240617\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240617_ML_extracted.nc already exists\n> Getting data for: 20240618\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240618_ML_extracted.nc already exists\n> Getting data for: 20240619\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240619_ML_extracted.nc already exists\n> Getting data for: 20240620\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240620_ML_extracted.nc already exists\n> Getting data for: 20240621\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240621_ML_extracted.nc already exists\n> Getting data for: 20240622\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240622_ML_extracted.nc already exists\n> Getting data for: 20240623\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240623_ML_extracted.nc already exists\n> Getting data for: 20240624\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240624_ML_extracted.nc already exists\n> Getting data for: 20240625\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240625_ML_extracted.nc already exists\n> Getting data for: 20240626\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240626_ML_extracted.nc already exists\n> Getting data for: 20240627\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240627_ML_extracted.nc already exists\n> Getting data for: 20240628\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240628_ML_extracted.nc already exists\n> Getting data for: 20240629\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240629_ML_extracted.nc already exists\n> Getting data for: 20240630\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240630_ML_extracted.nc already exists\n> Getting data for: 20240701\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240701_ML_extracted.nc already exists\n> Getting data for: 20240702\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240702_ML_extracted.nc already exists\n> Getting data for: 20240703\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240703_ML_extracted.nc already exists\n> Getting data for: 20240704\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240704_ML_extracted.nc already exists\n> Getting data for: 20240705\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240705_ML_extracted.nc already exists\n> Getting data for: 20240706\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240706_ML_extracted.nc already exists\n> Getting data for: 20240707\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240707_ML_extracted.nc already exists\n> Getting data for: 20240708\n> /groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240708_ML_extracted.nc already exists\n> Getting data for: 20240709\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_12.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_13.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_14.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_15.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_16.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_17.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_18.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_19.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_20.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_21.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_22.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240709_23.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_00.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_01.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_02.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_03.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_04.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_05.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_06.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_07.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_08.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_09.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_10.csv already exist\n> Hourly csv file /groups/ESS/zsun/cmaq//testing_input_hourly//test_data_20240710_11.csv already exist\n> Getting data for: 20240710\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240710.nc doesn't exist\n> Getting data for: 20240711\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240711.nc doesn't exist\n> Getting data for: 20240712\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240712.nc doesn't exist\n> Getting data for: 20240713\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240713.nc doesn't exist\n> Getting data for: 20240714\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240714.nc doesn't exist\n> Getting data for: 20240715\n> CMAQ file /scratch/sma8/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240715.nc doesn't exist\n> Done with preparing testing data!\nJob 2036768 has finished with state: JobState=COMPLETED\nSlurm job (2036768) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n2036768      processin+  COMPLETED      0:0            2024-07-09T23:25:35 2024-07-09T23:25:38 \n2036768.bat+      batch  COMPLETED      0:0        16K 2024-07-09T23:25:35 2024-07-09T23:25:38 \n2036768.ext+     extern  COMPLETED      0:0          0 2024-07-09T23:25:35 2024-07-09T23:25:38 \nAll slurm job for processing_test_data_generated.sh finishes.\n",
  "history_begin_time" : 1720581933337,
  "history_end_time" : 1720582034579,
  "history_notes" : null,
  "history_process" : "z4du0c",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cyd6t8zgs1n",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1720582781969,
  "history_notes" : null,
  "history_process" : "or8itt",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
