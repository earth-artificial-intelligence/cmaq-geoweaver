[{
  "history_id" : "0d86thahoi2",
  "history_input" : "# Write first python in Geoweaver\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport datetime\nimport numpy as np\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\ntoday=datetime.datetime.today().strftime('%Y%m%d')\npday_= datetime.datetime.today() - datetime.timedelta(days=1)\npday=pday_.strftime('%Y%m%d')\nfday_= datetime.datetime.today() + datetime.timedelta(days=1)\nfday=fday_.strftime('%Y%m%d')\ndays=[pday,today]\n\naa,bb,cc,dd,ee,ff,gg,hh,ii,jj,kk,ll,mm,nn,oo1,pp,qq,rr,ss=[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]\n\n#ff=[]\nfor i in days:\n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/\"+\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"+i+\"_extracted.nc\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in range(24):\n  \t# O3 variable\n      oo=df.variables['O3'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      aa.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      bb.append(o3tp)\n      # CO\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      cc.append(o3tp)\n       # PM25_EC\n      oo=df.variables['PM25_EC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      dd.append(o3tp)\n      # PM25_CO\n      oo=df.variables['PM25_OC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ee.append(o3tp)\n        # NO\n#      oo=df.variables['NO'][:].values[k,0]\n#      oo3=np.ravel(oo)\n#     o3tp=np.transpose(oo3)\n#      ff.append(o3tp)\n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/\"+\"emis_mole_all_\"+i+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in range(24):\n  \t# CO variable\n      oo=df.variables['CO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ff.append(o3tp)\n  \t# NO2\n      oo=df.variables['NO2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      gg.append(o3tp)\n      # NO\n      oo=df.variables['NO'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      hh.append(o3tp)\n      \n  files = glob.glob(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/\"+\"METCRO2D_\"+i+\".nc\")\n  for j in files:\n    df = xr.open_dataset(j)\n    for k in range(24):\n  \t# CO variable\n      oo=df.variables['PRSFC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ii.append(o3tp)\n  \t# NO2\n      oo=df.variables['PBL'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      jj.append(o3tp)\n      # NO\n      oo=df.variables['TEMP2'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      kk.append(o3tp)\n            # NO\n      oo=df.variables['WSPD10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      ll.append(o3tp)\n            # NO\n      oo=df.variables['WDIR10'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      mm.append(o3tp)\n            # NO\n      oo=df.variables['WSTAR'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      nn.append(o3tp)\n            # NO\n      oo=df.variables['RGRND'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      oo1.append(o3tp)\n            # NO\n      oo=df.variables['RN'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      pp.append(o3tp)\n        \t# NO2\n      oo=df.variables['RC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      qq.append(o3tp)\n        \t# NO2\n      oo=df.variables['CFRAC'][:].values[k,0]\n      oo3=np.ravel(oo)\n      o3tp=np.transpose(oo3)\n      rr.append(o3tp)\n\n\ncmaq_O3=list(np.concatenate(aa).flat) \ncmaq_NO2=list(np.concatenate(bb).flat) \ncmaq_CO=list(np.concatenate(cc).flat) \ncmaq_PM25_EC=list(np.concatenate(dd).flat) \ncmaq_PM25_CO=list(np.concatenate(ee).flat)\nCO_emi=list(np.concatenate(ff).flat) \nNO2_emi=list(np.concatenate(gg).flat) \nNO_emi=list(np.concatenate(hh).flat) \nPRSFC=list(np.concatenate(ii).flat) \nPBL=list(np.concatenate(jj).flat) \nTEMP2=list(np.concatenate(kk).flat) \nWSPD10=list(np.concatenate(ll).flat) \nWDIR10=list(np.concatenate(mm).flat) \nWSTAR=list(np.concatenate(nn).flat) \nRGRND=list(np.concatenate(oo1).flat) \nRN=list(np.concatenate(pp).flat)\nRC=list(np.concatenate(qq).flat)\nCFRAC=list(np.concatenate(rr).flat)\n\n## selecting lat and long\ndf = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\nlat_1 = df.variables['LAT'][:].values[0,0]\nlat_flt=np.ravel(lat_1)\n# need to manipulate 48 values if the next day data is available\nLAT=np.tile(lat_flt,48)\n# long\nlon_1 = df.variables['LON'][:].values[0,0]\nlon_flt=np.ravel(lon_1)\n# need to manipulate 48 values if the next day data is available\nLON=np.tile(lon_flt,48)\n# creating dataframe\n\n## creatime date-time dimension\n# date-time dimension for today\ntime0=[]\nt = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in t:\n    time_0=np.full((265,442),today+i)\n    time0.append(time_0)\nt_today=list(np.concatenate(time0).flat)  \n\n# date time dimension for yesterday\ntime1=[]\nfor i in t:\n    time_1=np.full((265,442),pday+i)\n    time1.append(time_1)\nt_pday=list(np.concatenate(time1).flat)\n\n# stacking 2 days values\ntime_var=np.vstack((t_pday,t_today))\nYYMMDDHH=list(np.concatenate(time_var).flat)\n\n# saving variables\ndat=pd.DataFrame({'Latitude':LAT,'Longitude':LON,'YYYYMMDDHH':YYMMDDHH,'CMAQ12KM_O3(ppb)':cmaq_O3,'CMAQ12KM_NO2(ppb)':cmaq_NO2,'CMAQ12KM_CO(ppm)':cmaq_CO,'CMAQ_EC(ug/m3)':cmaq_PM25_EC,'CMAQ_OC(ug/m3)':cmaq_PM25_CO,'NO2(moles/s)':NO2_emi,'CO(moles/s)':CO_emi,'NO(moles/s)':NO_emi,'PRSFC(Pa)':PRSFC,'PBL(m)':PBL,'TEMP2(K)':TEMP2,'WSPD10(m/s)':WSPD10,'WDIR10(degree)':WDIR10,'WSTAR(m/s)':WSTAR,'RGRND(W/m2)':RGRND,'RN(cm)':RN,'RC(cm)':RC,'CFRAC':CFRAC})\ndat.to_csv(home+'/cmaq/test_data.csv',index=False)\n",
  "history_output" : "Running",
  "history_begin_time" : 1648001836115,
  "history_end_time" : 1648001828605,
  "history_notes" : null,
  "history_process" : "ucci9a",
  "host_id" : "vneyuq",
  "indicator" : "Running"
},{
  "history_id" : "dhjqpkher3w",
  "history_input" : "## importing necessary libraries\nimport glob\nimport pandas as pd\nfrom pathlib import Path\n# home directory\nhome = str(Path.home())\nfiles=glob.glob(r'/groups/ESS/mislam25/cmaq_results_2020_2021/*.txt')\n#files=glob.glob(r'D:/Research/CMAQ/2021/text/*.txt')\ndata_frame = pd.DataFrame()\nmerged=[]\nfor file in files:\n    df=pd.read_csv(file)\n    merged.append(df)\ndata_frame = pd.concat(merged)\ndata_frame['YYYYMMDDHH'] = data_frame['YYYYMMDDHH'].map(str)\ndata_frame['year'] = data_frame['YYYYMMDDHH'].str[:4]\ndata_frame['month'] = data_frame['YYYYMMDDHH'].str[4:6]\ndata_frame['day'] = data_frame['YYYYMMDDHH'].str[6:8]\ndata_frame['hours'] = data_frame['YYYYMMDDHH'].str[8:10]\nnew_df=data_frame.drop(['Station ID','CMAQ_NO(ppb)','AirNOW_NO2(ppb)','AirNOW_CO(ppm)'],axis=1)\nfinal_df = new_df.rename(columns={new_df.columns[3]: 'AirNOW_O3'})\nfinal_df = final_df[final_df.AirNOW_O3!= -999]\nfinal_df.to_csv(home+'/cmaq/merged_2020_2021',index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1648001709591,
  "history_end_time" : 1648001739922,
  "history_notes" : null,
  "history_process" : "55afa1",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "yx2vzb3m83w",
  "history_input" : "# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\n# Required packages to run this process.\nrequired = {'pandas','pathlib','sklearn','numpy','keras','tensorflow','tensorflow-gpu','autokeras','kaleido','glob2'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n    print(\"Packages missing and will be installed: \", missing)\n    python = sys.executable\n    subprocess.check_call(\n        [python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "history_output" : "",
  "history_begin_time" : 1648001673291,
  "history_end_time" : 1648001709043,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "i8jgou11h34",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Poocessing the data - shifting columns of NO2\n\n## importing necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nfrom time import sleep\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/merged_2020_2021.csv')\n\n# defining training variables\nyear_2020=final.loc[final['year']==2020]\n\n# Processing training  data\nselected_vars = year_2020.drop(['YYYYMMDDHH','year'],axis=1)\n\n########################################################\n## Correlation matrix heatmap\n# Correlation between different variables\ncorr = selected_vars.corr()\n# Set up the matplotlib plot configuration\nf, ax = plt.subplots(figsize=(18, 10))\n# Generate a mask for upper traingle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Configure a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top + 0.5)\n#ax.set(xlim=(0, 12))\nplt.savefig(home+'/cmaq/exploratory_analysis/correlation.png')\n########################################################\n\n# plotting r2 values of different variables vs AirNOW_O3\n########################################################\ncolumns=list(selected_vars)\nfor i in columns:\n  try:\n  \tsns.lmplot(x=i, y=\"AirNOW_O3\", data=selected_vars);\n  \tplt.savefig(home+'/cmaq/exploratory_analysis/'+i+'_AirNOW_O3.png')\n  except FileNotFoundError:\n    pass\n    \n########################################################\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/i8jgou11h34/exploratory_data_analysis.py\", line 16, in <module>\n    final=pd.read_csv(home+'/cmaq/merged_2020_2021.csv')\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/mislam25/cmaq/merged_2020_2021.csv'\n",
  "history_begin_time" : 1648001740264,
  "history_end_time" : 1648001779276,
  "history_notes" : null,
  "history_process" : "smwp7r",
  "host_id" : "vneyuq",
  "indicator" : "Done"
},{
  "history_id" : "4o5142x7t6h",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\nprint(\"training_xgboost\")\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import VotingRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/merged_2020_2021.csv')\n\n# defining training variables\n#train=final.loc[final['year']==2022]\ntrain=final[final['year'].isin([2020,2021])]\n\n# Processing training  data\nX = train.drop(['AirNOW_O3','YYYYMMDDHH','year'],axis=1)\ny = train['AirNOW_O3']\n\n# Defining voting-ensemble based xgboost model\nmodels = list()\nmodels.append(('cart1', XGBRegressor(max_depth=1)))\nmodels.append(('cart2', XGBRegressor(max_depth=2)))\nmodels.append(('cart3', XGBRegressor(max_depth=3)))\nmodels.append(('cart4', XGBRegressor(max_depth=4)))\nmodels.append(('cart5', XGBRegressor(max_depth=5)))\nmodels.append(('cart6', XGBRegressor(max_depth=6)))\n# define the voting ensemble\nensemble = VotingRegressor(estimators=models)\n\n# fit the model on all available data\nensemble.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/xgboost.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(ensemble, open(filename, 'wb'))\n",
  "history_output" : "training_xgboost\nTraceback (most recent call last):\n  File \"/home/mislam25/gw-workspace/4o5142x7t6h/training_xgboost.py\", line 16, in <module>\n    final=pd.read_csv(home+'/cmaq/merged_2020_2021.csv')\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n    self._engine = self._make_engine(self.engine)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n    self._open_handles(src, kwds)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n    self.handles = get_handle(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 702, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/mislam25/cmaq/merged_2020_2021.csv'\n",
  "history_begin_time" : 1648001780237,
  "history_end_time" : 1648001827877,
  "history_notes" : null,
  "history_process" : "106i1i",
  "host_id" : "vneyuq",
  "indicator" : "Done"
}]
