[{
  "history_id" : "k87w6lx23ro",
  "history_input" : "from pyairnow.conv import aqi_to_concentration\nimport pandas as pd\nimport requests\nimport json\n\n\nlat = '32.047501'\nlon = '-110.773903'\nAPI_KEY = '9FF8FB8B-229C-424F-8882-3AF164DD8C17'\n\n# distance is used when no reporting area is associated with the latitude/longitude, \n# looks for an observation from a nearby reporting area within this distance (in miles).\ndistance = '50' \n\n# Construct a list of date string, each timed at midnight.\ndate_range_str = [date.strftime('%Y-%m-%dT00-0000') for date in pd.date_range('2021-01-01','2021-01-31')]\n\n# An empty list to save the O3 data returned.\nO3_data = []\nCO_data = []\nNO2_data = []\n\n# Loop through each date to intiate an API call for the duration specified above.\nfor dateStr in date_range_str:\n\n    # Construct URL to contact airnowapi.org and get the AQI (Air Quality Index) for our desired location.\n    url = f'https://www.airnowapi.org/aq/observation/latLong/historical/?format=application/json&latitude={lat}&longitude={lon}&date={dateStr}&distance={distance}&API_KEY={API_KEY}'\n\n    # Initiate the request to the API.\n    res = requests.get(url)\n    # Extract the AQI level for our location.\n    aqiData = json.loads(res.content)[0]['AQI']\n\n    # This function can be passed the AQI observations \n    # and return the corresponding O3 level in that area for the day.\n    O3_data.append(aqi_to_concentration(aqiData, 'O3'))\n    CO_data.append(aqi_to_concentration(aqiData, 'CO'))\n    NO2_data.append(aqi_to_concentration(aqiData, 'NO2'))\n\npd.DataFrame({\"Date\": date_range_str, \"Lat\": lat, \"Lon\": lon, \"AirNow_O3\": O3_data, \"AirNow_CO\": CO_data, \"AirNow_NO2\": NO2_data}).to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv\", index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1667310643532,
  "history_end_time" : 1667310651970,
  "history_notes" : null,
  "history_process" : "di7md3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "c2tpyv0nlhl",
  "history_input" : "import json\nimport pandas as pd\nimport ee\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize()\n\n# identify a 500 meter buffer around our Point Of Interest (POI)\npoi = ee.Geometry.Point(32.047501, -110.773903).buffer(500)\n\n# Get TROPOMI NRTI Image Collection for GoogleEarth Engine\ntropomiCollection = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\").filterDate('2021-01-01','2021-01-31')\n\ndef poi_mean(img):\n    # This function will reduce all the points in the area we specified in \"poi\" and average all the data into a single daily value\n    mean = img.reduceRegion(reducer=ee.Reducer.mean(), \n    geometry=poi,scale=250).get('O3_column_number_density')\n    return img.set('date', img.date().format()).set('mean',mean)\n    \n# Map function to our ImageCollection\npoi_reduced_imgs = tropomiCollection.map(poi_mean)\nnested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date','mean']).values().get(0)\n\n# we need to call the callback method \"getInfo\" to retrieve the data\ndf = pd.DataFrame(nested_list.getInfo(), columns=['Date','tropomi_O3_mean'])\n\n# Convert Date column to DateTime\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.set_index('Date')\n# convert tropomi_O3_mean to ppbV (parts-per-billion-volume)\ndf['tropomi_O3.ppbV'] = (df['tropomi_O3_mean']/3)*224\n\ndf.drop('tropomi_O3_mean', inplace=True, axis=1)\n# Save data to CSV file\ndf.to_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv')\n",
  "history_output" : "",
  "history_begin_time" : 1667310653555,
  "history_end_time" : 1667310659140,
  "history_notes" : null,
  "history_process" : "lz2spd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3l1hkkd06ud",
  "history_input" : "# Read Airnow data extract from AirnowAPI and saved to system\nairnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\n\n# Scale data to match target scale\nairnow.AirNow_O3 = airnow.AirNow_O3 * 1000\nairnow.AirNow_CO = airnow.AirNow_CO * 10\n\nairnow[\"Date\"] = airnow[\"Date\"].dt.strftime('%Y%m%d')\n\n# Read Tropomi extracted from Google Earth Engine and saved to system\ntropomi = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/tropomi_O3.csv', parse_dates=[\"Date\"])\ntropomi[\"Date\"] = tropomi[\"Date\"].dt.strftime('%Y%m%d')\n\n\n# Read downloaded CMAQ data.\ncmaq = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/cmaq_2017_Jan.csv')\n# Keep rows that are close to our station location specified in previous steps\ncmaq = cmaq[(cmaq['latitude'] == 33.00259) & (cmaq['longitude'] == -110.75935)]\n# Drop unnecessary columns\ncmaq.drop([\"column\", \"row\", \"Lambert_X\", \"LAMBERT_Y\"], axis=1, inplace=True)\n# Update date value to match our existing data, since the publicly accessible CMAQ data doesn't go beyond 2017.\ncmaq['date'] = pd.date_range('2021-01-01','2021-01-31').strftime('%Y%m%d')\n\n\n# Merge all data frames together\nfinal = airnow.merge(tropomi, on=\"Date\").merge(cmaq, left_on=\"Date\", right_on=\"date\")\n# Drop any duplicated rows\nfinal = final.drop_duplicates(\"Date\")\n\nfinal.to_csv(\"/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"ch15_merge_data.py\", line 2, in <module>\n    airnow = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/airnow_data.csv', parse_dates=[\"Date\"])\nNameError: name 'pd' is not defined\n",
  "history_begin_time" : 1667310661017,
  "history_end_time" : 1667310661210,
  "history_notes" : null,
  "history_process" : "b98i0m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "agbd3k3ecpr",
  "history_input" : "import pandas as pd\n\nfinal = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv')\n\n# Features to keep as predictors\nX = final.drop(['Date', 'AirNow_O3', 'Lat', 'Lon', 'latitude', 'longitude', 'date', 'O3_MDA8', 'O3_AVG'],axis=1)\n\n# The target feature to predict.\ny = final['AirNOW_O3']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\n# make a prediction with a voting ensemble\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# define the base models\nmodels = list()\nmodels.append(('cart1', XGBRegressor(max_depth=1)))\nmodels.append(('cart2', XGBRegressor(max_depth=2)))\nmodels.append(('cart3', XGBRegressor(max_depth=3)))\nmodels.append(('cart4', XGBRegressor(max_depth=4)))\nmodels.append(('cart5', XGBRegressor(max_depth=5)))\nmodels.append(('cart6', XGBRegressor(max_depth=6)))\n\n# define the voting ensemble\nensemble = VotingRegressor(estimators=models)\n\n# fit the model on all available data\nensemble.fit(X_train, y_train)\n\n\nimport math\nimport sklearn\n\npred = ensemble.predict(X_test)\n\nmse = sklearn.metrics.mean_squared_error(pred, y_test)\nrmse = math.sqrt(mse)\n\nprint(mse, rmse)\n\n\n\n\n# Get the equivalent rows of our test set from the main data frame for plotting\ndataset = final.iloc[21:].drop_duplicates(\"Date\")\n# Add the prediction result to the new dataframe\ndataset['prediction'] = pred.tolist()\n    \nimport matplotlib.pyplot as plt\nplt.rc('font', size=12)\nfig, ax = plt.subplots(figsize=(20, 13))\n\n# Specify how our lines should look\nax.plot(dataset.Date, pred.tolist(), color='tab:orange', label='prediction')\n# Use linestyle keyword to style our plot\nax.plot(dataset.Date, dataset.AirNow_O3, color='green', linestyle='--',\n        label='observed')\n\nax.plot(dataset.Date, dataset.O3_AVG, color='blue', linestyle='--',\n        label='CMAQ_Oâ‚ƒ')\n# Redisplay the legend to show our new wind gust line\nax.legend(loc='upper left')\n# Same as above\nax.set_xlabel('Time')\nax.set_ylabel('Values')\nax.set_title('Compare Observed, Prediction, CMAQ Simulation')\nax.grid(True)\nax.legend(loc='upper left');\n",
  "history_output" : "Traceback (most recent call last):\n  File \"ch15_train_xgboost.py\", line 3, in <module>\n    final = pd.read_csv('/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv')\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 317, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\", line 1729, in _make_engine\n    self.handles = get_handle(\n  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\", line 857, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/uhhmed/Desktop/CMAQ_Ch15_bookCode/final.csv'\n",
  "history_begin_time" : 1667310663165,
  "history_end_time" : 1667310664289,
  "history_notes" : null,
  "history_process" : "rm59xe",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
