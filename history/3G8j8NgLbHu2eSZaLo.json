[{
  "history_id" : "ez3y7oclgwa",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Poocessing the data - shifting columns of NO2\n\n# Checking required packages are installed or not\n\nimport sys\nimport subprocess\n#import pkg_resources\n\n# Required packages to run this process.\n#required = {'pandas','pathlib'}\n#installed = {pkg.key for pkg in pkg_resources.working_set}\n#missing = required - installed\n\n#if missing:\n #   print(\"Packages missing and will be installed: \", missing)\n #   python = sys.executable\n #   subprocess.check_call(\n    #    [python, '-m', 'pip3', 'install', *missing], stdout=subprocess.DEVNULL)\n\n################################\n#  END OF PACKAGES VALIDATION  #\n################################\n\n## importing necessary libraries\n\nimport pandas as pd\nimport datetime\nimport os\n\n# defining function for shifting data\nmonth_i=[1,2,3,4,5,6,7,8,9,10,11,12]\ndef shift(file):\n    station=file['Station.ID'].unique()\n    file['date']=pd.to_datetime(file[[\"year\", \"month\", \"day\",\"hours\"]]) # creating date\n    file['dayofyear'] = pd.to_datetime(file['date']).dt.dayofyear # converting monthly days to yearly dasy\n    dfs = dict(tuple(file.groupby('Station.ID'))) # grouping the data by station\n#    print(dfs)\n    list_final=[]\n    for site in station:\n        list1=dfs[site]  # selecting dataset for each station\n        o3_max=list1.loc[list1.groupby(\"dayofyear\")[\"AirNOW_O3\"].idxmax()] # daily max values\n        o3_month=pd.DataFrame(o3_max.groupby('month',as_index=False)['hours'].mean()) # monthly average hour\n        no2_max=list1.loc[list1.groupby(\"dayofyear\")[\"CMAQ12KM_NO2\"].idxmax()] # daily amx values\n        no2_month=pd.DataFrame(no2_max.groupby('month',as_index=False)['hours'].mean()) # montly average hour\n        merge_hour=pd.concat([o3_month,no2_month],axis = 1) # merging two table\n        merge_hour.columns = ['month1','hours1', 'month2','hours2'] # renaming column name\n        merge_hour['hours_diff']=merge_hour['hours1']-merge_hour['hours2']\n        merge_hour.hours_diff = merge_hour.hours_diff.astype(int) # converting hours_diff to int\n        months = dict(tuple(list1.groupby('month')))   # grouping the data by month\n        diff=merge_hour['hours_diff'] # extracting hours difference field\n        mon=merge_hour['month1'] # extracting month field\n\n        for (m,n) in zip(mon,diff):\n            list2= months[m] # selecting dataset for each month and for each station\n            list3=list2.loc[list2['month'] == m] # subsetting dataset for each month\n            list3['CMAQ12KM_NO2_new'] = list3['CMAQ12KM_NO2'].shift(n) # shifting rows for each month\n            list_final.append(list3)\n    return list_final\n\n  \n# Importing and merging 2020 and 2021 dataset\n\ndf1 = pd.read_csv('/home/mislam25/cmaq/full_20_lstm.csv')\ndf2 = pd.read_csv('/home/mislam25/cmaq/full_21_lstm.csv')\n\n#merging two dataframe vertically\nmrg=df1.append(df2, ignore_index=True)\ndf3_rs=pd.read_csv('/home/mislam25/cmaq/merged_rs.csv')\n\nfinal=pd.merge(mrg,df3_rs, on=['year', 'month','day','hours','Station.ID'])\n\n#shifting CMAQ NO2\nshift_df=shift(final)\ndata_frame = pd.concat(shift_df) # concatening the list\n\n# droping no data from all column and AirNOW_O3\ndata_new1 = data_frame.drop(['Unnamed: 0'],axis=1)\ndata_new=data_new1.dropna() \nfinal_df = data_new[data_new.AirNOW_O3!= -999]\n\n# saving the file into local drive\nfinal_df.to_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\n",
  "history_begin_time" : 1644718764896,
  "history_end_time" : 1644718908239,
  "history_notes" : null,
  "history_process" : "im9rct",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "fxaym9r3a0f",
  "history_input" : "# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.ensemble import VotingRegressor\nfrom matplotlib import pyplot\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\n\n# importing data\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n\n# defining training variables\ntrain=final.loc[final['year']==2020]\n\n# Processing training  data\nX = train.drop(['AirNOW_O3','Station.ID','YYYYMMDDHH','year','date','dayofyear','datetime'],axis=1)\ny = train['AirNOW_O3']\n\n# Defining voting-ensemble based xgboost model\nmodels = list()\nmodels.append(('cart1', XGBRegressor(max_depth=1)))\nmodels.append(('cart2', XGBRegressor(max_depth=2)))\nmodels.append(('cart3', XGBRegressor(max_depth=3)))\nmodels.append(('cart4', XGBRegressor(max_depth=4)))\nmodels.append(('cart5', XGBRegressor(max_depth=5)))\nmodels.append(('cart6', XGBRegressor(max_depth=6)))\n# define the voting ensemble\nensemble = VotingRegressor(estimators=models)\n\n\n# fit the model on all available data\nensemble.fit(X, y)\n\n# save the model to disk\nfilename = '/home/mislam25/cmaq/xgboost.sav'\npickle.dump(ensemble, open(filename, 'wb'))\n",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\nbash: line 0: cd: ..: No such file or directory\nrm: cannot remove 'vcGg1k0WrHcun08mWb4mDo1CJe*': Stale file handle\n",
  "history_begin_time" : 1644718908892,
  "history_end_time" : 1644719296977,
  "history_notes" : null,
  "history_process" : "106i1i",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "uhmov8adrnd",
  "history_input" : "# NASA Geoweaver\n# CMAQ-AI model: Prediction by Voting-XGBoost\n\n# Importing necessary libraries\nimport pandas as pd\nimport pickle\n# importing data\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n\n# defining  testing variables\ntest=final.loc[final['year']==2021]\n\n# processing test data\ntest_X = test.drop(['AirNOW_O3','Station.ID','YYYYMMDDHH','year','date','dayofyear','datetime'],axis=1)\ntest_y = test['AirNOW_O3']\ntest_X.head()\n\n# load the model from disk\nfilename = '/home/mislam25/cmaq/xgboost.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\n# making prediction\npred = loaded_model.predict(test_X)\n\n# adding prediction values to test dataset\ntest['prediction'] = pred.tolist()\n\n# saving the dataset into local drive\ntest.to_csv('/home/mislam25/cmaq/prediction_xgboost.csv')",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\n/home/mislam25/vcGg1k0WrHcun08mWb4mDo1CJe/prediction_xgboost.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['prediction'] = pred.tolist()\n",
  "history_begin_time" : 1644719297286,
  "history_end_time" : 1644719625599,
  "history_notes" : null,
  "history_process" : "mx09zi",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "2wgvjsxpzm7",
  "history_input" : null,
  "history_output" : "C:\\Users\\didar\\gw-workspace\\temp\\vcGg1k0WrHcun08mWb4mDo1CJe.tar is not a regular file or directory",
  "history_begin_time" : 1644718908968,
  "history_end_time" : 1644718957223,
  "history_notes" : null,
  "history_process" : "q2io1w",
  "host_id" : "w6jk5o",
  "indicator" : "Failed"
},{
  "history_id" : "k4fpp09sxxo",
  "history_input" : "# NASA Geoweaver\n# CMAQ-AI Model: Autokeras for Automated-Deep Learning\n\n#\n#import subprocess\n#subprocess.run([\"salloc\", \"-p\", \"gpuq\", \"-q\", \"gpu\", \"--ntasks-per-node=1\", \"--gres=gpu:A100.40gb:1\", \"-t\", \"0-00:12:00\" ])\n\n# Importing necessary libraries\nimport pandas as pd\nfrom numpy import asarray\nfrom sklearn.model_selection import train_test_split\nfrom autokeras import StructuredDataRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.models import model_from_json\nimport numpy\nimport os\nimport tensorflow as tf\n\n# load dataset\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n\n# defining training variables\ntrain=final.loc[final['year']==2020]\n\n# processing training  data\nX = train.drop(['AirNOW_O3','Station.ID','YYYYMMDDHH','year','date','dayofyear','datetime'],axis=1)\ny = train['AirNOW_O3']\nX.head()\nX=X.values.astype('float32')\ny=y.values.astype('float32')\n\n\n\n# load dataset\n# separate into train and test sets\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n# define the search\nsearch = StructuredDataRegressor(max_trials=15, loss='mean_absolute_error')\n# perform the search\n#search.fit(x=X_train, y=y_train, verbose=0)\nsearch.fit(x=X, y=y, verbose=0)\n# evaluate the model\n#mae, _ = search.evaluate(X_test, y_test, verbose=0)\n#print('MAE: %.3f' % mae)\n# use the model to make a prediction\n#X_new = asarray([[108]]).astype('float32')\n#yhat = search.predict(X_new)\n#print('Predicted: %.3f' % yhat[0])\n# get the best performing model\nmodel = search.export_model()\n# summarize the loaded model\n#model.summary()\n# save the best performing model to file\nmodel.save('/home/mislam25/cmaq/autokeras',save_format=\"tf\")",
  "history_output" : "mkdir: cannot create directory ‘vcGg1k0WrHcun08mWb4mDo1CJe’: File exists\npre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\n2022-02-12 21:28:24.490983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/hwloc/2.1.0/lib:/opt/ohpc/pub/mpi/libfabric/1.10.1/lib:/opt/ohpc/pub/mpi/ucx-ohpc/1.8.0/lib:/opt/ohpc/pub/mpi/openmpi4-gnu9/4.0.4/lib:/opt/ohpc/pub/compiler/gcc/9.3.0/lib64:/lib64\n2022-02-12 21:28:24.491010: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nTraceback (most recent call last):\n  File \"/home/mislam25/vcGg1k0WrHcun08mWb4mDo1CJe/training_autokeras.py\", line 40, in <module>\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/tasks/structured_data.py\", line 396, in __init__\n    super().__init__(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/tasks/structured_data.py\", line 195, in __init__\n    super().__init__(inputs=inputs, outputs=outputs, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/tasks/structured_data.py\", line 37, in __init__\n    super().__init__(inputs=inputs, outputs=outputs, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/auto_model.py\", line 142, in __init__\n    self.tuner = tuner(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/tuners/task_specific.py\", line 172, in __init__\n    super().__init__(initial_hps=STRUCTURED_DATA_REGRESSOR, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/tuners/greedy.py\", line 231, in __init__\n    super().__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/autokeras/engine/tuner.py\", line 52, in __init__\n    super().__init__(oracle, hypermodel, **kwargs)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 111, in __init__\n    super(Tuner, self).__init__(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 82, in __init__\n    self.oracle._set_project_dir(\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/oracle.py\", line 347, in _set_project_dir\n    if not overwrite and tf.io.gfile.exists(self._get_oracle_fname()):\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/oracle.py\", line 387, in _get_oracle_fname\n    return os.path.join(self._project_dir, \"oracle.json\")\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/engine/oracle.py\", line 358, in _project_dir\n    utils.create_directory(dirname)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/keras_tuner/utils.py\", line 48, in create_directory\n    tf.io.gfile.makedirs(path)\n  File \"/home/mislam25/anaconda3/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py\", line 511, in recursive_create_dir_v2\n    _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))\ntensorflow.python.framework.errors_impl.AbortedError: .; Stale file handle\nbash: line 0: cd: ..: No such file or directory\nrm: cannot remove 'vcGg1k0WrHcun08mWb4mDo1CJe*': Stale file handle\n",
  "history_begin_time" : 1644718957611,
  "history_end_time" : 1644719798788,
  "history_notes" : null,
  "history_process" : "5o4fxj",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "7s4loc20lfu",
  "history_input" : "## NASA GEOWEAVER##\n# CMAQ-AI Model: Autokeras prediction\n\n###########################\n# Chck required packages\n# If required packages are present, install it\n##########################\n\n\n# Importing necessariy libraries\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import load_model\nimport autokeras as ak\nimport subprocess\n\n# create HOPPER job salloc command\n#subprocess.run([\"salloc\", \"-p\", \"gpuq\", \"-q\", \"gpu\", \"--ntasks-per-node=1\", \"--gres=gpu:A100.40gb:1\", \"-t\", \"0-00:10:00\" ])\n\n# Import processed data\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n\n# defining and processing testing variables\ntest=final.loc[final['year']==2021]\ntest_X = test.drop(['AirNOW_O3','Station.ID','YYYYMMDDHH','year','date','dayofyear','datetime'],axis=1)\ntest_y = test['AirNOW_O3']\ntext_X=test_X.values.astype('float32')\n\n# load the model from disk\nloaded_model =load_model('/home/mislam25/cmaq/autokeras',custom_objects=ak.CUSTOM_OBJECTS)\n\n# Make prediction\npred = loaded_model.predict(test_X)\n# adding prediction values to test dataset and save the result\ntest['prediction'] = pred.tolist()\ntest.to_csv('/home/mislam25/cmaq/autokeras.csv')",
  "history_output" : "pre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\n2022-02-12 21:38:48.207470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/hwloc/2.1.0/lib:/opt/ohpc/pub/mpi/libfabric/1.10.1/lib:/opt/ohpc/pub/mpi/ucx-ohpc/1.8.0/lib:/opt/ohpc/pub/mpi/openmpi4-gnu9/4.0.4/lib:/opt/ohpc/pub/compiler/gcc/9.3.0/lib64:/lib64\n2022-02-12 21:38:48.207494: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-02-12 21:47:50.051585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/libs/hwloc/2.1.0/lib:/opt/ohpc/pub/mpi/libfabric/1.10.1/lib:/opt/ohpc/pub/mpi/ucx-ohpc/1.8.0/lib:/opt/ohpc/pub/mpi/openmpi4-gnu9/4.0.4/lib:/opt/ohpc/pub/compiler/gcc/9.3.0/lib64:/lib64\n2022-02-12 21:47:50.051617: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n2022-02-12 21:47:50.051632: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hopper2.orc.gmu.edu): /proc/driver/nvidia/version does not exist\n2022-02-12 21:47:50.051830: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/home/mislam25/vcGg1k0WrHcun08mWb4mDo1CJe/prediction_autokeras.py:35: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test['prediction'] = pred.tolist()\n",
  "history_begin_time" : 1644719799063,
  "history_end_time" : 1644720477205,
  "history_notes" : null,
  "history_process" : "2gxied",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
},{
  "history_id" : "ijs7kgdao3a",
  "history_input" : "# NASA Geoweaver\n# CMAQ-AI Model: Prediction by random forest\n\n# import necessary libraries\nimport pandas as pd\nimport pickle\n# import data\nfinal=pd.read_csv('/home/mislam25/cmaq/merged_2020_2021.csv')\n\n# defining  and processing test variables\ntest=final.loc[final['year']==2021]\n\ntest_X = test.drop(['AirNOW_O3','Station.ID','YYYYMMDDHH','year','date','dayofyear','datetime'],axis=1)\ntest_y = test['AirNOW_O3']\n\n# load the model from disk\nfilename = '/home/mislam25/cmaq/rf.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\n# making prediction\npred = loaded_model.predict(test_X)\n\n# adding prediction values to test dataset\ntest['prediction'] = pred.tolist()\ntest.to_csv('/home/mislam25/cmaq/prediction_rf.csv')",
  "history_output" : "mkdir: cannot create directory ‘vcGg1k0WrHcun08mWb4mDo1CJe’: File exists\npre_processing.py\ntraining_xgboost.py\ntraining_automl.py\nprediction_xgboost.py\nprediction_automl.py\ntraining_random_forest.py\nprediction_random_forest.py\ntraining_autokeras.py\nprediction_autokeras.py\ntest.py\nTraceback (most recent call last):\n  File \"/home/mislam25/vcGg1k0WrHcun08mWb4mDo1CJe/prediction_random_forest.py\", line 18, in <module>\n    loaded_model = pickle.load(open(filename, 'rb'))\nFileNotFoundError: [Errno 2] No such file or directory: '/home/mislam25/cmaq/rf.sav'\n",
  "history_begin_time" : 1644718957312,
  "history_end_time" : 1644719133348,
  "history_notes" : null,
  "history_process" : "oyz4io",
  "host_id" : "w6jk5o",
  "indicator" : "Done"
}]
