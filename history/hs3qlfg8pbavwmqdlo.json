[{
  "history_id" : "321oprwoni0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607458,
  "history_end_time" : 1704702607458,
  "history_notes" : null,
  "history_process" : "6up921",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "eogqg0pps27",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607487,
  "history_end_time" : 1704702607487,
  "history_notes" : null,
  "history_process" : "xpdg66",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lf5qlk8f948",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607490,
  "history_end_time" : 1704702607490,
  "history_notes" : null,
  "history_process" : "xlayd5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srgxpbuwqea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607492,
  "history_end_time" : 1704702607492,
  "history_notes" : null,
  "history_process" : "argv3i",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nh0gxndfd2j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607493,
  "history_end_time" : 1704702607493,
  "history_notes" : null,
  "history_process" : "l8vlic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cul7f3yib2k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607495,
  "history_end_time" : 1704702607495,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2abi2w0qa5n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607497,
  "history_end_time" : 1704702607497,
  "history_notes" : null,
  "history_process" : "3asyzj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "homj42q8rci",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607498,
  "history_end_time" : 1704702607498,
  "history_notes" : null,
  "history_process" : "9xdvh6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d8xipfifjrp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607499,
  "history_end_time" : 1704702607499,
  "history_notes" : null,
  "history_process" : "ex3vh9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wcpusutepm8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607500,
  "history_end_time" : 1704702607500,
  "history_notes" : null,
  "history_process" : "b8uv5z",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o6s1e254dtd",
  "history_input" : "#!/bin/bash\n\n\n# generate images and gif from the prediction NetCDF files and overlay the AirNow station observation on the top\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\nmkdir $cmaq_folder\"/plots\"\npermanent_location=/groups/ESS3/zsun/cmaq/ai_results/\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport graph_dir=\"/groups/ESS/zsun/cmaq/plots\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n\nsource /home/zsun/.bashrc\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\ncat <<EOF >>$cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\") \nd1 = getenv(\"stdate_post\") \nd2 = getenv(\"eddate_post\") \n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\nobs_dir = getenv(\"obs_dir_NCL\")\nplot_dir = getenv(\"graph_dir\") \n\nhr=new(24,\"string\")\nhr=(/\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"/)\n\nprint(plot_dir)\naconc_dir = getenv(\"postdata_dir\") \ngrid_dir = getenv(\"mcip_dir\")\n\nprint(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\n\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\ncdf_file2= addfile(grid_dir+\"/METCRO2D_\"+date+\".nc\",\"r\")\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\nwspd10=cdf_file2->WSPD10(:,0,:,:)\nwdir10=cdf_file2->WDIR10(:,0,:,:)\n\ntemp = cdf_file2->TEMP2\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(max(temp))\nprint(min(temp))\nprint(avg(temp))\n\n\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\no3@unit = \"ppbv\"\n\nUV10=wind_component(wspd10,wdir10,0)\nUV10@lat2d = lat\nUV10@lon2d = lon\n\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; dont advance frame\nres@gsnDraw = False\nres@gsnLeftString  = \"\"\nres@gsnRightString = \"\"\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n;res@tiMainFontHeightF = 0.02\n;res@vpWidthF        = 0.7\n;res@vpHeightF       = 0.7\n\n;;set map;;\nmpres                             = res\nmpres@mpLimitMode = \"LatLon\"\nmpres@mpDataSetName               = \"Earth..4\"\nmpres@mpDataBaseVersion           = \"MediumRes\"\nmpres@mpOutlineOn                 = True\nmpres@mpGeophysicalLineThicknessF = 1.5\nmpres@mpFillDrawOrder             = \"PostDraw\"\nmpres@mpFillOn                    = False\nmpres@mpAreaMaskingOn         = True\nmpres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nmpres@mpOutlineSpecifiers         = \"United States:States\"\nmpres@mpProjection           = \"LambertConformal\"\nmpres@mpLambertParallel1F    = 33.\nmpres@mpLambertParallel2F    = 45.\nmpres@mpLambertMeridianF     = -98.\nmpres@mpMinLonF = -120 ;min(lon)+0.2\nmpres@mpMaxLonF = -70 ;max(lon)-0.2\nmpres@mpMinLatF = 25 ;min(lat)+0.05\nmpres@mpMaxLatF = 50 ;max(lat)-0.05\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@mpLandFillColor         = \"white\"\nmpres@mpInlandWaterFillColor  = \"white\"\nmpres@mpOceanFillColor        = \"white\"\nmpres@mpGeophysicalLineColor    = \"Black\"\n\n;mpres@lbLabelAutoStride       = True\nmpres@tiXAxisFont             = 25\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@tmXBLabelFont           = 25\nmpres@tmXBLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmYLLabelFont           = 25\nmpres@tmYLLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmXTLabelsOn            = False\nmpres@tmXTLabelFont           = 25\nmpres@tmXTLabelFontHeightF    = 0.013\nmpres@tmYRLabelsOn            = False\nmpres@tmYRLabelFont           = 25\nmpres@tmYRLabelFontHeightF    = 0.013\n\n;;set contour;;\ncnres                         = res\ncnres@cnFillDrawOrder         = \"PreDraw\"\ncnres@cnFillOn                = True\ncnres@cnLinesOn               = False\ncnres@cnLineLabelsOn          = False\ncnres@lbLabelFont             = 25\ncnres@lbLabelFontHeightF      = 0.013\ncnres@tiXAxisFont             = 25\ncnres@pmLabelBarWidthF        = 0.5\ncnres@pmLabelBarHeightF       = 0.1\n;cnres@pmLabelBarOrthogonalPosF = -0.02\ncnres@lbLabelAutoStride       = True\n\n;set vector;;\nres_vc                        = res\nres_vc@vcGlyphStyle           = \"LineArrow\"\nres_vc@vcLineArrowThicknessF  = 3\nres_vc@vcMinDistanceF         = 0.03\nres_vc@vcRefLengthF           = 0.03\nres_vc@vcRefAnnoOn            = True\nres_vc@vcRefMagnitudeF           = 16\nres_vc@vcRefAnnoString1          = \"16m/s\"\nres_vc@vcRefAnnoSide             = \"Top\"\nres_vc@vcRefAnnoString2On        = False\nres_vc@vcRefAnnoPerimOn          = False\nres_vc@vcRefAnnoOrthogonalPosF   = -0.02\nres_vc@vcRefAnnoParallelPosF     = 0.999\n;res_vc@vcRefAnnoBackgroundColor = \"White\"\nres_vc@vcVectorDrawOrder         = \"PostDraw\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  ;print(time(it,0)+\" \"+time(it,1))\n  rundate = yyyyddd_to_yyyymmdd( time(it,0) )\n  runtime = hr( tointeger(time(it,1)/10000) )\n\n  site = readAsciiTable(obs_dir+\"/AQF5X_Hourly_\"+rundate+runtime+\".dat\",1,\"string\",1)\n  nrows = dimsizes(site)\n  sitename = str_get_field(site,1,\",\")\n  sitelat = stringtofloat(str_get_field(site,2,\",\"))\n  sitelon = stringtofloat(str_get_field(site,3,\",\"))\n  O3_obs = stringtofloat(str_get_field(site,4,\",\"))\n\n  obslon = sitelon(:,0)\n  obslat = sitelat(:,0)\n  obsO3 = O3_obs(:,0)\n\n  npts = nrows(0)\n\n  obsO3@_FillValue = -999.\n\n;--- levels for dividing\n  levels_O3  = ispan(0,80,4)\n\n  nlevels = dimsizes(levels_O3)\n\n  colors  = span_color_rgba(\"WhiteBlueGreenYellowRed\",nlevels+1)\n\n  num_distinct_markers = nlevels+1        ; number of distinct markers\n  lat_O3 = new((/num_distinct_markers,npts/),float)\n  lon_O3 = new((/num_distinct_markers,npts/),float)\n  lat_O3 = -999\n  lon_O3 = -999\n\n\n;\n; Group the points according to which range they fall in. At the\n; same time, create the label that we will use later in the labelbar\n;\n  do i = 0, num_distinct_markers-1\n    if (i.eq.0) then\n      indexes_O3 = ind(obsO3(:).lt.levels_O3(0))\n    end if\n    if (i.eq.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.max(levels_O3))\n    end if\n    if (i.gt.0.and.i.lt.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.levels_O3(i-1).and.obsO3(:).lt.levels_O3(i))\n    end if\n\n;\n; Now that we have the set of indexes whose values fall within\n; the given range, take the corresponding lat/lon values and store\n; them, so later we can color this set of markers with the appropriate\n; color.\n;\n    if (.not.any(ismissing(indexes_O3))) then\n      npts_range_O3 = dimsizes(indexes_O3)   ; # of points in this range.\n\n      lat_O3(i,0:npts_range_O3-1) = obslat(indexes_O3)\n      lon_O3(i,0:npts_range_O3-1) = obslon(indexes_O3)\n  ;print(\"O3: \"+npts_range_O3)\n    end if\n\n\n    delete(indexes_O3)            ; Necessary b/c \"indexes\" may be a different\n  end do\n\n  lat_O3@_FillValue = -999\n  lon_O3@_FillValue = -999\n\n  gsres               = True\n  gsres@gsMarkerIndex = 16          ; Use filled dots for markers.\n\n  hollowres           = True\n  hollowres@gsMarkerIndex    = 4\n  hollowres@gsMarkerColor    = \"black\"\n  hollowres@gsMarkerSizeF    = 0.008\n\n;;;;;;;;;   Plot Ozone\n  pname=plot_dir+\"/OBS-FORECAST_O3_\"+rundate+runtime\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  pmid_O3 = new(num_distinct_markers,graphic)\n  hollow_O3 = new(num_distinct_markers,graphic)\n\n  cnres@tiMainString =  pdate+\" \"+runtime+\" UTC O~B~3~N~ (ppbV)\"\n  cnres@cnLevelSelectionMode = \"ManualLevels\"\n  cnres@cnMinLevelValF          = 0.\n  cnres@cnMaxLevelValF          = 80\n  cnres@cnLevelSpacingF         = 4\n\n  ;plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  map = gsn_csm_map(wks,mpres)\n  contour = gsn_csm_contour(wks,o3(it,:,:),cnres)\n  vector  = gsn_csm_vector(wks,UV10(0,it,:,:),UV10(1,it,:,:),res_vc)\n  overlay(map,contour)\n  overlay(map,vector)\n\n  pmid = new(num_distinct_markers,graphic)\n  hollow = new(num_distinct_markers,graphic)\n  do i = 0, num_distinct_markers-1\n    if (.not.ismissing(lat_O3(i,0)))\n      gsres@gsMarkerColor      = colors(i,:)\n      gsres@gsMarkerSizeF      = 0.008\n      gsres@gsMarkerThicknessF = 1\n       pmid(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),gsres)\n       hollow(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),hollowres)\n    end if\n  end do\n\n  draw(map)\n  frame(wks)\n  delete(wks)\n  delete(pmid_O3)\n  delete(hollow_O3)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\n\n\n  delete(pmid)\n  delete(hollow)\n  delete(site)\n  delete(sitename)\n  delete(sitelat)\n  delete(sitelon)\n  delete(O3_obs)\n  delete(obslon)\n  delete(obslat)\n  delete(obsO3)\n  delete([/lon_O3,lat_O3/])\n\nend do\ndelete(res)\n\n;/\n\nend\nprint(\"ncl business is done\")\nexit\nEOF\n\ndays_back=7\n# for regular workflow run, assign false\n# for refreshing all the previous generated gifs, assign true\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') \n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d')\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') \n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d')\n  \n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/\"Airnow_\"$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\n\n  convert -delay 100 $cmaq_folder/plots/OBS*.png $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif\n\n  # copy the result files to permanent location\n  echo \"Copy Airnow gif to permanent location\"\n  cp $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n\ndone\n\nif [ $? -eq 0 ]; then\n    echo \"Generating AirNow images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nelse\n    echo \"Generating AirNow images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nfi\n\n\n\n",
  "history_output" : "mkdir: cannot create directory ‘/groups/ESS/zsun/cmaq/plots’: File exists\nFATAL: Missing required source file.\n  (1) Install package 'bash-completion'\n  (2) $ source /usr/share/bash-completion/bash_completion\n  (3) $ source slurm_completion.sh\n1 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106_ML_extracted.nc doesn't exist. Skipping...\n2 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105_ML_extracted.nc doesn't exist. Skipping...\n3 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Airnow_20240104.gif doesn't exist. Generating...\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\nVariable: plot_dir\nType: string\nTotal Size: 8 bytes\n            1 values\nNumber of Dimensions: 1\nDimensions and sizes:\t[1]\nCoordinates: \n(0)\t/groups/ESS/zsun/cmaq/plots\n(0)\t/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc\n(0)\t300.4968\n(0)\t250.153\n(0)\t277.5186\n(0)\t24 265 442\n(0)\t55.16\n(0)\t0.93\n(0)\t29.66022\nfatal:asciiread: Unable to open input file (/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/AQF5X_Hourly_2024010412.dat)\nfatal:[\"Execute.c\":8637]:Execute: Error occurred at or near line 9684 in file /opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\nfatal:[\"Execute.c\":8637]:Execute: Error occurred at or near line 9766 in file /opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\nfatal:[\"Execute.c\":8637]:Execute: Error occurred at or near line 164 in file /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3_Airnow.ncl\n(0)\tncl business is done\nconvert: unable to open image `/groups/ESS/zsun/cmaq/plots/OBS*.png': No such file or directory @ error/blob.c/OpenBlob/2881.\nconvert: no images defined `/groups/ESS/zsun/cmaq/plots/Airnow_20240104.gif' @ error/convert.c/ConvertImageCommand/3226.\nCopy Airnow gif to permanent location\ncp: cannot stat '/groups/ESS/zsun/cmaq/plots/Airnow_20240104.gif': No such file or directory\n4 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240103_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Airnow_20240103.gif exists. Skipping...\n5 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240102_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Airnow_20240102.gif exists. Skipping...\n6 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240101_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Airnow_20240101.gif exists. Skipping...\n7 days ago\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231231_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Airnow_20231231.gif exists. Skipping...\nGenerating AirNow images/gif Completed Successfully\nRemoving ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\n",
  "history_begin_time" : 1704702855236,
  "history_end_time" : 1704702858568,
  "history_notes" : null,
  "history_process" : "is1w3m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2fo2bw1rhcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607524,
  "history_end_time" : 1704702607524,
  "history_notes" : null,
  "history_process" : "h76ld0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rdig0r23ixq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607527,
  "history_end_time" : 1704702607527,
  "history_notes" : null,
  "history_process" : "s6hbic",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "agbiw04wdf9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607529,
  "history_end_time" : 1704702607529,
  "history_notes" : null,
  "history_process" : "pvzabv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uuigwlyt6ey",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607530,
  "history_end_time" : 1704702607530,
  "history_notes" : null,
  "history_process" : "8i9ptn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lu0mx0l6ryu",
  "history_input" : "#!/bin/bash\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\n\n# generate images and gif from the NetCDF files\n\necho \"cmaq_folder=\"${cmaq_folder}\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\nmkdir \"${cmaq_folder}/plots\"\n\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport dir_graph=\"${cmaq_folder}/plots\"\n\necho \"Loading NCL\"\nsource /home/zsun/.bashrc\nmodule load ncl\necho \"Loaded NCL\"\n\nrm ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\n\necho \"Drafting \"${cmaq_folder}/geoweaver_plot_daily_O3.ncl\ncat <<EOF >> ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\nprint(\"NCL script successfully begin: \")\n\ndate = getenv(\"YYYYMMDD_POST\")\nd1 = getenv(\"stdate_post\")\nd2 = getenv(\"eddate_post\")\n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\n;print(\"Passed Date: \"+date)\n\n;aconc_dir = getenv(\"postdata_dir\")\ngrid_dir = getenv(\"mcip_dir\")\nplot_dir = getenv(\"dir_graph\")\n\nprint(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\ncdf_file1 = addfile(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\n\nptime = (/\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"/)\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\n;pm25 = cdf_file1->PM25_TOT(:,0,:,:)\n\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\n;print(max(pm25))\n;print(min(pm25))\n;print(avg(pm25))\n\n;print(time)\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; don't advance frame\nres@gsnDraw = False\n;res@gsnSpreadColors = True\nres@lbLabelAutoStride = True\n;res@lbBoxLinesOn = False\nres@pmLabelBarHeightF = 0.1\nres@pmLabelBarWidthF = 0.5\nres@cnFillOn=True\n;res@cnMonoFillPattern=True\n;res@cnMonoLineColor=True\nres@cnLinesOn=False\n;res@pmLabelBarDisplayMode=\"never\"\nres@gsnLeftString  = \"\";\nres@gsnRightString = \"\"\n\nres@mpLimitMode = \"LatLon\"\nres@mpMinLonF = -120 ;min(lon)+0.2\nres@mpMaxLonF = -70 ;max(lon)-0.2\nres@mpMinLatF = 25 ;min(lat)+0.05\nres@mpMaxLatF = 50 ;max(lat)-0.05\nres@mpDataBaseVersion = \"MediumRes\"\n;res@tiMainString = times(it)\nres@mpDataBaseVersion       = \"MediumRes\"\nres@mpDataSetName           = \"Earth..4\"\nres@mpAreaMaskingOn         = True\nres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nres@mpOutlineSpecifiers=\"United States : States\"\nres@mpLandFillColor         = \"white\"\nres@mpInlandWaterFillColor  = \"white\"\nres@mpOceanFillColor        = \"white\"\nres@mpGeophysicalLineColor    = \"Black\"\nres@mpGeophysicalLineThicknessF = 1.5\n\n;res@gsnSpreadColors         = True\nres@lbLabelAutoStride       = True\nres@lbLabelFont             = 25\nres@tiXAxisFont             = 25\nres@pmTickMarkDisplayMode   = \"Always\"\nres@tmXBLabelFont           = 25\nres@tmXBLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmYLLabelFont           = 25\nres@tmYLLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmXTLabelsOn            = False\nres@tmXTLabelFont           = 25\nres@tmXTLabelFontHeightF    = 0.013\nres@tmYRLabelsOn            = False\nres@tmYRLabelFont           = 25\nres@tmYRLabelFontHeightF    = 0.013\n\n\nres@mpProjection           = \"LambertConformal\" ;\"CylindricalEquidistant\"\nres@mpLambertParallel1F    = 33.\nres@mpLambertParallel2F    = 45.\nres@mpLambertMeridianF     = -98.\n\nres@cnLevelSelectionMode = \"ManualLevels\"\nres@cnMinLevelValF          = 0.\nres@cnMaxLevelValF          = 80\nres@cnLevelSpacingF         = 4\n\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  pname=plot_dir+\"/testPlot_\"+pdate+\"_\"+ptime(it)\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  res@tiMainString = pdate+\" \"+ptime(it)+\" UTC O~B~3~N~ Forecast (ppbV)\"\n  plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  draw(plot)\n  frame(wks)\n  delete(wks)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\nend do\ndelete(res)\n\nend\nexit\nEOF\n\necho \"Start to run the NCL script: \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho \"ncl \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho $(date -d '1 day ago' '+%Y%m%d')\n\ndays_back=7\n\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date `date -d \"-2 day ${1}\" +%Y%m%d`\n  #export YYYYMMDD_POST='20220806'\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export stdate_post='2022-08-06'\n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export eddate_post='2022-08-08'\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export stdate_file='20220806'\n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export eddate_file='20220808'\n  stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d')\n  echo \"stdate_file=\"$stdate_file\n  # determine if the prediction netcdf is there\n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/Map_$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything in the folder first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3.ncl\n\n  echo \"Finished \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\n  # convert -delay 100 *.png 20220613_20220614.gif\n  convert -delay 100 $cmaq_folder/plots/testPlot*.png $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif\n  echo \"Converted images to gif\"\n\n  # cp the results to permanent location\n  cp $predict_nc_file $permanent_location/netcdfs/\n  cp $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n  echo \"Moved the generated netcdfs and gifs to permanent locations\"\n  \ndone\n\n\n\nif [ $? -eq 0 ]; then\n    echo \"Generating images/gif Completed Successfully\"\nelse\n    echo \"Generating images/gif Failed!\"\nfi\n\n",
  "history_output" : "cmaq_folder=/groups/ESS/zsun/cmaq\nmkdir: cannot create directory ‘/groups/ESS/zsun/cmaq/plots’: File exists\nLoading NCL\nFATAL: Missing required source file.\n  (1) Install package 'bash-completion'\n  (2) $ source /usr/share/bash-completion/bash_completion\n  (3) $ source slurm_completion.sh\nLoaded NCL\nDrafting /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\nStart to run the NCL script: /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\nncl /groups/ESS/zsun/cmaq/geoweaver_plot_daily_O3.ncl\n20240107\n1 days ago\nstdate_file=20240106\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106_ML_extracted.nc doesn't exist. Skipping...\n2 days ago\nstdate_file=20240105\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105_ML_extracted.nc doesn't exist. Skipping...\n3 days ago\nstdate_file=20240104\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240104.gif exists. Skipping...\n4 days ago\nstdate_file=20240103\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240103_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240103.gif exists. Skipping...\n5 days ago\nstdate_file=20240102\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240102_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240102.gif exists. Skipping...\n6 days ago\nstdate_file=20240101\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240101_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20240101.gif exists. Skipping...\n7 days ago\nstdate_file=20231231\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231231_ML_extracted.nc exists.\n/groups/ESS3/zsun/cmaq/ai_results//gifs/Map_20231231.gif exists. Skipping...\nGenerating images/gif Completed Successfully\n",
  "history_begin_time" : 1704702850945,
  "history_end_time" : 1704702853688,
  "history_notes" : null,
  "history_process" : "iicy7w",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "kv7nbjb3alk",
  "history_input" : "#!/bin/bash\n\ndays_back=40\n\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\ncmaq_gif_location=\"/groups/ESS/share/projects/SWUS3km/graph/12km/\"\n\necho \"start to traverse \"${cmaq_gif_location}\n\nfor i in $(seq 0 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i))\n  # Setting env variables\n  YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  #/groups/ESS/share/projects/SWUS3km/graph/12km/20221108/FORECAST_O3_20221108.gif\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/\"FORECAST_O3_\"$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/obsoverlay/gif/OBS-FORECAST_O3_$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  \ndone\n",
  "history_output" : "start to traverse /groups/ESS/share/projects/SWUS3km/graph/12km/\n0 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240108/FORECAST_O3_20240108.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240108/obsoverlay/gif/OBS-FORECAST_O3_20240108.gif': No such file or directory\n1 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240107/FORECAST_O3_20240107.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240107/obsoverlay/gif/OBS-FORECAST_O3_20240107.gif': No such file or directory\n2 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240106/FORECAST_O3_20240106.gif': No such file or directory\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240106/obsoverlay/gif/OBS-FORECAST_O3_20240106.gif': No such file or directory\n3 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240105/obsoverlay/gif/OBS-FORECAST_O3_20240105.gif': No such file or directory\n4 days ago\ncp: cannot stat '/groups/ESS/share/projects/SWUS3km/graph/12km//20240104/obsoverlay/gif/OBS-FORECAST_O3_20240104.gif': No such file or directory\n5 days ago\n6 days ago\n7 days ago\n8 days ago\n9 days ago\n10 days ago\n11 days ago\n12 days ago\n13 days ago\n14 days ago\n15 days ago\n16 days ago\n17 days ago\n18 days ago\n19 days ago\n20 days ago\n21 days ago\n22 days ago\n23 days ago\n24 days ago\n25 days ago\n26 days ago\n27 days ago\n28 days ago\n29 days ago\n30 days ago\n31 days ago\n32 days ago\n33 days ago\n34 days ago\n35 days ago\n36 days ago\n37 days ago\n38 days ago\n39 days ago\n40 days ago\n",
  "history_begin_time" : 1704702608398,
  "history_end_time" : 1704702610166,
  "history_notes" : null,
  "history_process" : "nndpw6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "4kmt52glk14",
  "history_input" : "#!/bin/bash\n\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"upload_results_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J upload_results       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 1               # Number of CPUs per task (threads)\n#SBATCH --mem=10G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-02:00         # Runtime in D-HH:MM format\n\n# this step will only upload the gifs to the web server for visualization\n# the netcdfs are too big to move.\n\necho \"Saving file name list to filelist.txt\"\nfind /groups/ESS3/zsun/cmaq/ai_results/gifs/  -printf \"%f\\n\" > /groups/ESS3/zsun/cmaq/ai_results/gifs/filelist.txt\n\necho \"Copying evaluation txt to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\n\n\necho \"Copying animation gifs to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/gifs/* zsun@129.174.131.229:/var/www/html/cmaq_site/gifs/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS3/zsun/cmaq/ai_results/gifs/* zsun@129.174.131.229:/var/www/html/cmaq_site/gifs/\n  \necho \"Copying CMAQ evaluation metrics to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/alleva_12km_o3_fore.txt\n\necho \"Done\"\n\n\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "write the slurm script into upload_results_generated.sh\nsbatch upload_results_generated.sh\njob_id=1401814\nJob 1401814 has finished with state: JobState=COMPLETED\nSlurm job (1401814) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1401814      upload_re+  COMPLETED      0:0            2024-01-08T03:34:55 2024-01-08T03:35:31 \n1401814.bat+      batch  COMPLETED      0:0          0 2024-01-08T03:34:55 2024-01-08T03:35:31 \n1401814.ext+     extern  COMPLETED      0:0          0 2024-01-08T03:34:55 2024-01-08T03:35:32 \nSaving file name list to filelist.txt\nCopying evaluation txt to public server..\nsending incremental file list\nAFAR_plot.png\nAH_plot.png\nAVG_MOD_plot.png\nAVG_OBS_plot.png\nCORR_plot.png\nMB_plot.png\nME_plot.png\nNMB_plot.png\nNME_plot.png\nNSITES_plot.png\nRMSE_plot.png\neval_20231231.txt\neval_20240101.txt\neval_20240102.txt\neval_20240103.txt\nsent 9,853 bytes  received 12,735 bytes  9,035.20 bytes/sec\ntotal size is 1,489,824  speedup is 65.96\nCopying animation gifs to public server..\nsending incremental file list\nfilelist.txt\nsent 36,352 bytes  received 385 bytes  14,694.80 bytes/sec\ntotal size is 6,597,790,427  speedup is 179,595.24\nCopying CMAQ evaluation metrics to public server..\nsending incremental file list\nsent 81 bytes  received 12 bytes  62.00 bytes/sec\ntotal size is 79,489  speedup is 854.72\nDone\nAll slurm job for upload_results_generated.sh finishes.\n",
  "history_begin_time" : 1704702893550,
  "history_end_time" : 1704702935290,
  "history_notes" : null,
  "history_process" : "gn54f0",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "f0bq4qaaba4",
  "history_input" : "#!/bin/bash\n\n\n# evaluate the prediction accuracy\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\npermanent_eval_folder=\"/groups/ESS3/zsun/cmaq/ai_results/evaluation/\"\nmkdir -p $cmaq_folder/results/\nmkdir -p $permanent_eval_folder\nchmod +x $cmaq_folder/results/ -R\nchmod +x $permanent_eval_folder -R\n\n#export YYYYMMDD_POST='20220806'\n#export stdate_file='20220806'\n#export eddate_file='20220808'\n\n#This needs to be auto date\n\nexport dx=12000\n\nsource /home/zsun/.bashrc\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_eva_daily_O3.ncl\n\ncat <<EOF >> $cmaq_folder/geoweaver_eva_daily_O3.ncl\n\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\nsdate=getenv(\"YYYYMMDD_POST\")\nwfname=getenv(\"wfname\")\nobs_dir=getenv(\"obs_dir_NCL\")\nofname=getenv(\"ofname\")\nmod_dir=getenv(\"postdata_dir\")\nmfname=getenv(\"mfname\")\ndkm=tofloat(getenv(\"dx\"))\ngrid_fname=(getenv(\"grid_fname\"))\n\nmaxdist=dkm/90000.0*1.414\n;maxarea=0.25\n;thd=70\n;maxdist=0.13*1.414\nmaxarea=0.25\nthd=35.0\n\n;-----read model lat lon------\n;read lat lon\nf1 = addfile(grid_fname,\"r\")\nmlat = f1->LAT(0,0,:,:)\nmlon = f1->LON(0,0,:,:)\ndelete(f1)\nmlat1d = ndtooned(mlat)\nmlon1d = ndtooned(mlon)\ndelete([/mlat,mlon/])\n\n;-----read cmaq results-----\nf2 = addfile(mod_dir+mfname,\"r\")\nmO3 = f2->O3(:,0,:,:) ;ppb\n\n\nnt = dimsizes(mO3(:,0,0))\nny = dimsizes(mO3(0,:,0))\nnx = dimsizes(mO3(0,0,:))\n\nm8O3 = new((/17,ny,nx/),\"double\")\nm8maxO3 = new((/ny,nx/),\"double\")\n\ndo ih=0,16\n  m8O3(ih,:,:)=dim_avg_n(mO3(ih:ih+7,:,:),0)\nend do\nm8maxO3 = dim_max_n(m8O3,0) ;type double\nmO31d_d=ndtooned(m8maxO3) ; type double\nmO31d=tofloat(mO31d_d)\n\ndelete([/f2,mO3,m8O3,m8maxO3/])\n\n;-----read obs-----\nsyyyy1=str_get_cols(sdate,0,3)\nsmm1=str_get_cols(sdate,4,5)\nsdd1=str_get_cols(sdate,6,7)\n\nymd=jul2greg(greg2jul(tointeger(syyyy1),tointeger(smm1),tointeger(sdd1),-1)+1)\nsyyyy2=tostring_with_format(ymd(0),\"%0.4i\")\nsmm2=tostring_with_format(ymd(1),\"%0.2i\")\nsdd2=tostring_with_format(ymd(2),\"%0.2i\")\n\ntolat=(/-999.0/) ;set the first data to 0\ntolon=tolat\ntoO3=tolat\n\ndo ih=12,35\n  if (ih.lt.24) then\n    shh=tostring_with_format(ih,\"%0.2i\")\n    syyyy=syyyy1\n    smm=smm1\n    sdd=sdd1\n  else\n    shh=tostring_with_format(ih-24,\"%0.2i\")\n    syyyy=syyyy2\n    smm=smm2\n    sdd=sdd2\n  end if\n  data=asciiread(obs_dir+ofname+syyyy+smm+sdd+shh+\".dat\",-1,\"string\")\n  xx=array_append_record(tolat,stringtofloat(str_get_field(data(1::), 2,\",\")),0)\n  yy=array_append_record(tolon,stringtofloat(str_get_field(data(1::), 3,\",\")),0)\n  zz=array_append_record(toO3,stringtofloat(str_get_field(data(1::), 4,\",\")),0)\n  delete([/tolat,tolon,toO3/])\n  tolat=xx\n  tolon=yy\n  toO3=zz\n  delete([/xx,yy,zz/])\n  delete(data)\nend do\n\ntoO3@_FillValue = -999.0\n\n;-----calculate max ave 8 hour o3-----\noflag=tolat*0+1\naa=ind((oflag.gt.0).and.(toO3.ge.0))\nii=0\nprint(\"8h start\")\nif (any(ismissing(aa))) then\n  iflag=0\nelse\n  iflag=1\n  olat=(/tolat(aa(0))/)\n  olon=(/tolon(aa(0))/)\n  oO3=(/-999.0/)\n  o8O3 = new(17,\"float\")\n  o8O3 = -999.0\nend if\ndelete(aa)\ndo while (iflag.gt.0)\n  aa=ind((tolat.eq.olat(ii)).and.(tolon.eq.olon(ii)).and.(toO3.ge.0))\n  oflag(aa)=0\n  if (dimsizes(aa).eq.24) then  ; calculate 24 h, so calculate 8hr ozone here\n    do ih = 0, 16\n      o8O3(ih) = avg(toO3(aa(ih:ih+7)))\n    end do\n    oO3(ii)=max(o8O3)\n  end if\n  o8O3 = -999.0\n  delete(aa)\n  aa=ind((oflag.gt.0).and.(toO3.ge.0))\n  if (any(ismissing(aa))) then\n    iflag=0\n  else\n    xx=array_append_record(olat,(/tolat(aa(0))/),0)\n    yy=array_append_record(olon,(/tolon(aa(0))/),0)\n    zz=array_append_record(oO3,(/-999.0/),0)\n    delete([/olat,olon,oO3/])\n    olat=xx\n    olon=yy\n    oO3=zz\n    delete([/xx,yy,zz/])\n    ii=ii+1\n  end if\n  delete(aa)\nend do\nprint(\"obs 8hour max end\")\naa=ind(oO3.ge.0)\nnobs=dimsizes(aa)\nolat24=olat(aa)\nolon24=olon(aa)\noO324=oO3(aa)\n;print(\"oO324: \"+oO324)\ndelete([/aa,olat,olon,oO3/])\nmO324=oO324*0-999.0\n;print(\"mO324: \"+mO324)\n;print(\"mO31d: \"+mO31d)\nareaa=oO324*0-999.0\nareab=areaa\naread=areaa\n;print(\"areaa: \"+areaa)\n;print(\"areab: \"+areab)\n;print(\"aread: \"+aread)\n\n;-----find model point-----\ndo in=0,nobs-1\n  dis=sqrt((mlat1d-olat24(in))^2+(mlon1d-olon24(in))^2)\n  aa=minind(dis)\n  ;print(in+\" \"+aa)\n  if (dis(aa).lt.maxdist) then\n    mO324(in)=mO31d(aa)\n    cc=ind((mlat1d.ge.(olat24(in)-maxarea)).and.(mlat1d.le.(olat24(in)+maxarea)).and.\\\n           (mlon1d.ge.(olon24(in)-maxarea)).and.(mlon1d.le.(olon24(in)+maxarea)))\n    areaa(in)=0\n    areab(in)=0\n    if (oO324(in).ge.thd) then\n      aread(in)=0\n      if (max(mO31d(cc)).ge.thd) then\n        areab(in)=1\n      else\n        aread(in)=1\n      end if\n    else\n      bb=ind((olat24.ge.(olat24(in)-maxarea)).and.(olat24.le.(olat24(in)+maxarea)).and.\\\n             (olon24.ge.(olon24(in)-maxarea)).and.(olon24.le.(olon24(in)+maxarea)))\n      if (max(mO31d(aa)).ge.thd) then\n        if (max(oO324(bb)).ge.thd) then\n          areaa(in)=0\n        else\n          areaa(in)=1\n        end if\n      else\n        areaa(in)=0\n      end if\n      delete(bb)\n    end if\n    delete(cc)\n  end if\n  delete(aa)\nend do\n\n;-----cal rmse corr nme nmb me mb-----\ntt=ind((mO324.ge.0).and.(oO324.ge.0))\n\nif (any(ismissing(tt))) then\n  rmse=-999.0\n  corr=-999.0\n  nmb=-999.0\n  nme=-999.0\n  me=-999.0\n  mb=-999.0\nelse\n  rmse=dim_rmsd_n(oO324(tt),mO324(tt),0)\n  corr=esccr(oO324(tt),mO324(tt),0)\n  nmb=sum((mO324(tt)-oO324(tt)))/sum(oO324(tt))\n  nme=sum(abs(oO324(tt)-mO324(tt)))/sum(oO324(tt))\n  me=avg(abs(oO324(tt)-mO324(tt)))\n  mb=avg((mO324(tt)-oO324(tt)))\nend if\n;-----cal ah afar-----\n;print(\"areaa: \"+areaa)\n;print(\"areab: \"+areab)\naa=ind((areaa+areab).gt.0)\nbb=ind((aread+areab).gt.0)\n;print(\"aa: \"+aa)\n;print(\"bb: \"+bb)\nif (any(ismissing(aa))) then\n  afar=0.\nelse\n  afar=tofloat(sum(areaa(aa)))/tofloat(sum(areab(aa))+sum(areaa(aa)))*100\nend if\ndelete(aa)\nif (any(ismissing(bb))) then\n  ah=-999.0\nelse\n  ah=tofloat(sum(areab(bb)))/tofloat(sum(areab(bb))+sum(aread(bb)))*100\nend if\ndelete(bb)\nwrite_table(wfname,\"a\",[/sdate,dimsizes(tt),avg(oO324(tt)),avg(mO324(tt)),rmse,corr,nmb,nme,mb,me,ah,afar/],\\\n            \"%s,%i,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f\")\ndelete(tt)\nend\n\nexit\nEOF\n\ndays_back=7\nforce=true\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date\n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d') #This needs to be auto date\n  export wfname=$cmaq_folder\"/results/geoweaver_evalution_\"$YYYYMMDD_POST\"_results.txt\"\n\n  export obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n  export ofname=\"/AQF5X_Hourly_\"\n\n  export postdata_dir=$cmaq_folder\"/prediction_nc_files/\"\n\n  export mfname=\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n\n  export grid_fname=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/GRIDCRO2D_\"$YYYYMMDD_POST\".nc\" \n  echo \"Current Day: \"$stdate_file\n  # determine if the prediction netcdf is there\n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_eval_file=$permanent_eval_folder\"eval_\"$stdate_file\".txt\"\n  \n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_eval_file\" ]; then\n      echo \"$predict_eval_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_eval_file doesn't exist. Generating...\"\n    fi\n  fi\n  \n  rm -rf $cmaq_folder/results/* # clean everything first\n  ncl $cmaq_folder/geoweaver_eva_daily_O3.ncl\n  \n  if [ $? -eq 0 ]; then\n    echo \"Evaluation Completed Successfully\"\n    cat $wfname\n    cp $wfname $predict_eval_file\n  else\n    echo \"Evaluation Failed!\"\n  fi\n  \ndone\n\n\n",
  "history_output" : "FATAL: Missing required source file.\n  (1) Install package 'bash-completion'\n  (2) $ source /usr/share/bash-completion/bash_completion\n  (3) $ source slurm_completion.sh\n1 days ago\nCurrent Day: 20240106\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106_ML_extracted.nc doesn't exist. Skipping...\n2 days ago\nCurrent Day: 20240105\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105_ML_extracted.nc doesn't exist. Skipping...\n3 days ago\nCurrent Day: 20240104\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc exists.\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\nfatal:asciiread: Unable to open input file (/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/AQF5X_Hourly_2024010412.dat)\nfatal:[\"Execute.c\":8637]:Execute: Error occurred at or near line 84 in file /groups/ESS/zsun/cmaq/geoweaver_eva_daily_O3.ncl\nEvaluation Completed Successfully\ncat: /groups/ESS/zsun/cmaq/results/geoweaver_evalution_20240104_results.txt: No such file or directory\ncp: cannot stat '/groups/ESS/zsun/cmaq/results/geoweaver_evalution_20240104_results.txt': No such file or directory\n4 days ago\nCurrent Day: 20240103\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240103_ML_extracted.nc exists.\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t8h start\n(0)\tobs 8hour max end\nEvaluation Completed Successfully\n20240103,506,28.902420,28.974022,6.051471,0.737265,0.002477,0.155766,0.071602,4.502021,78.472221,14.393939\n5 days ago\nCurrent Day: 20240102\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240102_ML_extracted.nc exists.\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t8h start\n(0)\tobs 8hour max end\nEvaluation Completed Successfully\n20240102,523,28.885756,29.273241,5.029873,0.760801,0.013414,0.125672,0.387486,3.630126,92.500000,13.281250\n6 days ago\nCurrent Day: 20240101\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240101_ML_extracted.nc exists.\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t8h start\n(0)\tobs 8hour max end\nEvaluation Completed Successfully\n20240101,575,30.272608,29.701254,6.183760,0.738583,-0.018874,0.160574,-0.571355,4.860981,81.521736,12.280702\n7 days ago\nCurrent Day: 20231231\n/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231231_ML_extracted.nc exists.\n Copyright (C) 1995-2019 - All Rights Reserved\n University Corporation for Atmospheric Research\n NCAR Command Language Version 6.6.2\n The use of this software is governed by a License Agreement.\n See http://www.ncl.ucar.edu/ for more details.\n(0)\t8h start\n(0)\tobs 8hour max end\nEvaluation Completed Successfully\n20231231,501,31.829092,29.308107,6.419813,0.718433,-0.079204,0.168563,-2.520984,5.365204,72.777779,3.676471\n",
  "history_begin_time" : 1704702859712,
  "history_end_time" : 1704702886462,
  "history_notes" : null,
  "history_process" : "fsk7f2",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "f7084hh9r8t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704702607561,
  "history_end_time" : 1704702607561,
  "history_notes" : null,
  "history_process" : "slsirb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1f6fiimnj37",
  "history_input" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"rf_prediction_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J rf_prediction       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 8               # Number of CPUs per task (threads)\n#SBATCH --mem=150G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\nprint(\"create and clean the prediction folder\")\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      file_path = os.path.join(testing_path, file_name)\n      testing_df = pd.read_csv(file_path)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      print(\"adding month, day, and hours\")\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[4:6], errors='coerce', downcast='integer')\n      testing_df['day'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[6:8], errors='coerce', downcast='integer')\n      testing_df['hours'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[8:10], errors='coerce', downcast='integer')\n\n      print(testing_df['YYYYMMDDHH'].values[0])\n      print(testing_df['month'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      print(f\"file_dateTime={file_dateTime}\")\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)\n        \nprint(\"Prediction is all done.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "start to run test_data_slurm_generated.sh\n/home/zsun/gw-workspace/1f6fiimnj37\nwrite the slurm script into rf_prediction_slurm_generated.sh\nsbatch rf_prediction_slurm_generated.sh\njob_id=1401812\nJob 1401812 has finished with state: JobState=COMPLETED\nSlurm job (1401812) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1401812      rf_predic+  COMPLETED      0:0            2024-01-08T03:30:54 2024-01-08T03:33:04 \n1401812.bat+      batch  COMPLETED      0:0 108936064K 2024-01-08T03:30:54 2024-01-08T03:33:04 \n1401812.ext+     extern  COMPLETED      0:0          0 2024-01-08T03:30:54 2024-01-08T03:33:04 \ncreate and clean the prediction folder\nstart to load model\nmodel is loaded\nPrediction is all done.\nAll slurm job for rf_prediction_slurm_generated.sh finishes.\n",
  "history_begin_time" : 1704702652203,
  "history_end_time" : 1704702795908,
  "history_notes" : null,
  "history_process" : "xa1jxg",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "i99wcbea5cs",
  "history_input" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"preprocessing_test_netcdf.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J preprocessing_test_netcdf       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# load the prediction_rf.csv into a NetCDF file for visualization\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\ntoday = datetime.today()\nedate = today\nsdate = today - timedelta(days=days_back)\ndays = get_days_list_for_prediction(sdate, edate)\n\nprediction_path = f\"{cmaq_folder}/prediction_files/\"\n\nall_hourly_files = sorted(glob.glob(os.path.join(prediction_path, \"*.csv\")))\n# print(\"overall hourly files: \", all_hourly_files)\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor i in range(len(days)-1):\n  print(days[i])\n  current_day = days[i]\n  next_day = days[i+1]\n  \n  cmaq_cdf_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n  \n  if not os.path.exists(cmaq_cdf_file):\n    print(f\"{cmaq_cdf_file} doesn't exist\")\n    continue\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cdf = xr.open_dataset(cmaq_cdf_file)\n  daily_hourly_files = []\n  for k in real_hour_list:\n    real_hour_value = real_hour_list[k]\n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    #daily_hourly_files.append(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    daily_hourly_files.append(f'{cmaq_folder}/prediction_files/prediction_rf_{day}{turn_2_digits(real_hour_value)}.csv')\n  \n  daily_hourly_files = sorted(daily_hourly_files)\n  #print(\"single day hourly files: \", all_hourly_files[i*24:(i+1)*24])\n  print(\"single day hourly files: \", daily_hourly_files)\n  df_from_each_hourly_file = (pd.read_csv(f) for f in daily_hourly_files)\n  \n  df_csv = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n  reshaped_prediction = df_csv['prediction'].to_numpy().reshape(24, 1, 265, 442).astype(np.float32)\n  print(reshaped_prediction.shape)\n  \n  # retain only two essential variables\n  clean_df_cdf = df_cdf[['O3', 'TFLAG']]\n  print(\"O3 attrs is: \", df_cdf.O3.attrs)\n  \n  # reduce VAR dim to 1\n  new_tflag = df_cdf['TFLAG'].to_numpy()\n  new_tflag = new_tflag[:, 0, :].reshape(24, 1, 2)\n  \n  # Apply changes to data variable in nc file\n  clean_df_cdf['O3'] = (['TSTEP', 'LAY', 'ROW', 'COL'], reshaped_prediction)\n  clean_df_cdf['TFLAG'] = (['TSTEP', 'VAR', 'DATE-TIME'], new_tflag)\n\n  clean_df_cdf.O3.attrs = df_cdf.O3.attrs\n  clean_df_cdf.TFLAG.attrs = df_cdf.TFLAG.attrs\n  clean_df_cdf.attrs['VGLVLS'] = \"1.f, 0.9941f\"\n  clean_df_cdf.attrs['VAR-LIST'] = \"O3              \"\n#   create_and_clean_folder(f\"{cmaq_folder}/prediction_nc_files\")\n  clean_df_cdf.to_netcdf(target_cdf_file,)\n\n  print(f'Saved updated netCDF file: {target_cdf_file}')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"",
  "history_output" : "write the slurm script into preprocessing_test_netcdf.sh\nsbatch preprocessing_test_netcdf.sh\njob_id=1401813\nJob 1401813 has finished with state: JobState=COMPLETED\nSlurm job (1401813) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1401813      preproces+  COMPLETED      0:0            2024-01-08T03:33:19 2024-01-08T03:33:55 \n1401813.bat+      batch  COMPLETED      0:0       124K 2024-01-08T03:33:19 2024-01-08T03:33:55 \n1401813.ext+     extern  COMPLETED      0:0          0 2024-01-08T03:33:19 2024-01-08T03:33:55 \n20231129\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231129_ML_extracted.nc already exists\n20231130\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231130_ML_extracted.nc already exists\n20231201\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231201_ML_extracted.nc already exists\n20231202\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231202_ML_extracted.nc already exists\n20231203\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231203_ML_extracted.nc already exists\n20231204\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231204_ML_extracted.nc already exists\n20231205\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231205_ML_extracted.nc already exists\n20231206\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231206_ML_extracted.nc already exists\n20231207\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231207_ML_extracted.nc already exists\n20231208\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231208_ML_extracted.nc already exists\n20231209\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231209_ML_extracted.nc already exists\n20231210\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231210_ML_extracted.nc already exists\n20231211\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231211_ML_extracted.nc already exists\n20231212\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231212_ML_extracted.nc already exists\n20231213\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231213_ML_extracted.nc already exists\n20231214\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231214_ML_extracted.nc already exists\n20231215\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231215_ML_extracted.nc already exists\n20231216\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231216_ML_extracted.nc already exists\n20231217\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231217_ML_extracted.nc already exists\n20231218\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231218_ML_extracted.nc already exists\n20231219\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231219_ML_extracted.nc already exists\n20231220\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231220_ML_extracted.nc already exists\n20231221\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231221_ML_extracted.nc already exists\n20231222\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231222_ML_extracted.nc already exists\n20231223\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231223_ML_extracted.nc already exists\n20231224\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231224_ML_extracted.nc already exists\n20231225\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231225_ML_extracted.nc already exists\n20231226\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231226_ML_extracted.nc already exists\n20231227\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231227_ML_extracted.nc already exists\n20231228\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231228_ML_extracted.nc already exists\n20231229\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231229_ML_extracted.nc already exists\n20231230\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231230_ML_extracted.nc already exists\n20231231\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231231_ML_extracted.nc already exists\n20240101\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240101_ML_extracted.nc already exists\n20240102\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240102_ML_extracted.nc already exists\n20240103\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240103_ML_extracted.nc already exists\n20240104\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc already exists\n20240105\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105.nc doesn't exist\n20240106\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106.nc doesn't exist\n20240107\n/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240107.nc doesn't exist\nAll slurm job for preprocessing_test_netcdf.sh finishes.\n",
  "history_begin_time" : 1704702797295,
  "history_end_time" : 1704702849214,
  "history_notes" : null,
  "history_process" : "klx3sc",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2dyixy46wjx",
  "history_input" : "#!/bin/bash\n\n\necho \"start to run processing_test_data_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"processing_test_data_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J processing_test_data       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)  # don't clean folder anymore\n#os.makedirs(test_folder, exist_ok=True)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file)\n  \n  # read mcip results \n  mcip_file = \"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file)\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    hourly_target_file = f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    if os.path.exists(hourly_target_file):\n      print(f\"Hourly csv file {hourly_target_file} already exist\")\n      continue\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(hourly_target_file, index=False)\n\nprint('Done with preparing testing data!')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "history_output" : "start to run processing_test_data_generated.sh\n/home/zsun/gw-workspace/2dyixy46wjx\nwrite the slurm script into processing_test_data_generated.sh\nsbatch processing_test_data_generated.sh\njob_id=1401811\nJob 1401811 has finished with state: JobState=COMPLETED\nSlurm job (1401811) has finished.\nPrint the job's output logs\nJobID           JobName      State ExitCode     MaxRSS               Start                 End \n------------ ---------- ---------- -------- ---------- ------------------- ------------------- \n1401811      processin+  COMPLETED      0:0            2024-01-08T03:30:10 2024-01-08T03:30:46 \n1401811.bat+      batch  COMPLETED      0:0       240K 2024-01-08T03:30:10 2024-01-08T03:30:46 \n1401811.ext+     extern  COMPLETED      0:0          0 2024-01-08T03:30:10 2024-01-08T03:30:46 \nGetting data for: 20231129\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231129_ML_extracted.nc already exists\nGetting data for: 20231130\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231130_ML_extracted.nc already exists\nGetting data for: 20231201\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231201_ML_extracted.nc already exists\nGetting data for: 20231202\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231202_ML_extracted.nc already exists\nGetting data for: 20231203\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231203_ML_extracted.nc already exists\nGetting data for: 20231204\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231204_ML_extracted.nc already exists\nGetting data for: 20231205\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231205_ML_extracted.nc already exists\nGetting data for: 20231206\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231206_ML_extracted.nc already exists\nGetting data for: 20231207\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231207_ML_extracted.nc already exists\nGetting data for: 20231208\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231208_ML_extracted.nc already exists\nGetting data for: 20231209\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231209_ML_extracted.nc already exists\nGetting data for: 20231210\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231210_ML_extracted.nc already exists\nGetting data for: 20231211\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231211_ML_extracted.nc already exists\nGetting data for: 20231212\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231212_ML_extracted.nc already exists\nGetting data for: 20231213\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231213_ML_extracted.nc already exists\nGetting data for: 20231214\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231214_ML_extracted.nc already exists\nGetting data for: 20231215\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231215_ML_extracted.nc already exists\nGetting data for: 20231216\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231216_ML_extracted.nc already exists\nGetting data for: 20231217\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231217_ML_extracted.nc already exists\nGetting data for: 20231218\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231218_ML_extracted.nc already exists\nGetting data for: 20231219\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231219_ML_extracted.nc already exists\nGetting data for: 20231220\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231220_ML_extracted.nc already exists\nGetting data for: 20231221\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231221_ML_extracted.nc already exists\nGetting data for: 20231222\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231222_ML_extracted.nc already exists\nGetting data for: 20231223\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231223_ML_extracted.nc already exists\nGetting data for: 20231224\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231224_ML_extracted.nc already exists\nGetting data for: 20231225\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231225_ML_extracted.nc already exists\nGetting data for: 20231226\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231226_ML_extracted.nc already exists\nGetting data for: 20231227\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231227_ML_extracted.nc already exists\nGetting data for: 20231228\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231228_ML_extracted.nc already exists\nGetting data for: 20231229\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231229_ML_extracted.nc already exists\nGetting data for: 20231230\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231230_ML_extracted.nc already exists\nGetting data for: 20231231\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20231231_ML_extracted.nc already exists\nGetting data for: 20240101\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240101_ML_extracted.nc already exists\nGetting data for: 20240102\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240102_ML_extracted.nc already exists\nGetting data for: 20240103\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240103_ML_extracted.nc already exists\nGetting data for: 20240104\n/groups/ESS/zsun/cmaq//prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_20240104_ML_extracted.nc already exists\nGetting data for: 20240105\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240105.nc doesn't exist\nGetting data for: 20240106\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240106.nc doesn't exist\nGetting data for: 20240107\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240107.nc doesn't exist\nGetting data for: 20240108\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240108.nc doesn't exist\nGetting data for: 20240109\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240109.nc doesn't exist\nGetting data for: 20240110\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240110.nc doesn't exist\nGetting data for: 20240111\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240111.nc doesn't exist\nGetting data for: 20240112\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240112.nc doesn't exist\nGetting data for: 20240113\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240113.nc doesn't exist\nGetting data for: 20240114\nCMAQ file /scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_20240114.nc doesn't exist\nDone with preparing testing data!\nAll slurm job for processing_test_data_generated.sh finishes.\n",
  "history_begin_time" : 1704702608398,
  "history_end_time" : 1704702650233,
  "history_notes" : null,
  "history_process" : "z4du0c",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fkp940cx0ly",
  "history_input" : "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n\ndef draw_charts(column_name, ai_df_sorted, cmaq_df_clipped):\n  # Plot the RMSE values\n  plt.figure(figsize=(12, 6))\n  plt.plot(ai_df_sorted['DATE'], ai_df_sorted[column_name], label='AI', color='green')\n  plt.plot(cmaq_df_clipped['DATE'], cmaq_df_clipped[column_name], label='CMAQ', color='blue')\n\n  # Customize the plot\n  plt.xlabel('Date')\n  plt.ylabel(column_name)\n  plt.title(f'CMAQ AI Metrics - {column_name}')\n  plt.legend()\n\n  # Save the plot as a PNG file\n  plt.grid(True)\n  plt.tight_layout()\n  plot_png_path = f'/groups/ESS3/zsun/cmaq/ai_results/evaluation/{column_name}_plot.png'\n  plt.savefig(plot_png_path)\n  print(f\"Plot saved to {plot_png_path}\")\n\ndef do_it():\n  # read the CMAQ evaluation txt\n  cmaq_eval_txt = \"/groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt\"\n  cmaq_df = pd.read_csv(cmaq_eval_txt)  # Assuming your data has no header\n  \n  # read all txt files in the evaluation folder\n  directory_path = '/groups/ESS3/zsun/cmaq/ai_results/evaluation/'\n\n  # Initialize an empty list to store DataFrames.\n  dfs = []\n  \n  column_names = ['DATE', 'NSITES', 'AVG_OBS', 'AVG_MOD', 'RMSE', 'CORR', 'NMB', 'NME', 'MB', 'ME', 'AH', 'AFAR']\n  \n  # Iterate over the files in the directory.\n  for filename in os.listdir(directory_path):\n      if filename.startswith('eval'):  # Adjust the file extension as needed.\n          file_path = os.path.join(directory_path, filename)\n\n          # Read the CSV file into a DataFrame.\n          df = pd.read_csv(file_path, names = column_names, header=None)  # Assuming your data has no header.\n\n          # Append the DataFrame to the list.\n          dfs.append(df)\n\n  # Concatenate all DataFrames into a single DataFrame.\n  ai_df = pd.concat(dfs, ignore_index=True)\n  \n  # Convert the 'DATE' column to a datetime object\n  ai_df['DATE'] = pd.to_datetime(ai_df['DATE'], format='%Y%m%d')\n  cmaq_df['DATE'] = pd.to_datetime(cmaq_df['DATE'], format='%Y%m%d')\n  \n  # Sort the DataFrame by the 'DATE' column\n  ai_df_sorted = ai_df.sort_values(by='DATE')\n  cmaq_df_sorted = cmaq_df.sort_values(by='DATE')\n  \n  min_date = ai_df_sorted['DATE'].min()\n  cmaq_df_clipped = cmaq_df_sorted[cmaq_df_sorted['DATE'] >= min_date]\n  cmaq_df_clipped.loc[cmaq_df_clipped['AH'] < 0, 'AH'] = 0\n  \n  for column_name in column_names:\n    if column_name != \"DATE\":\n      draw_charts(column_name, ai_df_sorted, cmaq_df_clipped)\n  \n  print(\"all done\")\n  \ndo_it()\n",
  "history_output" : "Plot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/NSITES_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/AVG_OBS_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/AVG_MOD_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/RMSE_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/CORR_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/NMB_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/NME_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/MB_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/ME_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/AH_plot.png\nPlot saved to /groups/ESS3/zsun/cmaq/ai_results/evaluation/AFAR_plot.png\nall done\n",
  "history_begin_time" : 1704702887595,
  "history_end_time" : 1704702891705,
  "history_notes" : null,
  "history_process" : "or8itt",
  "host_id" : "100001",
  "indicator" : "Done"
}]
