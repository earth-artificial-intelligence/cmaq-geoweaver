[{
  "id" : "6up921",
  "name" : "preparing_cmaq_training_data",
  "description" : null,
  "code" : "# get all of the cmaq model output variables and save to hourly csvs\nfrom cmaq_ai_utils import *\n\nsdate = date(2022, 8, 1)   # start date\nedate = date(2022, 8, 2)   # end date\n\ndays = get_days_list(sdate, edate)\n\n# k = time dimension - start from 12 to match with data\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntraining_input_folder = f\"{cmaq_folder}/training_input_hourly/\"\ncreate_and_clean_folder(training_input_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  df_cmaq = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/CCTMout/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\"_extracted.nc\")\n  \n  # read mcip results \n  df_mcip = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\")\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value >= 0 and real_hour_value <=11:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n    # CMAQ data\n    # O3 variable\n    \n    o3=df_cmaq.variables['O3'][:].values[k,0]\n    cmaq_O3=list(np.ravel(o3).transpose().round())  \n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k,0]\n    cmaq_NO2=list(np.ravel(no2).transpose().round())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k,0]\n    cmaq_CO=list(np.ravel(co).transpose().round())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k,0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose().round())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k,0]\n    CO_emi=list(np.ravel(co_emis).transpose().round())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k,0]\n    PRSFC=list(np.ravel(prsfc).transpose().round())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k,0]\n    PBL=list(np.ravel(pbl).transpose().round())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k,0]\n    TEMP2=list(np.ravel(temp2).transpose().round())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k,0]\n    WSPD10=list(np.ravel(wspd10).transpose().round())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k,0]\n    WDIR10=list(np.ravel(wdir10).transpose().round())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k,0]\n    RGRND=list(np.ravel(rgrnd).transpose().round())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k,0]\n    CFRAC=list(np.ravel(cfrac).transpose().round())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    lat_flt=np.ravel(lat)\n    LAT=np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    \n    filename = f'/groups/ESS/zsun/cmaq/training_input_hourly/train_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    print(f'Saving file: train_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(filename,index=False)\n\nprint('Done!')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xpdg66",
  "name" : "observation",
  "description" : null,
  "code" : "# get all the airnow station data and save to csvs as well\nimport glob, os\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2021, 8, 1)   # start date\nedate = date(2022, 8, 2)   # end date\ndays = get_days_list(sdate, edate)\n\nobservation_folder = f\"{cmaq_folder}/observation/\"\ncreate_and_clean_folder(observation_folder)\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\nfor i in range(len(days)-1):\n  current_day = days[i]\n  next_day = days[i+1]\n  for x in range(len(time)):\n    if x >= 12:\n      i = next_day\n    else:\n      i = current_day\n    t = time[x]\n    files = \"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X/\"+\"AQF5X_Hourly_\"+i+t+\".dat\"\n    \n    with open(files, 'r') as file:\n      text = file.read()\n      new_string = text.replace('\"', '')\n      out_file = f\"{observation_folder}/AQF5X_Hourly_{i}{t}.txt\"\n      print(\"Saving to :\", out_file)\n      outF = open(out_file, \"w\")\n      for line in new_string:\n        outF.write(line)\n      \n      outF.close()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xlayd5",
  "name" : "processing_observation_data",
  "description" : null,
  "code" : "# take all the airnow observation csvs and merge into one observation.csv\n\nimport glob\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom cmaq_ai_utils import *\n\nsdate = date(2021, 8, 1)   # start date\nedate = date(2022, 8, 2)   # end date\ndays = get_days_list(sdate, edate)\n    \ndata_frame = pd.DataFrame()\nmerged=[]\ndate_time=[]\n\ntime = ['12','13','14','15','16','17','18','19','20','21','22','23','00','01','02','03','04','05','06','07','08','09','10','11']\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  for y in range(len(time)):\n    t = time[y]\n    if y>=12:\n      d = next_day\n    else:\n      d = current_day\n    files=glob.glob(f\"{cmaq_folder}/observation/AQF5X_Hourly_{d}{t}.txt\")\n    for file in files:\n      print(file)\n      data = np.loadtxt(file, skiprows=1,dtype='str')\n      dt=d+t\n      print(dt)\n      dt=np.tile(dt,len(data)) # constructs an array for each hour of each day with length of data from total stations available\n      date_time.append(dt)\n      merged.append(data)\n            \ndata_frame = np.concatenate(merged)\n\n# This gets the first 4 columns in the observation file (AQSID, Latitude, Longitude, OZONE(ppb))\ndata_frame = np.delete(data_frame, np.s_[4:9], axis=1) \n\ndf = pd.DataFrame(data_frame, columns = ['StationID','Latitude','Longitude','AirNOW_O3'])\ndff=df.replace(',','', regex=True)\n\ndt = np.concatenate(date_time)\ndff['YYYYMMDDHH'] = dt.tolist()\ndff.to_csv(f\"{cmaq_folder}/observation_one_year.csv\",index=False)",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "argv3i",
  "name" : "merge_training_data",
  "description" : null,
  "code" : "# combine cmaq and airnow into training.csv\nfrom cmaq_ai_utils import *\n\nobs=pd.read_csv(f\"{cmaq_folder}/observation_one_year.csv\")\nobs['Latitude'] = obs['Latitude'].astype(float)\nobs['Longitude'] = obs['Longitude'].astype(float)\nobs['YYYYMMDDHH'] = obs['YYYYMMDDHH'].astype(str)\nprint(\"obs head: \", obs.columns, obs.head())\nref_stations=pd.read_csv(f\"{cmaq_folder}/updated_station_to_cell.csv\")\nref_stations = ref_stations.astype(float)\nprint(\"ref_stations head: \", ref_stations.columns, ref_stations.head())\n\nprint(\"obs shape: \", obs.shape)\nprint(\"ref_station shape: \", ref_stations.shape)\n\nchunksize = 10000\ncount = 0\nwith pd.read_csv(\"/groups/ESS3/aalnaim/cmaq/merged_cmaq_one_year.csv\", chunksize=chunksize) as reader:\n  for chunk in reader:\n    chunk = chunk[pd.to_numeric(chunk['Latitude'], errors='coerce').notnull()]\n    chunk = chunk[pd.to_numeric(chunk['Longitude'], errors='coerce').notnull()]\n    chunk['Latitude'] = chunk['Latitude'].astype(float)\n    chunk['Longitude'] = chunk['Longitude'].astype(float)\n    new_df = pd.merge(chunk,  ref_stations, how='left', left_on=['Latitude','Longitude'], right_on = ['Lat_cmaq','Lon_cmaq'])\n    new_df = new_df[new_df.Lat_cmaq.notnull()][new_df.Lat_airnow!=-999]\n    new_df['Lat_airnow'] = new_df['Lat_airnow'].astype(float)\n    new_df['Lon_airnow'] = new_df['Lon_airnow'].astype(float)\n    new_df['YYYYMMDDHH'] = new_df['YYYYMMDDHH'].astype(str)\n    chunk_final = pd.merge(new_df, obs, how='left', left_on=['Lat_airnow','Lon_airnow','YYYYMMDDHH'], right_on = ['Latitude','Longitude','YYYYMMDDHH'])\n    chunk_final['month'] = chunk_final['YYYYMMDDHH'].str[4:6]\n    chunk_final['day'] = chunk_final['YYYYMMDDHH'].str[6:8]\n    chunk_final['hours'] = chunk_final['YYYYMMDDHH'].str[8:10]\n    new_chunk_df=chunk_final.drop(['YYYYMMDDHH'],axis=1)\n    final_chunk_df = new_chunk_df[new_chunk_df.AirNOW_O3.notnull()][new_chunk_df.AirNOW_O3!=-999]\n    #print(\"final_chunk_df\", final_chunk_df)\n    #count += 1\n    #if count == 10:\n    final_chunk_df.to_csv(f\"{cmaq_folder}/training_new_one_year_valid_test.csv\",mode='a',index=False)\n    break\n    \n    \nprint(\"All records should be incorporated into :\", f\"{cmaq_folder}/training_new_one_year_valid_test.csv\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "l8vlic",
  "name" : "rf_prediction",
  "description" : null,
  "code" : "# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\nprint(\"create and clean the prediction folder\")\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      testing_df = pd.read_csv(f)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = testing_df['YYYYMMDDHH'].str[4:6]\n      testing_df['day'] = testing_df['YYYYMMDDHH'].str[6:8]\n      testing_df['hours'] = testing_df['YYYYMMDDHH'].str[8:10]\n\n      #print(testing_df['YYYYMMDDHH'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      #print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)\n        \nprint(\"Prediction is all done.\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wny2dz",
  "name" : "rf_pyCaret_gpu",
  "description" : null,
  "code" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/zsu/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/zsun/cmaq/models/rf_from_hourly_fixed.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "3asyzj",
  "name" : "processing_test_netcdf",
  "description" : null,
  "code" : "# load the prediction_rf.csv into a NetCDF file for visualization\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\ntoday = datetime.today()\nedate = today\nsdate = today - timedelta(days=7)\ndays = get_days_list_for_prediction(sdate, edate)\n\nprediction_path = f\"{cmaq_folder}/prediction_files/\"\n\nall_hourly_files = sorted(glob.glob(os.path.join(prediction_path, \"*.csv\")))\n# print(\"overall hourly files: \", all_hourly_files)\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor i in range(len(days)-1):\n  print(days[i])\n  current_day = days[i]\n  next_day = days[i+1]\n  \n  cmaq_cdf_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n  \n  if not os.path.exists(cmaq_cdf_file):\n    print(f\"{cmaq_cdf_file} doesn't exist\")\n    continue\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cdf = xr.open_dataset(cmaq_cdf_file)\n  daily_hourly_files = []\n  for k in real_hour_list:\n    real_hour_value = real_hour_list[k]\n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    #daily_hourly_files.append(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    daily_hourly_files.append(f'{cmaq_folder}/prediction_files/prediction_rf_{day}{turn_2_digits(real_hour_value)}.csv')\n  \n  daily_hourly_files = sorted(daily_hourly_files)\n  #print(\"single day hourly files: \", all_hourly_files[i*24:(i+1)*24])\n  print(\"single day hourly files: \", daily_hourly_files)\n  df_from_each_hourly_file = (pd.read_csv(f) for f in daily_hourly_files)\n  \n  df_csv = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n  reshaped_prediction = df_csv['prediction'].to_numpy().reshape(24, 1, 265, 442).astype(np.float32)\n  print(reshaped_prediction.shape)\n  \n  # retain only two essential variables\n  clean_df_cdf = df_cdf[['O3', 'TFLAG']]\n  print(\"O3 attrs is: \", df_cdf.O3.attrs)\n  \n  # reduce VAR dim to 1\n  new_tflag = df_cdf['TFLAG'].to_numpy()\n  new_tflag = new_tflag[:, 0, :].reshape(24, 1, 2)\n  \n  # Apply changes to data variable in nc file\n  clean_df_cdf['O3'] = (['TSTEP', 'LAY', 'ROW', 'COL'], reshaped_prediction)\n  clean_df_cdf['TFLAG'] = (['TSTEP', 'VAR', 'DATE-TIME'], new_tflag)\n\n  clean_df_cdf.O3.attrs = df_cdf.O3.attrs\n  clean_df_cdf.TFLAG.attrs = df_cdf.TFLAG.attrs\n  clean_df_cdf.attrs['VGLVLS'] = \"1.f, 0.9941f\"\n  clean_df_cdf.attrs['VAR-LIST'] = \"O3              \"\n#   create_and_clean_folder(f\"{cmaq_folder}/prediction_nc_files\")\n  clean_df_cdf.to_netcdf(target_cdf_file,)\n\n  print(f'Saved updated netCDF file: {target_cdf_file}')\n  ",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "9xdvh6",
  "name" : "environment_setting",
  "description" : null,
  "code" : "# install all dependencies\n# NASA-GEOWEAVER: Environment setting\n\nimport os\nimport sys\nimport subprocess\nimport pkg_resources\n\nwith open('requirements.txt','w') as out:\n  out.write('''\nabsl-py==1.0.0\naffine==2.3.1\nasttokens==2.0.5\nastunparse==1.6.3\nattrs==21.4.0\nautokeras==1.0.18\nbackcall==0.2.0\ncachetools==5.0.0\ncertifi==2021.10.8\ncftime==1.6.0\ncharset-normalizer==2.0.12\nclick==8.1.3\nclick-plugins==1.1.1\ncligj==0.7.2\ncmaps==1.0.5\ncycler==0.11.0\ndecorator==5.1.1\nearthpy==0.9.4\nexecuting==0.8.3\nFiona==1.8.21\nflatbuffers==2.0\nfonttools==4.29.1\ngast==0.5.3\ngeopandas==0.10.2\nglob2==0.7\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.44.0\nh5py==3.6.0\nidna==3.3\nimageio==2.19.0\nimageio-ffmpeg==0.4.7\nimportlib-metadata==4.11.2\nipython==8.1.1\njedi==0.18.1\njoblib==1.1.0\nkaleido==0.2.1\nkeras==2.8.0\nKeras-Preprocessing==1.1.2\nkeras-tuner==1.1.0\nkiwisolver==1.3.2\nkt-legacy==1.0.4\nlibclang==13.0.0\nMarkdown==3.3.6\nmatplotlib==3.5.1\nmatplotlib-inline==0.1.3\nmunch==2.5.0\nnetCDF4==1.5.8\nnetworkx==2.8\nnumpy==1.22.2\noauthlib==3.2.0\nopencv-python==4.5.5.64\nopt-einsum==3.3.0\npackaging==21.3\npandas==1.4.1\nparso==0.8.3\npathlib==1.0.1\npathlib2==2.3.7.post1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.0.1\nplotly==5.7.0\nprompt-toolkit==3.0.28\nprotobuf==3.19.4\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\nPygments==2.11.2\npyparsing==3.0.7\npyproj==3.3.1\npython-dateutil==2.8.2\npytz==2021.3\nPyWavelets==1.3.0\nrasterio==1.2.10\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrsa==4.8\nscikit-image==0.19.2\nscikit-learn==1.0.2\nscipy==1.8.0\nseaborn==0.11.2\nShapely==1.8.2\nsix==1.16.0\nsklearn==0.0\nsnuggs==1.4.7\nstack-data==0.2.0\ntenacity==8.0.1\ntensorboard==2.8.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.8.0\ntensorflow-gpu==2.8.0\ntensorflow-io-gcs-filesystem==0.24.0\ntermcolor==1.1.0\ntf-estimator-nightly==2.8.0.dev2021122109\nthreadpoolctl==3.1.0\ntifffile==2022.5.4\ntraitlets==5.1.1\ntyping_extensions==4.1.1\nurllib3==1.26.8\nwcwidth==0.2.5\nWerkzeug==2.0.3\nwrapt==1.13.3\nxarray==2022.3.0\nxgboost==1.6.0\nzipp==3.7.0''')\n  \npython = sys.executable\nsubprocess.check_call([python, '-m', 'pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.DEVNULL)\n    #subprocess.check_call(\n        #[python, '-m', 'conda', 'install', '-c','conda-forge','xgboost'],\n      #stdout=subprocess.DEVNULL)\n\n\n################################\n#  END OF PACKAGES Installation  #\n\n\n# Creating directoris \nfrom pathlib import Path\nhome = str(Path.home())\nfolders = ['cmaq/exploratory_analysis', 'cmaq/prediction_maps', 'cmaq/prediction_files','cmaq/models','cmaq/observation']\nfor folder in folders:\n  paths=Path(home+'/'+folder)\n  paths.mkdir(parents=True,exist_ok=True)\n  \n  ###############################\n  # END OF DIRECTORY CREATION #",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ex3vh9",
  "name" : "processing_test_data",
  "description" : null,
  "code" : "# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file)\n  \n  # read mcip results \n  mcip_file = \"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file)\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\")\n  \n  for k in time_step_in_netcdf_list:\n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv',index=False)\n\nprint('Done with preparing testing data!')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b8uv5z",
  "name" : "test_data",
  "description" : null,
  "code" : "# merge all hourly testing data into daily files\n\nimport pandas as pd\nimport glob\nimport os\nfrom pathlib import Path\nfrom cmaq_ai_utils import *\n\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\nprint(f\"testing_path: {testing_path}\")\nall_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))   \n# advisable to use os.path.join as this makes concatenation OS independent\n\ndf_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\ncmaq = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n# dropping unnecessary variables\ncmaq['YYYYMMDDHH'] = cmaq['YYYYMMDDHH'].map(str)\ncmaq['month'] = cmaq['YYYYMMDDHH'].str[4:6]\ncmaq['day'] = cmaq['YYYYMMDDHH'].str[6:8]\ncmaq['hours'] = cmaq['YYYYMMDDHH'].str[8:10]\n\nremove_file(f\"{testing_path}/testing.csv\")\nprint(f\"removed all file {testing_path}/testing.csv\")\ncmaq.to_csv(f\"{testing_path}/testing.csv\",index=False)\n\nprint('Done with generating testing.csv!')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "is1w3m",
  "name" : "generate_AirNow_ncl",
  "description" : null,
  "code" : "#!/bin/bash\n\n\n# generate images and gif from the prediction NetCDF files and overlay the AirNow station observation on the top\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\nmkdir $cmaq_folder\"/plots\"\npermanent_location=/groups/ESS3/zsun/cmaq/ai_results/\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport graph_dir=\"/groups/ESS/zsun/cmaq/plots\"\n\nexport obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n\nsource /home/zsun/.bashrc\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\ncat <<EOF >>$cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\ndate = getenv(\"YYYYMMDD_POST\") \nd1 = getenv(\"stdate_post\") \nd2 = getenv(\"eddate_post\") \n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\nobs_dir = getenv(\"obs_dir_NCL\")\nplot_dir = getenv(\"graph_dir\") \n\nhr=new(24,\"string\")\nhr=(/\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\"/)\n\nprint(plot_dir)\naconc_dir = getenv(\"postdata_dir\") \ngrid_dir = getenv(\"mcip_dir\")\n\nprint(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\n\ncdf_file1 = addfile(aconc_dir+\"/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\ncdf_file2= addfile(grid_dir+\"/METCRO2D_\"+date+\".nc\",\"r\")\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\nwspd10=cdf_file2->WSPD10(:,0,:,:)\nwdir10=cdf_file2->WDIR10(:,0,:,:)\n\ntemp = cdf_file2->TEMP2\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(max(temp))\nprint(min(temp))\nprint(avg(temp))\n\n\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\no3@unit = \"ppbv\"\n\nUV10=wind_component(wspd10,wdir10,0)\nUV10@lat2d = lat\nUV10@lon2d = lon\n\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; dont advance frame\nres@gsnDraw = False\nres@gsnLeftString  = \"\"\nres@gsnRightString = \"\"\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n;res@tiMainFontHeightF = 0.02\n;res@vpWidthF        = 0.7\n;res@vpHeightF       = 0.7\n\n;;set map;;\nmpres                             = res\nmpres@mpLimitMode = \"LatLon\"\nmpres@mpDataSetName               = \"Earth..4\"\nmpres@mpDataBaseVersion           = \"MediumRes\"\nmpres@mpOutlineOn                 = True\nmpres@mpGeophysicalLineThicknessF = 1.5\nmpres@mpFillDrawOrder             = \"PostDraw\"\nmpres@mpFillOn                    = False\nmpres@mpAreaMaskingOn         = True\nmpres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nmpres@mpOutlineSpecifiers         = \"United States:States\"\nmpres@mpProjection           = \"LambertConformal\"\nmpres@mpLambertParallel1F    = 33.\nmpres@mpLambertParallel2F    = 45.\nmpres@mpLambertMeridianF     = -98.\nmpres@mpMinLonF = -120 ;min(lon)+0.2\nmpres@mpMaxLonF = -70 ;max(lon)-0.2\nmpres@mpMinLatF = 25 ;min(lat)+0.05\nmpres@mpMaxLatF = 50 ;max(lat)-0.05\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@mpLandFillColor         = \"white\"\nmpres@mpInlandWaterFillColor  = \"white\"\nmpres@mpOceanFillColor        = \"white\"\nmpres@mpGeophysicalLineColor    = \"Black\"\n\n;mpres@lbLabelAutoStride       = True\nmpres@tiXAxisFont             = 25\nmpres@pmTickMarkDisplayMode   = \"Always\"\nmpres@tmXBLabelFont           = 25\nmpres@tmXBLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmYLLabelFont           = 25\nmpres@tmYLLabelFontHeightF    = 0.013\nmpres@tmXBLabelDeltaF         = -0.5\nmpres@tmXTLabelsOn            = False\nmpres@tmXTLabelFont           = 25\nmpres@tmXTLabelFontHeightF    = 0.013\nmpres@tmYRLabelsOn            = False\nmpres@tmYRLabelFont           = 25\nmpres@tmYRLabelFontHeightF    = 0.013\n\n;;set contour;;\ncnres                         = res\ncnres@cnFillDrawOrder         = \"PreDraw\"\ncnres@cnFillOn                = True\ncnres@cnLinesOn               = False\ncnres@cnLineLabelsOn          = False\ncnres@lbLabelFont             = 25\ncnres@lbLabelFontHeightF      = 0.013\ncnres@tiXAxisFont             = 25\ncnres@pmLabelBarWidthF        = 0.5\ncnres@pmLabelBarHeightF       = 0.1\n;cnres@pmLabelBarOrthogonalPosF = -0.02\ncnres@lbLabelAutoStride       = True\n\n;set vector;;\nres_vc                        = res\nres_vc@vcGlyphStyle           = \"LineArrow\"\nres_vc@vcLineArrowThicknessF  = 3\nres_vc@vcMinDistanceF         = 0.03\nres_vc@vcRefLengthF           = 0.03\nres_vc@vcRefAnnoOn            = True\nres_vc@vcRefMagnitudeF           = 16\nres_vc@vcRefAnnoString1          = \"16m/s\"\nres_vc@vcRefAnnoSide             = \"Top\"\nres_vc@vcRefAnnoString2On        = False\nres_vc@vcRefAnnoPerimOn          = False\nres_vc@vcRefAnnoOrthogonalPosF   = -0.02\nres_vc@vcRefAnnoParallelPosF     = 0.999\n;res_vc@vcRefAnnoBackgroundColor = \"White\"\nres_vc@vcVectorDrawOrder         = \"PostDraw\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  ;print(time(it,0)+\" \"+time(it,1))\n  rundate = yyyyddd_to_yyyymmdd( time(it,0) )\n  runtime = hr( tointeger(time(it,1)/10000) )\n\n  site = readAsciiTable(obs_dir+\"/AQF5X_Hourly_\"+rundate+runtime+\".dat\",1,\"string\",1)\n  nrows = dimsizes(site)\n  sitename = str_get_field(site,1,\",\")\n  sitelat = stringtofloat(str_get_field(site,2,\",\"))\n  sitelon = stringtofloat(str_get_field(site,3,\",\"))\n  O3_obs = stringtofloat(str_get_field(site,4,\",\"))\n\n  obslon = sitelon(:,0)\n  obslat = sitelat(:,0)\n  obsO3 = O3_obs(:,0)\n\n  npts = nrows(0)\n\n  obsO3@_FillValue = -999.\n\n;--- levels for dividing\n  levels_O3  = ispan(0,80,4)\n\n  nlevels = dimsizes(levels_O3)\n\n  colors  = span_color_rgba(\"WhiteBlueGreenYellowRed\",nlevels+1)\n\n  num_distinct_markers = nlevels+1        ; number of distinct markers\n  lat_O3 = new((/num_distinct_markers,npts/),float)\n  lon_O3 = new((/num_distinct_markers,npts/),float)\n  lat_O3 = -999\n  lon_O3 = -999\n\n\n;\n; Group the points according to which range they fall in. At the\n; same time, create the label that we will use later in the labelbar\n;\n  do i = 0, num_distinct_markers-1\n    if (i.eq.0) then\n      indexes_O3 = ind(obsO3(:).lt.levels_O3(0))\n    end if\n    if (i.eq.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.max(levels_O3))\n    end if\n    if (i.gt.0.and.i.lt.num_distinct_markers-1) then\n      indexes_O3 = ind(obsO3(:).ge.levels_O3(i-1).and.obsO3(:).lt.levels_O3(i))\n    end if\n\n;\n; Now that we have the set of indexes whose values fall within\n; the given range, take the corresponding lat/lon values and store\n; them, so later we can color this set of markers with the appropriate\n; color.\n;\n    if (.not.any(ismissing(indexes_O3))) then\n      npts_range_O3 = dimsizes(indexes_O3)   ; # of points in this range.\n\n      lat_O3(i,0:npts_range_O3-1) = obslat(indexes_O3)\n      lon_O3(i,0:npts_range_O3-1) = obslon(indexes_O3)\n  ;print(\"O3: \"+npts_range_O3)\n    end if\n\n\n    delete(indexes_O3)            ; Necessary b/c \"indexes\" may be a different\n  end do\n\n  lat_O3@_FillValue = -999\n  lon_O3@_FillValue = -999\n\n  gsres               = True\n  gsres@gsMarkerIndex = 16          ; Use filled dots for markers.\n\n  hollowres           = True\n  hollowres@gsMarkerIndex    = 4\n  hollowres@gsMarkerColor    = \"black\"\n  hollowres@gsMarkerSizeF    = 0.008\n\n;;;;;;;;;   Plot Ozone\n  pname=plot_dir+\"/OBS-FORECAST_O3_\"+rundate+runtime\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  pmid_O3 = new(num_distinct_markers,graphic)\n  hollow_O3 = new(num_distinct_markers,graphic)\n\n  cnres@tiMainString =  pdate+\" \"+runtime+\" UTC O~B~3~N~ (ppbV)\"\n  cnres@cnLevelSelectionMode = \"ManualLevels\"\n  cnres@cnMinLevelValF          = 0.\n  cnres@cnMaxLevelValF          = 80\n  cnres@cnLevelSpacingF         = 4\n\n  ;plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  map = gsn_csm_map(wks,mpres)\n  contour = gsn_csm_contour(wks,o3(it,:,:),cnres)\n  vector  = gsn_csm_vector(wks,UV10(0,it,:,:),UV10(1,it,:,:),res_vc)\n  overlay(map,contour)\n  overlay(map,vector)\n\n  pmid = new(num_distinct_markers,graphic)\n  hollow = new(num_distinct_markers,graphic)\n  do i = 0, num_distinct_markers-1\n    if (.not.ismissing(lat_O3(i,0)))\n      gsres@gsMarkerColor      = colors(i,:)\n      gsres@gsMarkerSizeF      = 0.008\n      gsres@gsMarkerThicknessF = 1\n       pmid(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),gsres)\n       hollow(i) = gsn_add_polymarker(wks,vector,lon_O3(i,:),lat_O3(i,:),hollowres)\n    end if\n  end do\n\n  draw(map)\n  frame(wks)\n  delete(wks)\n  delete(pmid_O3)\n  delete(hollow_O3)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\n\n\n  delete(pmid)\n  delete(hollow)\n  delete(site)\n  delete(sitename)\n  delete(sitelat)\n  delete(sitelon)\n  delete(O3_obs)\n  delete(obslon)\n  delete(obslat)\n  delete(obsO3)\n  delete([/lon_O3,lat_O3/])\n\nend do\ndelete(res)\n\n;/\n\nend\nprint(\"ncl business is done\")\nexit\nEOF\n\ndays_back=7\n# for regular workflow run, assign false\n# for refreshing all the previous generated gifs, assign true\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') \n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d')\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') \n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d')\n  \n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/\"Airnow_\"$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\n\n  convert -delay 100 $cmaq_folder/plots/OBS*.png $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif\n\n  # copy the result files to permanent location\n  echo \"Copy Airnow gif to permanent location\"\n  cp $cmaq_folder/plots/\"Airnow_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n\ndone\n\nif [ $? -eq 0 ]; then\n    echo \"Generating AirNow images/gif Completed Successfully\"\n\techo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nelse\n    echo \"Generating AirNow images/gif Failed!\"\n    echo \"Removing ncl file: geoweaver_plot_daily_O3_Airnow.ncl...\"\n\t#rm $cmaq_folder/geoweaver_plot_daily_O3_Airnow.ncl\nfi\n\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h76ld0",
  "name" : "cmaq_ai_utils",
  "description" : null,
  "code" : "# All the utility functions that most steps in CMAQ-AI need\n# this file should not contain any direct call of function\n# it should be dedicated to define functions or variables\n\nimport xarray as xr\nimport pandas as pd\nimport glob, os\nimport numpy as np\nfrom pathlib import Path\nfrom datetime import date, datetime, timedelta\nimport matplotlib.pyplot as plt\n\n# home directory\nhome = str(Path.home())\ncmaq_folder = \"/groups/ESS/zsun/cmaq/\" # change if you want to use your own folder\nmodel_folder = \"/groups/ESS3/zsun/cmaq/models/\"\n\n\ndef get_days_list(sdate, edate):\n  days=[]\n  \n  delta = edate - sdate       # as timedelta\n\n  for i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n  # add one more day\n  one_more_day = sdate + timedelta(days=delta.days + 1)\n  list_day=one_more_day.strftime('%Y%m%d')\n  days.append(list_day)\n  \n  return days\n\ndef get_days_list_for_prediction(sdate, edate):\n  \"\"\"\n  This will return the day before sdate to enddate,for example, if sdate = 10142022, edate = 10152022. This function will return [10132022, 10142022, 10152022].\n  \"\"\"\n  days=[]\n  \n  delta = edate - sdate       # as timedelta\n\n  for i in range(delta.days + 1):\n    day = sdate + timedelta(days=i)\n    list_day=day.strftime('%Y%m%d')\n    days.append(list_day)\n  \n  return days\n\ndef create_and_clean_folder(folder_path):\n  os.makedirs(folder_path, exist_ok=True)\n  # clean all files inside the folder\n  for f in os.listdir(folder_path):\n    os.remove(os.path.join(folder_path, f))\n\ndef remove_file(file_path):\n  print(f'remove old files{file_path}')\n  if os.path.exists(file_path):\n    os.remove(file_path)\n    \ndef turn_2_digits(a):\n  return f\"{a:02}\"\n\n#today = datetime.today()\n#edate = today - timedelta(days=1)\n#sdate = today - timedelta(days=4)\n\n#print(get_days_list_for_prediction(sdate, edate))\ndays_back = 60\ndays_forward = 7\ntoday = datetime.today()\nedate = today + timedelta(days=days_forward)\nsdate = today - timedelta(days=days_back)\ndays = get_days_list_for_prediction(sdate, edate)\n# cmaq_folder = \"/Users/uhhmed/localCMAQ\" # change if you want to use your own folder\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "s6hbic",
  "name" : "train_cmaq_directly_no_slurm",
  "description" : null,
  "code" : "# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom cmaq_ai_utils import *\n\n# importing data\ncolnames=[\"Latitude_x\",\"Longitude_x\",\"CMAQ12KM_O3(ppb)\",\"CMAQ12KM_NO2(ppb)\",\"CMAQ12KM_CO(ppm)\",\"CMAQ_OC(ug/m3)\",\"PRSFC(Pa)\",\"PBL(m)\",\"TEMP2(K)\",\"WSPD10(m/s)\",\"WDIR10(degree)\",\"RGRND(W/m2)\",\"CFRAC\",\"month\",\"day\",\"hours\",\"Lat_airnow\",\"Lon_airnow\",\"Lat_cmaq\",\"Lon_cmaq\",\"StationID\",\"Latitude_y\",\"Longitude_y\",\"AirNOW_O3\"] \nfinal=pd.read_csv(f'{cmaq_folder}/training_one_year.csv', names=colnames, header=None)\nprint(final.head())\nfinal=final.dropna()\n#print(\"shape before: \", final.shape)\n#final = final.loc[(final['CMAQ12KM_O3(ppb)']-final['AirNOW_O3'])/final['AirNOW_O3']<0.05]\n#print(\"shape 1 after: \", final.shape)\n#final = final.loc[(final['CMAQ12KM_O3(ppb)']-final['AirNOW_O3'])/final['AirNOW_O3']>-0.05]\n#print(\"shape 2 after: \", final.shape)\n\nfinal['time_of_day'] = (final['hours'] % 24 + 4) // 4\n\n# Make coords even more coarse by rounding to closest multiple of 5 \n# (e.g., 40, 45, 85, 55)\n#final['Latitude_ExtraCoarse'] = 0.1 * round(final['Latitude_x']/0.1)\n#final['Longitude_ExtraCoarse'] = 0.1 * round(final['Longitude_x']/0.1)\n\n\n\ncreate_and_clean_folder(f\"{cmaq_folder}/models/\")\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x','Lat_airnow','Lon_airnow', 'Lat_cmaq', 'Lon_cmaq', 'Latitude_y', 'Longitude_y', 'StationID'],axis=1)\nprint(\"input shape:\", X.shape)\nprint(\"input variables: \", X.columns)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = f'{cmaq_folder}/models/rf_pycaret_o3_one_year_1.5g.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nprint(f\"Model is trained and saved to {filename}\")",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pvzabv",
  "name" : "show_cmaq_gif",
  "description" : null,
  "code" : "{\"operation\":\"ShowResultMap\",\"params\":[{\"name\":\"FilePath\",\"value\":\"/groups/ESS/zsun/cmaq/plots/Map_20220810.gif\"}]}",
  "lang" : "builtin",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "8i9ptn",
  "name" : "prepare_update_cell_csv",
  "description" : null,
  "code" : "# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport glob, os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\nfrom scipy import spatial\n\n# from geopy import distance\nfrom math import radians, cos, sin, asin, sqrt\nimport time\n\n\ndef match_closest_airnow_with_gridCell():\n  \"\"\"\n  Match all airnow stations to closest grid cell.\n  \"\"\"\n  print(\"Starting!\")\n  remove_file(f'{cmaq_folder}/updated_station_to_cell.csv')\n  testing_path = f'{cmaq_folder}/testing_input_hourly'\n  all_hourly_files = sorted(glob.glob(os.path.join(testing_path, \"test_data_*.csv\")))\n  print(\"reading stations csv\")\n  stations = pd.read_csv(f'{cmaq_folder}/AQF5X_Hourly_2022091304.dat', sep=', ', skiprows=1, names=['AQSID', 'Latitude', 'Longitude', 'OZONE(ppb)', 'NO2(ppb)', 'CO(ppm)', 'PM25(ug/m3)', 'SO2(ppb)', 'PM10(ug/m3)'])\n\n  # stations = stations.replace(',','', regex=True)\n  print(stations.columns)\n  print(stations[['Latitude', 'Longitude']])\n  station_locations = stations[['Latitude', 'Longitude']].astype(float).values\n  print(\"station_locations - \", station_locations)\n  print(\"station_locations.shape: \", station_locations.shape)\n  print(\"reading testing data csv\")\n\n  testing_df = pd.read_csv(all_hourly_files[0]) # just pick the first one to generate the mapping\n  print(testing_df['YYYYMMDDHH'].values[0])\n\n  closest_stations = []\n  cmaq_cell_array = []\n  final_mapping_array = []\n  for j, cmaq in testing_df.iterrows():\n    cmaq_location = [cmaq['Latitude'], cmaq['Longitude']]\n    cmaq_cell_array.append(cmaq_location)\n  \n  print(\"cmaq_cell_array.shape: \", len(cmaq_cell_array))\n  \n  for station_loc in station_locations:\n    distance,index = spatial.KDTree(cmaq_cell_array).query(station_loc)\n    if distance > 0.2:\n      continue\n    closest_cell = cmaq_cell_array[index]\n    new_row = [station_loc[0], station_loc[1], closest_cell[0], closest_cell[1]]\n    final_mapping_array.append(new_row)\n\n  print(\"final_mapping_array length: \", len(final_mapping_array))\n  closest = pd.DataFrame(final_mapping_array, columns=[\"Lat_airnow\", \"Lon_airnow\", \"Lat_cmaq\", \"Lon_cmaq\"])\n  #closest.drop_duplicates().reset_index(drop=True)\n  print(\"Saving fixed_station_cmaq_location.csv...\")\n  closest.to_csv(f'{cmaq_folder}/updated_station_to_cell.csv',index=False)\n\n  \n\ndef prepare_update_grid_cells_with_distance(station_distance=0.2):\n  \"\"\"\n  Get all grid cells within the specified distance around airnow stations\n  args: station_distance, default: 50km (0.2 degrees)\n  \"\"\"\n  print(\"Starting!\")\n  testing_path = f'{cmaq_folder}/testing_input_hourly'\n  all_hourly_files = sorted(glob.glob(os.path.join(testing_path, \"test_data_*.csv\")))\n  print(\"reading stations csv\")\n  airnow_obs_path = '/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X'\n  stations = pd.read_csv(f'{cmaq_folder}/station_cmaq_location.csv')\n  print(\"reading testing data csv\")\n\n  testing_df = pd.read_csv(all_hourly_files[0])\n  print(testing_df['YYYYMMDDHH'].values[0])\n  file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n  print(\"copying testing_df to new_df\")\n  new_df = testing_df.copy()\n  new_df.drop(new_df.index, inplace=True)\n  print(testing_df.shape, new_df.shape)\n\n  for j, cmaq in testing_df.iterrows():\n    if j % 1000 == 0:\n  \t  print(\"Looping through: \", j)\n    for i, station in stations.iterrows():\n      #print(\"inner-Looping through: \", i)\n      airnow_stations = (station['Latitude_y'], station['Longitude_y'])\n      prediction_location = (cmaq['Latitude'], cmaq['Longitude'])\n        \n      if (station['Latitude_y'] < cmaq['Latitude'] + station_distance) and (station['Latitude_y'] > cmaq['Latitude'] - station_distance) and (station['Longitude_y'] < cmaq['Longitude'] + station_distance) and (station['Longitude_y'] > cmaq['Longitude'] - station_distance):\n        new_df.loc[j] = cmaq\n        break\n  new_df.to_csv(f'{cmaq_folder}/prediction_files/update_cell.csv',index=False)\n\ndef prepare_update_grid_cells_with_distance(station_distance=0.2):\n  \"\"\"\n  Get all grid cells within the specified distance around airnow stations\n  args: station_distance, default: 50km (0.2 degrees)\n  \"\"\"\n  print(\"Starting!\")\n  testing_path = f'{cmaq_folder}/testing_input_hourly'\n  all_hourly_files = sorted(glob.glob(os.path.join(testing_path, \"test_data_*.csv\")))\n  print(\"reading stations csv\")\n  stations = pd.read_csv(f'{cmaq_folder}/station_cmaq_location.csv')\n  print(\"reading testing data csv\")\n\n  testing_df = pd.read_csv(all_hourly_files[0])\n  print(testing_df['YYYYMMDDHH'].values[0])\n  file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n  print(\"copying testing_df to new_df\")\n  new_df = testing_df.copy()\n  new_df.drop(new_df.index, inplace=True)\n  print(testing_df.shape, new_df.shape)\n\n  for j, cmaq in testing_df.iterrows():\n    if j % 1000 == 0:\n  \t  print(\"Looping through: \", j)\n    for i, station in stations.iterrows():\n      #print(\"inner-Looping through: \", i)\n      airnow_stations = (station['Latitude_y'], station['Longitude_y'])\n      prediction_location = (cmaq['Latitude'], cmaq['Longitude'])\n        \n      if (station['Latitude_y'] < cmaq['Latitude'] + station_distance) and (station['Latitude_y'] > cmaq['Latitude'] - station_distance) and (station['Longitude_y'] < cmaq['Longitude'] + station_distance) and (station['Longitude_y'] > cmaq['Longitude'] - station_distance):\n        new_df.loc[j] = cmaq\n        break\n  new_df.to_csv(f'{cmaq_folder}/prediction_files/update_cell.csv',index=False)\n  \n#prepare_update_grid_cells(0.2)\nmatch_closest_airnow_with_gridCell()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "iicy7w",
  "name" : "generate_images_ncl",
  "description" : null,
  "code" : "#!/bin/bash\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\n\n# generate images and gif from the NetCDF files\n\necho \"cmaq_folder=\"${cmaq_folder}\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\nmkdir \"${cmaq_folder}/plots\"\n\n\nexport postdata_dir=$cmaq_folder\"/prediction_nc_files\"\nexport mcip_dir=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km\"\nexport dir_graph=\"${cmaq_folder}/plots\"\n\necho \"Loading NCL\"\nsource /home/zsun/.bashrc\nmodule load ncl\necho \"Loaded NCL\"\n\nrm ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\n\necho \"Drafting \"${cmaq_folder}/geoweaver_plot_daily_O3.ncl\ncat <<EOF >> ${cmaq_folder}/geoweaver_plot_daily_O3.ncl\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\n\nprint(\"NCL script successfully begin: \")\n\ndate = getenv(\"YYYYMMDD_POST\")\nd1 = getenv(\"stdate_post\")\nd2 = getenv(\"eddate_post\")\n\ndFile1 = getenv(\"stdate_file\")\ndFile2 = getenv(\"eddate_file\")\n\n;print(\"Passed Date: \"+date)\n\n;aconc_dir = getenv(\"postdata_dir\")\ngrid_dir = getenv(\"mcip_dir\")\nplot_dir = getenv(\"dir_graph\")\n\nprint(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\")\ncdf_file1 = addfile(\"/groups/ESS/zsun/cmaq/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+dFile1+\"_ML_extracted.nc\",\"r\")\ncdf_file= addfile(grid_dir+\"/GRIDCRO2D_\"+date+\".nc\",\"r\")\n\nptime = (/\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"00\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\"/)\n\ntime = cdf_file1->TFLAG(:,0,:)\no3 = cdf_file1->O3(:,0,:,:) ;ppb\n;pm25 = cdf_file1->PM25_TOT(:,0,:,:)\n\n\nnt = dimsizes(o3(:,0,0))\nny = dimsizes(o3(0,:,0))\nnx = dimsizes(o3(0,0,:))\n\nprint(nt+\" \"+ny+\" \"+nx)\nprint(max(o3))\nprint(min(o3))\nprint(avg(o3))\n\n;print(max(pm25))\n;print(min(pm25))\n;print(avg(pm25))\n\n;print(time)\n\nlat = cdf_file->LAT(0,0,:,:)\nlon = cdf_file->LON(0,0,:,:)\n\no3@lat2d = lat\no3@lon2d = lon\n\nres = True\nres@gsnMaximize = True                ; maximize pot in frame\nres@gsnFrame = False               ; don't advance frame\nres@gsnDraw = False\n;res@gsnSpreadColors = True\nres@lbLabelAutoStride = True\n;res@lbBoxLinesOn = False\nres@pmLabelBarHeightF = 0.1\nres@pmLabelBarWidthF = 0.5\nres@cnFillOn=True\n;res@cnMonoFillPattern=True\n;res@cnMonoLineColor=True\nres@cnLinesOn=False\n;res@pmLabelBarDisplayMode=\"never\"\nres@gsnLeftString  = \"\";\nres@gsnRightString = \"\"\n\nres@mpLimitMode = \"LatLon\"\nres@mpMinLonF = -120 ;min(lon)+0.2\nres@mpMaxLonF = -70 ;max(lon)-0.2\nres@mpMinLatF = 25 ;min(lat)+0.05\nres@mpMaxLatF = 50 ;max(lat)-0.05\nres@mpDataBaseVersion = \"MediumRes\"\n;res@tiMainString = times(it)\nres@mpDataBaseVersion       = \"MediumRes\"\nres@mpDataSetName           = \"Earth..4\"\nres@mpAreaMaskingOn         = True\nres@mpOutlineBoundarySets = \"GeophysicalAndUSStates\"\nres@mpOutlineSpecifiers=\"United States : States\"\nres@mpLandFillColor         = \"white\"\nres@mpInlandWaterFillColor  = \"white\"\nres@mpOceanFillColor        = \"white\"\nres@mpGeophysicalLineColor    = \"Black\"\nres@mpGeophysicalLineThicknessF = 1.5\n\n;res@gsnSpreadColors         = True\nres@lbLabelAutoStride       = True\nres@lbLabelFont             = 25\nres@tiXAxisFont             = 25\nres@pmTickMarkDisplayMode   = \"Always\"\nres@tmXBLabelFont           = 25\nres@tmXBLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmYLLabelFont           = 25\nres@tmYLLabelFontHeightF    = 0.013\nres@tmXBLabelDeltaF         = -0.5\nres@tmXTLabelsOn            = False\nres@tmXTLabelFont           = 25\nres@tmXTLabelFontHeightF    = 0.013\nres@tmYRLabelsOn            = False\nres@tmYRLabelFont           = 25\nres@tmYRLabelFontHeightF    = 0.013\n\n\nres@mpProjection           = \"LambertConformal\" ;\"CylindricalEquidistant\"\nres@mpLambertParallel1F    = 33.\nres@mpLambertParallel2F    = 45.\nres@mpLambertMeridianF     = -98.\n\nres@cnLevelSelectionMode = \"ManualLevels\"\nres@cnMinLevelValF          = 0.\nres@cnMaxLevelValF          = 80\nres@cnLevelSpacingF         = 4\n\nres@txFont   = \"times-roman\"\nres@tiMainFont   = \"times-roman\"\n\ndo it = 0, nt-1\n  if (it .lt. 12) then\n    pdate=d1\n  else\n    pdate=d2\n  end if\n\n  pname=plot_dir+\"/testPlot_\"+pdate+\"_\"+ptime(it)\n  wks = gsn_open_wks(\"png\",pname)\n  gsn_define_colormap(wks, \"WhiteBlueGreenYellowRed\")\n\n  res@tiMainString = pdate+\" \"+ptime(it)+\" UTC O~B~3~N~ Forecast (ppbV)\"\n  plot = gsn_csm_contour_map(wks,o3(it,:,:),res)\n  draw(plot)\n  frame(wks)\n  delete(wks)\n  system(\"composite -geometry 100x70+900+900 /groups/ESS/zsun/cmaq/mason-logo-green.png \"+pname+\".png \"+pname+\".png\")\nend do\ndelete(res)\n\nend\nexit\nEOF\n\necho \"Start to run the NCL script: \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho \"ncl \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\necho $(date -d '1 day ago' '+%Y%m%d')\n\ndays_back=7\n\nforce=false\n\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date `date -d \"-2 day ${1}\" +%Y%m%d`\n  #export YYYYMMDD_POST='20220806'\n  export stdate_post=$(date -d $begin_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export stdate_post='2022-08-06'\n  export eddate_post=$(date -d $end_day' day ago' '+%Y-%m-%d') #This needs to be auto date\n  #export eddate_post='2022-08-08'\n\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export stdate_file='20220806'\n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d') #This needs to be auto date\n  #export eddate_file='20220808'\n  stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d')\n  echo \"stdate_file=\"$stdate_file\n  # determine if the prediction netcdf is there\n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_gif_file=$permanent_location/gifs/Map_$YYYYMMDD_POST.gif\n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_gif_file\" ]; then\n      echo \"$predict_gif_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_gif_file doesn't exist. Generating...\"\n    fi\n  else\n    echo \"force to regenerate $predict_gif_file..\"\n  fi\n  \n  rm -rf $cmaq_folder/plots/* # clean everything in the folder first\n\n  ncl $cmaq_folder/geoweaver_plot_daily_O3.ncl\n\n  echo \"Finished \"$cmaq_folder\"/geoweaver_plot_daily_O3.ncl\"\n\n  # convert -delay 100 *.png 20220613_20220614.gif\n  convert -delay 100 $cmaq_folder/plots/testPlot*.png $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif\n  echo \"Converted images to gif\"\n\n  # cp the results to permanent location\n  cp $predict_nc_file $permanent_location/netcdfs/\n  cp $cmaq_folder/plots/\"Map_\"$YYYYMMDD_POST.gif $permanent_location/gifs/\n  echo \"Moved the generated netcdfs and gifs to permanent locations\"\n  \ndone\n\n\n\nif [ $? -eq 0 ]; then\n    echo \"Generating images/gif Completed Successfully\"\nelse\n    echo \"Generating images/gif Failed!\"\nfi\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "nndpw6",
  "name" : "collect_cmaq_gifs",
  "description" : null,
  "code" : "#!/bin/bash\n\ndays_back=40\n\npermanent_location=\"/groups/ESS3/zsun/cmaq/ai_results/\"\ncmaq_gif_location=\"/groups/ESS/share/projects/SWUS3km/graph/12km/\"\n\necho \"start to traverse \"${cmaq_gif_location}\n\nfor i in $(seq 0 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i))\n  # Setting env variables\n  YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  #/groups/ESS/share/projects/SWUS3km/graph/12km/20221108/FORECAST_O3_20221108.gif\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/\"FORECAST_O3_\"$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  cp -u $cmaq_gif_location/$YYYYMMDD_POST/obsoverlay/gif/OBS-FORECAST_O3_$YYYYMMDD_POST.gif $permanent_location/gifs/ || true\n  \ndone\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gn54f0",
  "name" : "upload_results",
  "description" : null,
  "code" : "#!/bin/bash\n\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"upload_results_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J upload_results       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 1               # Number of CPUs per task (threads)\n#SBATCH --mem=10G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-02:00         # Runtime in D-HH:MM format\n\n# this step will only upload the gifs to the web server for visualization\n# the netcdfs are too big to move.\n\necho \"Saving file name list to filelist.txt\"\nfind /groups/ESS3/zsun/cmaq/ai_results/gifs/  -printf \"%f\\n\" > /groups/ESS3/zsun/cmaq/ai_results/gifs/filelist.txt\n\necho \"Copying evaluation txt to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS3/zsun/cmaq/ai_results/evaluation/* zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\n\n\necho \"Copying animation gifs to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS3/zsun/cmaq/ai_results/gifs/* zsun@129.174.131.229:/var/www/html/cmaq_site/gifs/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS3/zsun/cmaq/ai_results/gifs/* zsun@129.174.131.229:/var/www/html/cmaq_site/gifs/\n  \necho \"Copying CMAQ evaluation metrics to public server..\"\n#scp -i /home/zsun/.ssh/id_geobrain_no.pem /groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/\nrsync -u -e \"ssh -i /home/zsun/.ssh/id_geobrain_no.pem\" -avz /groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt zsun@129.174.131.229:/var/www/html/cmaq_site/evaluation/alleva_12km_o3_fore.txt\n\necho \"Done\"\n\n\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "fsk7f2",
  "name" : "evaluate_prediction_ncl",
  "description" : null,
  "code" : "#!/bin/bash\n\n\n# evaluate the prediction accuracy\n\ncmaq_folder=\"/groups/ESS/zsun/cmaq\"\npermanent_eval_folder=\"/groups/ESS3/zsun/cmaq/ai_results/evaluation/\"\nmkdir -p $cmaq_folder/results/\nmkdir -p $permanent_eval_folder\nchmod +x $cmaq_folder/results/ -R\nchmod +x $permanent_eval_folder -R\n\n#export YYYYMMDD_POST='20220806'\n#export stdate_file='20220806'\n#export eddate_file='20220808'\n\n#This needs to be auto date\n\nexport dx=12000\n\nsource /home/zsun/.bashrc\nmodule load ncl\n\nrm $cmaq_folder/geoweaver_eva_daily_O3.ncl\n\ncat <<EOF >> $cmaq_folder/geoweaver_eva_daily_O3.ncl\n\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/gsn_csm.ncl\"\nload \"/opt/sw/spack/apps/linux-centos8-cascadelake/gcc-9.3.0-openmpi-4.0.4/ncl-6.6.2-fr/lib/ncarg/nclscripts/csm/contributed.ncl\"\n\nsetvalues NhlGetWorkspaceObjectId()\n\"wsMaximumSize\": 600000000\nend setvalues\n\nbegin\nsdate=getenv(\"YYYYMMDD_POST\")\nwfname=getenv(\"wfname\")\nobs_dir=getenv(\"obs_dir_NCL\")\nofname=getenv(\"ofname\")\nmod_dir=getenv(\"postdata_dir\")\nmfname=getenv(\"mfname\")\ndkm=tofloat(getenv(\"dx\"))\ngrid_fname=(getenv(\"grid_fname\"))\n\nmaxdist=dkm/90000.0*1.414\n;maxarea=0.25\n;thd=70\n;maxdist=0.13*1.414\nmaxarea=0.25\nthd=35.0\n\n;-----read model lat lon------\n;read lat lon\nf1 = addfile(grid_fname,\"r\")\nmlat = f1->LAT(0,0,:,:)\nmlon = f1->LON(0,0,:,:)\ndelete(f1)\nmlat1d = ndtooned(mlat)\nmlon1d = ndtooned(mlon)\ndelete([/mlat,mlon/])\n\n;-----read cmaq results-----\nf2 = addfile(mod_dir+mfname,\"r\")\nmO3 = f2->O3(:,0,:,:) ;ppb\n\n\nnt = dimsizes(mO3(:,0,0))\nny = dimsizes(mO3(0,:,0))\nnx = dimsizes(mO3(0,0,:))\n\nm8O3 = new((/17,ny,nx/),\"double\")\nm8maxO3 = new((/ny,nx/),\"double\")\n\ndo ih=0,16\n  m8O3(ih,:,:)=dim_avg_n(mO3(ih:ih+7,:,:),0)\nend do\nm8maxO3 = dim_max_n(m8O3,0) ;type double\nmO31d_d=ndtooned(m8maxO3) ; type double\nmO31d=tofloat(mO31d_d)\n\ndelete([/f2,mO3,m8O3,m8maxO3/])\n\n;-----read obs-----\nsyyyy1=str_get_cols(sdate,0,3)\nsmm1=str_get_cols(sdate,4,5)\nsdd1=str_get_cols(sdate,6,7)\n\nymd=jul2greg(greg2jul(tointeger(syyyy1),tointeger(smm1),tointeger(sdd1),-1)+1)\nsyyyy2=tostring_with_format(ymd(0),\"%0.4i\")\nsmm2=tostring_with_format(ymd(1),\"%0.2i\")\nsdd2=tostring_with_format(ymd(2),\"%0.2i\")\n\ntolat=(/-999.0/) ;set the first data to 0\ntolon=tolat\ntoO3=tolat\n\ndo ih=12,35\n  if (ih.lt.24) then\n    shh=tostring_with_format(ih,\"%0.2i\")\n    syyyy=syyyy1\n    smm=smm1\n    sdd=sdd1\n  else\n    shh=tostring_with_format(ih-24,\"%0.2i\")\n    syyyy=syyyy2\n    smm=smm2\n    sdd=sdd2\n  end if\n  data=asciiread(obs_dir+ofname+syyyy+smm+sdd+shh+\".dat\",-1,\"string\")\n  xx=array_append_record(tolat,stringtofloat(str_get_field(data(1::), 2,\",\")),0)\n  yy=array_append_record(tolon,stringtofloat(str_get_field(data(1::), 3,\",\")),0)\n  zz=array_append_record(toO3,stringtofloat(str_get_field(data(1::), 4,\",\")),0)\n  delete([/tolat,tolon,toO3/])\n  tolat=xx\n  tolon=yy\n  toO3=zz\n  delete([/xx,yy,zz/])\n  delete(data)\nend do\n\ntoO3@_FillValue = -999.0\n\n;-----calculate max ave 8 hour o3-----\noflag=tolat*0+1\naa=ind((oflag.gt.0).and.(toO3.ge.0))\nii=0\nprint(\"8h start\")\nif (any(ismissing(aa))) then\n  iflag=0\nelse\n  iflag=1\n  olat=(/tolat(aa(0))/)\n  olon=(/tolon(aa(0))/)\n  oO3=(/-999.0/)\n  o8O3 = new(17,\"float\")\n  o8O3 = -999.0\nend if\ndelete(aa)\ndo while (iflag.gt.0)\n  aa=ind((tolat.eq.olat(ii)).and.(tolon.eq.olon(ii)).and.(toO3.ge.0))\n  oflag(aa)=0\n  if (dimsizes(aa).eq.24) then  ; calculate 24 h, so calculate 8hr ozone here\n    do ih = 0, 16\n      o8O3(ih) = avg(toO3(aa(ih:ih+7)))\n    end do\n    oO3(ii)=max(o8O3)\n  end if\n  o8O3 = -999.0\n  delete(aa)\n  aa=ind((oflag.gt.0).and.(toO3.ge.0))\n  if (any(ismissing(aa))) then\n    iflag=0\n  else\n    xx=array_append_record(olat,(/tolat(aa(0))/),0)\n    yy=array_append_record(olon,(/tolon(aa(0))/),0)\n    zz=array_append_record(oO3,(/-999.0/),0)\n    delete([/olat,olon,oO3/])\n    olat=xx\n    olon=yy\n    oO3=zz\n    delete([/xx,yy,zz/])\n    ii=ii+1\n  end if\n  delete(aa)\nend do\nprint(\"obs 8hour max end\")\naa=ind(oO3.ge.0)\nnobs=dimsizes(aa)\nolat24=olat(aa)\nolon24=olon(aa)\noO324=oO3(aa)\n;print(\"oO324: \"+oO324)\ndelete([/aa,olat,olon,oO3/])\nmO324=oO324*0-999.0\n;print(\"mO324: \"+mO324)\n;print(\"mO31d: \"+mO31d)\nareaa=oO324*0-999.0\nareab=areaa\naread=areaa\n;print(\"areaa: \"+areaa)\n;print(\"areab: \"+areab)\n;print(\"aread: \"+aread)\n\n;-----find model point-----\ndo in=0,nobs-1\n  dis=sqrt((mlat1d-olat24(in))^2+(mlon1d-olon24(in))^2)\n  aa=minind(dis)\n  ;print(in+\" \"+aa)\n  if (dis(aa).lt.maxdist) then\n    mO324(in)=mO31d(aa)\n    cc=ind((mlat1d.ge.(olat24(in)-maxarea)).and.(mlat1d.le.(olat24(in)+maxarea)).and.\\\n           (mlon1d.ge.(olon24(in)-maxarea)).and.(mlon1d.le.(olon24(in)+maxarea)))\n    areaa(in)=0\n    areab(in)=0\n    if (oO324(in).ge.thd) then\n      aread(in)=0\n      if (max(mO31d(cc)).ge.thd) then\n        areab(in)=1\n      else\n        aread(in)=1\n      end if\n    else\n      bb=ind((olat24.ge.(olat24(in)-maxarea)).and.(olat24.le.(olat24(in)+maxarea)).and.\\\n             (olon24.ge.(olon24(in)-maxarea)).and.(olon24.le.(olon24(in)+maxarea)))\n      if (max(mO31d(aa)).ge.thd) then\n        if (max(oO324(bb)).ge.thd) then\n          areaa(in)=0\n        else\n          areaa(in)=1\n        end if\n      else\n        areaa(in)=0\n      end if\n      delete(bb)\n    end if\n    delete(cc)\n  end if\n  delete(aa)\nend do\n\n;-----cal rmse corr nme nmb me mb-----\ntt=ind((mO324.ge.0).and.(oO324.ge.0))\n\nif (any(ismissing(tt))) then\n  rmse=-999.0\n  corr=-999.0\n  nmb=-999.0\n  nme=-999.0\n  me=-999.0\n  mb=-999.0\nelse\n  rmse=dim_rmsd_n(oO324(tt),mO324(tt),0)\n  corr=esccr(oO324(tt),mO324(tt),0)\n  nmb=sum((mO324(tt)-oO324(tt)))/sum(oO324(tt))\n  nme=sum(abs(oO324(tt)-mO324(tt)))/sum(oO324(tt))\n  me=avg(abs(oO324(tt)-mO324(tt)))\n  mb=avg((mO324(tt)-oO324(tt)))\nend if\n;-----cal ah afar-----\n;print(\"areaa: \"+areaa)\n;print(\"areab: \"+areab)\naa=ind((areaa+areab).gt.0)\nbb=ind((aread+areab).gt.0)\n;print(\"aa: \"+aa)\n;print(\"bb: \"+bb)\nif (any(ismissing(aa))) then\n  afar=0.\nelse\n  afar=tofloat(sum(areaa(aa)))/tofloat(sum(areab(aa))+sum(areaa(aa)))*100\nend if\ndelete(aa)\nif (any(ismissing(bb))) then\n  ah=-999.0\nelse\n  ah=tofloat(sum(areab(bb)))/tofloat(sum(areab(bb))+sum(aread(bb)))*100\nend if\ndelete(bb)\nwrite_table(wfname,\"a\",[/sdate,dimsizes(tt),avg(oO324(tt)),avg(mO324(tt)),rmse,corr,nmb,nme,mb,me,ah,afar/],\\\n            \"%s,%i,%f,%f,%f,%f,%f,%f,%f,%f,%f,%f\")\ndelete(tt)\nend\n\nexit\nEOF\n\ndays_back=7\nforce=true\nfor i in $(seq 1 $days_back)\ndo\n  end_day=$i\n  echo \"$end_day days ago\"\n  begin_day=$((i+1))\n  # Setting env variables\n  export YYYYMMDD_POST=$(date -d $begin_day' day ago' '+%Y%m%d')\n  export stdate_file=$(date -d $begin_day' day ago' '+%Y%m%d') #This needs to be auto date\n  export eddate_file=$(date -d $end_day' day ago' '+%Y%m%d') #This needs to be auto date\n  export wfname=$cmaq_folder\"/results/geoweaver_evalution_\"$YYYYMMDD_POST\"_results.txt\"\n\n  export obs_dir_NCL=\"/groups/ESS/share/projects/SWUS3km/data/OBS/AirNow/AQF5X\"\n  export ofname=\"/AQF5X_Hourly_\"\n\n  export postdata_dir=$cmaq_folder\"/prediction_nc_files/\"\n\n  export mfname=\"COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n\n  export grid_fname=\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/GRIDCRO2D_\"$YYYYMMDD_POST\".nc\" \n  echo \"Current Day: \"$stdate_file\n  # determine if the prediction netcdf is there\n  predict_nc_file=$cmaq_folder\"/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_\"$stdate_file\"_ML_extracted.nc\"\n  if [ -f \"$predict_nc_file\" ]; then\n    echo \"$predict_nc_file exists.\"\n  else\n    echo \"$predict_nc_file doesn't exist. Skipping...\"\n    continue\n  fi\n  \n  predict_eval_file=$permanent_eval_folder\"eval_\"$stdate_file\".txt\"\n  \n  if [ \"$force\" != true ] ; then\n    if [ -f \"$predict_eval_file\" ]; then\n      echo \"$predict_eval_file exists. Skipping...\"\n      continue\n    else\n      echo \"$predict_eval_file doesn't exist. Generating...\"\n    fi\n  fi\n  \n  rm -rf $cmaq_folder/results/* # clean everything first\n  ncl $cmaq_folder/geoweaver_eva_daily_O3.ncl\n  \n  if [ $? -eq 0 ]; then\n    echo \"Evaluation Completed Successfully\"\n    cat $wfname\n    cp $wfname $predict_eval_file\n  else\n    echo \"Evaluation Failed!\"\n  fi\n  \ndone\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "slsirb",
  "name" : "test_data_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\necho \"write the slurm script into test_data_slurm_generated.sh\"\ncat > test_data_slurm_generated.sh << EOF\n#!/bin/bash\n#SBATCH -J test_data_slurm       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=8G          # Memory per node (use units like G for gigabytes)\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n# merge all hourly testing data into daily files\n\nimport pandas as pd\nimport glob\nimport os\nfrom pathlib import Path\nfrom cmaq_ai_utils import *\n\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\nprint(f\"testing_path: {testing_path}\") \n# advisable to use os.path.join as this makes concatenation OS independent\n# stupid move. why reading all files to concat a new file. Just append all the rows to the file!\n\ncombined_testing_csv_path = f\"{testing_path}/testing.csv\"\nprint(f\"removed all file {combined_testing_csv_path}\")\nremove_file(f\"{combined_testing_csv_path}\")\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n        print(f\"adding {file_name}\")\n        file_path = os.path.join(testing_path, file_name)\n        df = pd.read_csv(file_path)  # Read the CSV file into a dataframe\n        # Perform any desired data processing on 'df' here\n        # dropping unnecessary variables\n        df['YYYYMMDDHH'] = df['YYYYMMDDHH'].map(str)\n        df['month'] = df['YYYYMMDDHH'].str[4:6]\n        df['day'] = df['YYYYMMDDHH'].str[6:8]\n        df['hours'] = df['YYYYMMDDHH'].str[8:10]\n\n        #cmaq.to_csv(f\"{testing_path}/testing.csv\",index=False)\n        # Append or write the processed dataframe to the final CSV file\n        mode = 'w' if write_header else 'a'\n        header = True if write_header else False\n        df.to_csv(combined_testing_csv_path, mode=mode, header=header, index=False)\n        print(f\"{combined_testing_csv_path} is updated\")\n        # After the first file, set the flag to False to avoid writing headers\n        write_header = False\n\nprint(f'Combined data saved to {combined_testing_csv_path}')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch test_data_slurm_generated.sh\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"test_data_slurm_generated.sh\"\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    else\n        echo \"Job $job_id is still running with state: $job_status\"\nfi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\ncat /scratch/zsun/test_data_slurm-hop046-$job_id.out\n\necho \"All slurm job for test_data finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xa1jxg",
  "name" : "rf_prediction_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\necho \"start to run test_data_slurm_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"rf_prediction_slurm_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J rf_prediction       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 8               # Number of CPUs per task (threads)\n#SBATCH --mem=150G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n## Slurm can send you updates via email\n#SBATCH --mail-type=FAIL  # BEGIN,END,FAIL         # ALL,NONE,BEGIN,END,FAIL,REQUEUE,..\n#SBATCH --mail-user=zsun@gmu.edu     # Put your GMU email address here\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# use the trained model to predict on the testing data and save the results to prediction_rf.csv\n\nimport pandas as pd\nimport pickle\nfrom pathlib import Path\nfrom time import sleep\nimport os\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom cmaq_ai_utils import *\n\nprint(\"create and clean the prediction folder\")\ncreate_and_clean_folder(f\"{cmaq_folder}/prediction_files/\")\n\n# importing data\n# final=pd.read_csv(f\"{cmaq_folder}/testing_input_hourly/testing.csv\")\ntesting_path = f'{cmaq_folder}/testing_input_hourly'\n#all_hourly_files = glob.glob(os.path.join(testing_path, \"test_data_*.csv\"))\n#df_from_each_hourly_file = (pd.read_csv(f) for f in all_hourly_files)\n\n# load the model from disk\n# filename = f'{cmaq_folder}/models/rf_pycaret.sav'\n\nprint(\"start to load model\")\n\nfilename = f'{model_folder}/rf_pycaret_o3_one_year_good.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\nprint(\"model is loaded\")\n\n#for testing_df in df_from_each_hourly_file:\nfile_list = os.listdir(testing_path)\n\n# Initialize a flag to indicate if the final CSV file needs a header\nwrite_header = True\n\nfor file_name in file_list:\n    if file_name.endswith('.csv') and file_name.startswith('test_data_'):  # Adjust the file extension as needed\n      print(f\"adding {file_name}\")\n      file_path = os.path.join(testing_path, file_name)\n      testing_df = pd.read_csv(file_path)\n      # Perform any desired data processing on 'df' here\n      # dropping unnecessary variables\n      print(\"adding month, day, and hours\")\n      testing_df['YYYYMMDDHH'] = testing_df['YYYYMMDDHH'].map(str)\n      testing_df['month'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[4:6], errors='coerce', downcast='integer')\n      testing_df['day'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[6:8], errors='coerce', downcast='integer')\n      testing_df['hours'] = pd.to_numeric(testing_df['YYYYMMDDHH'].str[8:10], errors='coerce', downcast='integer')\n\n      print(testing_df['YYYYMMDDHH'].values[0])\n      print(testing_df['month'].values[0])\n      file_dateTime = testing_df['YYYYMMDDHH'].values[0]\n      print(f\"file_dateTime={file_dateTime}\")\n      #X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude'],axis=1)\n      testing_df['time_of_day'] = (testing_df['hours'] % 24 + 4) // 4\n\n      # Make coords even more coarse by rounding to closest multiple of 5 \n      # (e.g., 40, 45, 85, 55)\n      #testing_df['Latitude_ExtraCoarse'] = 0.1 * round(testing_df['Latitude']/0.1)\n      #testing_df['Longitude_ExtraCoarse'] = 0.1 * round(testing_df['Longitude']/0.1)\n      X = testing_df.drop(['YYYYMMDDHH','Latitude','Longitude', 'CO(moles/s)'],axis=1)\n\n      print(X.columns)\n\n      # # making prediction\n      pred = loaded_model.predict(X)\n\n      # adding prediction values to test dataset\n      #testing_df['prediction'] = testing_df['CMAQ12KM_O3(ppb)'].tolist()\n      testing_df['prediction'] = pred\n\n      testing_df = testing_df[['Latitude', 'Longitude','YYYYMMDDHH','prediction']]\n      # saving the dataset into local drive\n      print(f'Saving: {cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv')\n      testing_df.to_csv(f'{cmaq_folder}/prediction_files/prediction_rf_{file_dateTime}.csv',index=False)\n        \nprint(\"Prediction is all done.\")\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "klx3sc",
  "name" : "preprocessing_test_netcdf_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"preprocessing_test_netcdf.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J preprocessing_test_netcdf       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=20G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n\n# load the prediction_rf.csv into a NetCDF file for visualization\nfrom cmaq_ai_utils import *\n\n# end_date = datetime.today()\n# base = end_date - timedelta(days=2)\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\ntoday = datetime.today()\nedate = today\nsdate = today - timedelta(days=days_back)\ndays = get_days_list_for_prediction(sdate, edate)\n\nprediction_path = f\"{cmaq_folder}/prediction_files/\"\n\nall_hourly_files = sorted(glob.glob(os.path.join(prediction_path, \"*.csv\")))\n# print(\"overall hourly files: \", all_hourly_files)\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\nfor i in range(len(days)-1):\n  print(days[i])\n  current_day = days[i]\n  next_day = days[i+1]\n  \n  cmaq_cdf_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n  \n  if not os.path.exists(cmaq_cdf_file):\n    print(f\"{cmaq_cdf_file} doesn't exist\")\n    continue\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cdf = xr.open_dataset(cmaq_cdf_file, engine='netcdf4')\n  daily_hourly_files = []\n  for k in real_hour_list:\n    real_hour_value = real_hour_list[k]\n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    #daily_hourly_files.append(f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    daily_hourly_files.append(f'{cmaq_folder}/prediction_files/prediction_rf_{day}{turn_2_digits(real_hour_value)}.csv')\n  \n  daily_hourly_files = sorted(daily_hourly_files)\n  #print(\"single day hourly files: \", all_hourly_files[i*24:(i+1)*24])\n  print(\"single day hourly files: \", daily_hourly_files)\n  df_from_each_hourly_file = (pd.read_csv(f) for f in daily_hourly_files)\n  \n  df_csv = pd.concat(df_from_each_hourly_file, ignore_index=True)\n\n  reshaped_prediction = df_csv['prediction'].to_numpy().reshape(24, 1, 265, 442).astype(np.float32)\n  print(reshaped_prediction.shape)\n  \n  # retain only two essential variables\n  clean_df_cdf = df_cdf[['O3', 'TFLAG']]\n  print(\"O3 attrs is: \", df_cdf.O3.attrs)\n  \n  # reduce VAR dim to 1\n  new_tflag = df_cdf['TFLAG'].to_numpy()\n  new_tflag = new_tflag[:, 0, :].reshape(24, 1, 2)\n  \n  # Apply changes to data variable in nc file\n  clean_df_cdf['O3'] = (['TSTEP', 'LAY', 'ROW', 'COL'], reshaped_prediction)\n  clean_df_cdf['TFLAG'] = (['TSTEP', 'VAR', 'DATE-TIME'], new_tflag)\n\n  clean_df_cdf.O3.attrs = df_cdf.O3.attrs\n  clean_df_cdf.TFLAG.attrs = df_cdf.TFLAG.attrs\n  clean_df_cdf.attrs['VGLVLS'] = \"1.f, 0.9941f\"\n  clean_df_cdf.attrs['VAR-LIST'] = \"O3              \"\n#   create_and_clean_folder(f\"{cmaq_folder}/prediction_nc_files\")\n  clean_df_cdf.to_netcdf(target_cdf_file,)\n\n  print(f'Saved updated netCDF file: {target_cdf_file}')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "z4du0c",
  "name" : "processing_test_data_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n\n\necho \"start to run processing_test_data_generated.sh\"\npwd\n\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"processing_test_data_generated.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J processing_test_data       # Job name\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n#SBATCH -n 1               # Number of tasks\n#SBATCH -c 4               # Number of CPUs per task (threads)\n#SBATCH --mem=50G          # Memory per node (use units like G for gigabytes) - this job must need 200GB lol\n#SBATCH -t 0-01:00         # Runtime in D-HH:MM format\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\npython << INNER_EOF\n\n# get hourly CMAQ data into csv for prediction\n\nfrom cmaq_ai_utils import *\n\n\n#edate = datetime.today()\n#sdate = edate - timedelta(days=1)\n# today = datetime.today()\n# edate = today\n# sdate = today - timedelta(days=days_back)\n\n#sdate = date(2022, 8, 6)   # start date\n#edate = date(2022, 8, 8)   # end date\n# days = get_days_list_for_prediction(sdate, edate)\n\nreal_hour_list = [12,13,14,15,16,17,18,19,20,21,22,23,0,1,2,3,4,5,6,7,8,9,10,11]\ntime_step_in_netcdf_list = range(0,24)\n\ntest_folder = f\"{cmaq_folder}/testing_input_hourly/\"\ncreate_and_clean_folder(test_folder)  # don't clean folder anymore\n#os.makedirs(test_folder, exist_ok=True)\n\nfor x in range(len(days)-1):\n  current_day = days[x]\n  next_day = days[x+1]\n  print(\"Getting data for: \"+current_day)\n  \n  # read cmaq results\n  cmaq_file = \"/scratch/yli74/forecast/12km/POST/COMBINE3D_ACONC_v531_gcc_AQF5X_\"+current_day+\".nc\"\n  if not os.path.exists(cmaq_file):\n    print(f\"CMAQ file {cmaq_file} doesn't exist\")\n    continue\n  \n  target_cdf_file = f'{cmaq_folder}/prediction_nc_files/COMBINE3D_ACONC_v531_gcc_AQF5X_'+current_day+'_ML_extracted.nc'\n    \n  if os.path.exists(target_cdf_file):\n    print(f\"{target_cdf_file} already exists\")\n    continue\n  \n  df_cmaq = xr.open_dataset(cmaq_file, engine='netcdf4')\n  \n  # read mcip results \n  mcip_file = \"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/mcip/12km/METCRO2D_\"+current_day+\".nc\"\n  df_mcip = xr.open_dataset(mcip_file, engine='netcdf4')\n  \n  # read emissions results \n  df_emis = xr.open_dataset(\"/groups/ESS/share/projects/SWUS3km/data/cmaqdata/emis2021/12km/all/emis_mole_all_\"+current_day+\"_AQF5X_cmaq_cb6ae7_2017gb_17j.ncf\", engine='netcdf4')\n  \n  for k in time_step_in_netcdf_list:\n    \n    real_hour_value = real_hour_list[k]\n    \n    if real_hour_value<12:\n      day = next_day\n    else:\n      day = current_day\n    \n    hourly_target_file = f'{test_folder}/test_data_{day}_{turn_2_digits(real_hour_value)}.csv'\n    if os.path.exists(hourly_target_file):\n      print(f\"Hourly csv file {hourly_target_file} already exist\")\n      continue\n    \n    df_hourly = pd.DataFrame()\n    \n    #print(\"df_cmaq.variables['O3'] shape: \", df_cmaq.variables['O3'].shape)\n    #print(\"df_cmaq.variables['O3'][:] shape: \", df_cmaq.variables['O3'][:].shape)\n    #print(\"df_cmaq.variables['O3'][:].values[k, 0].shape\", df_cmaq.variables['O3'][:].values[k, 0].shape)\n    # CMAQ data\n    # O3 variable\n    o3=df_cmaq.variables['O3'][:].values[k, 0]\n    cmaq_O3=list(np.ravel(o3).transpose())\n    #print(\"o3 shape: \", o3.shape)\n    #print(\"cmaq_O3 shape: \", np.ravel(o3).transpose().shape)\n    \n    # NO2\n    no2=df_cmaq.variables['NO2'][:].values[k, 0]\n    cmaq_NO2=list(np.ravel(no2).transpose())\n    \n    # CO\n    co=df_cmaq.variables['CO'][:].values[k, 0]\n    cmaq_CO=list(np.ravel(co).transpose())\n    \n    # PM25_CO\n    pm25=df_cmaq.variables['PM25_OC'][:].values[k, 0]\n    cmaq_PM25_CO=list(np.ravel(pm25).transpose())\n    \n    # EMIS data\n    co_emis=df_emis.variables['CO'][:].values[k, 0]\n    CO_emi=list(np.ravel(co_emis).transpose())    \n    \n    # MCIP data\n    # CO variable\n    prsfc=df_mcip.variables['PRSFC'][:].values[k, 0]\n    PRSFC=list(np.ravel(prsfc).transpose())\n    \n    # NO2\n    pbl=df_mcip.variables['PBL'][:].values[k, 0]\n    PBL=list(np.ravel(pbl).transpose())\n    \n    # TEMP2\n    temp2=df_mcip.variables['TEMP2'][:].values[k, 0]\n    TEMP2=list(np.ravel(temp2).transpose())\n    \n    # WSPD10\n    wspd10=df_mcip.variables['WSPD10'][:].values[k, 0]\n    WSPD10=list(np.ravel(wspd10).transpose())\n    \n    # WDIR10\n    wdir10=df_mcip.variables['WDIR10'][:].values[k, 0]\n    WDIR10=list(np.ravel(wdir10).transpose())\n    \n    # RGRND\n    rgrnd=df_mcip.variables['RGRND'][:].values[k, 0]\n    RGRND=list(np.ravel(rgrnd).transpose())\n    \n    # CFRAC\n    cfrac=df_mcip.variables['CFRAC'][:].values[k, 0]\n    CFRAC=list(np.ravel(cfrac).transpose())\n    \n    ## LAT/LON data\n    df_coords = xr.open_dataset('/home/yli74/scripts/plots/2020fire/GRIDCRO2D', engine='netcdf4')\n    \n    lat = df_coords.variables['LAT'][:].values[0,0]\n    #print(\"lat shape\", lat.shape)\n    lat_flt=np.ravel(lat)\n    LAT=lat_flt #np.tile(lat_flt,1)\n    \n    lon = df_coords.variables['LON'][:].values[0,0]\n    lon_flt=np.ravel(lon)\n    LON=lon_flt #np.tile(lon_flt,1)\n    \n    df_hourly['Latitude'] = LAT\n    df_hourly['Longitude'] = LON\n    df_hourly['YYYYMMDDHH'] = day+turn_2_digits(real_hour_value)\n    df_hourly['CMAQ12KM_O3(ppb)'] = cmaq_O3\n    df_hourly['CMAQ12KM_NO2(ppb)'] = cmaq_NO2\n    df_hourly['CMAQ12KM_CO(ppm)'] = cmaq_CO\n    df_hourly['CMAQ_OC(ug/m3)'] = cmaq_PM25_CO\n    df_hourly['CO(moles/s)'] = CO_emi\n    df_hourly['PRSFC(Pa)'] = PRSFC\n    df_hourly['PBL(m)'] = PBL\n    df_hourly['TEMP2(K)'] = TEMP2\n    df_hourly['WSPD10(m/s)'] = WSPD10\n    df_hourly['WDIR10(degree)'] = WDIR10\n    df_hourly['RGRND(W/m2)'] = RGRND\n    df_hourly['CFRAC'] = CFRAC\n    df_hourly['month'] = df_hourly['YYYYMMDDHH'].str[4:6]\n    df_hourly['day'] = df_hourly['YYYYMMDDHH'].str[6:8]\n    df_hourly['hours'] = df_hourly['YYYYMMDDHH'].str[8:10]\n    print(f'Saving file: test_data_{day}_{turn_2_digits(real_hour_value)}.csv')\n    df_hourly.to_csv(hourly_target_file, index=False)\n\nprint('Done with preparing testing data!')\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n# should have another check. if there is another job running, should cancel it before submitting a new job.\n\n# Find and cancel existing running jobs with the same script name\n#existing_jobs=$(squeue -h -o \"%A %j\" -u $(whoami) | awk -v script=\"$SCRIPT_NAME\" '$2 == script {print $1}')\n\n# if [ -n \"$existing_jobs\" ]; then\n#     echo \"Canceling existing jobs with the script name '$SCRIPT_NAME'...\"\n#     for job_id in $existing_jobs; do\n#         scancel $job_id\n#     done\n# else\n#     echo \"No existing jobs with the script name '$SCRIPT_NAME' found.\"\n# fi\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nwhile true; do\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    #echo \"job_status \"$job_status\n    #if [[ $job_status == \"JobState=COMPLETED\" ]]; then\n    #    break\n    #fi\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\nfind /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\n#cat /scratch/zsun/test_data_slurm-*-$job_id.out\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "or8itt",
  "name" : "plot_evaluation_charts",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n\ndef draw_charts(column_name, ai_df_sorted, cmaq_df_clipped):\n  # Plot the RMSE values\n  plt.figure(figsize=(12, 6))\n  plt.plot(ai_df_sorted['DATE'], ai_df_sorted[column_name], label='AI', color='green')\n  plt.plot(cmaq_df_clipped['DATE'], cmaq_df_clipped[column_name], label='CMAQ', color='blue')\n\n  # Customize the plot\n  plt.xlabel('Date')\n  plt.ylabel(column_name)\n  plt.title(f'CMAQ AI Metrics - {column_name}')\n  plt.legend()\n\n  # Save the plot as a PNG file\n  plt.grid(True)\n  plt.tight_layout()\n  plot_png_path = f'/groups/ESS3/zsun/cmaq/ai_results/evaluation/{column_name}_plot.png'\n  plt.savefig(plot_png_path)\n  print(f\"Plot saved to {plot_png_path}\")\n\ndef do_it():\n  # read the CMAQ evaluation txt\n  cmaq_eval_txt = \"/groups/ESS/share/projects/SWUS3km/graph/12km/alleva_12km_o3_fore.txt\"\n  cmaq_df = pd.read_csv(cmaq_eval_txt)  # Assuming your data has no header\n  \n  # read all txt files in the evaluation folder\n  directory_path = '/groups/ESS3/zsun/cmaq/ai_results/evaluation/'\n\n  # Initialize an empty list to store DataFrames.\n  dfs = []\n  \n  column_names = ['DATE', 'NSITES', 'AVG_OBS', 'AVG_MOD', 'RMSE', 'CORR', 'NMB', 'NME', 'MB', 'ME', 'AH', 'AFAR']\n  \n  # Iterate over the files in the directory.\n  for filename in os.listdir(directory_path):\n      if filename.startswith('eval'):  # Adjust the file extension as needed.\n          file_path = os.path.join(directory_path, filename)\n\n          # Read the CSV file into a DataFrame.\n          df = pd.read_csv(file_path, names = column_names, header=None)  # Assuming your data has no header.\n\n          # Append the DataFrame to the list.\n          dfs.append(df)\n\n  # Concatenate all DataFrames into a single DataFrame.\n  ai_df = pd.concat(dfs, ignore_index=True)\n  \n  # Convert the 'DATE' column to a datetime object\n  ai_df['DATE'] = pd.to_datetime(ai_df['DATE'], format='%Y%m%d')\n  cmaq_df['DATE'] = pd.to_datetime(cmaq_df['DATE'], format='%Y%m%d')\n  \n  # Sort the DataFrame by the 'DATE' column\n  ai_df_sorted = ai_df.sort_values(by='DATE')\n  cmaq_df_sorted = cmaq_df.sort_values(by='DATE')\n  \n  min_date = ai_df_sorted['DATE'].min()\n  cmaq_df_clipped = cmaq_df_sorted[cmaq_df_sorted['DATE'] >= min_date]\n  cmaq_df_clipped.loc[cmaq_df_clipped['AH'] < 0, 'AH'] = 0\n  \n  for column_name in column_names:\n    if column_name != \"DATE\":\n      draw_charts(column_name, ai_df_sorted, cmaq_df_clipped)\n  \n  print(\"all done\")\n  \ndo_it()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
